"""
Advanced AI Analytics and Monitoring System

Comprehensive monitoring for AI usage, performance metrics, cost tracking,
and detailed analytics dashboard for intelligent compliance system.
"""

from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

from config.logging_config import get_logger

logger = get_logger(__name__)


class MetricType(Enum):
    """Types of metrics to track."""

    PERFORMANCE = "performance"
    USAGE = "usage"
    COST = "cost"
    QUALITY = "quality"
    ERROR = "error"
    CACHE = "cache"


class AlertLevel(Enum):
    """Alert severity levels."""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class MetricEvent:
    """Individual metric event."""

    timestamp: datetime
    metric_type: MetricType
    name: str
    value: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    user_id: Optional[str] = None
    session_id: Optional[str] = None


@dataclass
class Alert:
    """System alert."""

    id: str
    level: AlertLevel
    title: str
    description: str
    timestamp: datetime
    resolved: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalyticsSummary:
    """Analytics summary for a time period."""

    period_start: datetime
    period_end: datetime
    total_requests: int
    average_response_time: float
    cache_hit_rate: float
    total_cost: float
    error_rate: float
    top_frameworks: List[Dict[str, Any]]
    performance_trends: Dict[str, List[float]]


class AIAnalyticsMonitor:
    """
    Advanced AI Analytics and Monitoring System

    Features:
    - Real-time performance monitoring
    - Cost tracking and optimization insights
    - Usage analytics and trends
    - Quality metrics and feedback loops
    - Alert system for anomalies
    - Comprehensive dashboard data
    """

    def __init__(self) -> None:
        # Metric storage (in production, this would use a time-series database)
        self.metrics: deque = deque(maxlen=10000)  # Keep last 10k metrics
        self.alerts: List[Alert] = []
        self.active_sessions: Dict[str, Dict[str, Any]] = {}

        # Aggregated metrics for quick access
        self.hourly_metrics: Dict[str, Dict[str, float]] = defaultdict(
            lambda: defaultdict(float)
        )
        self.daily_metrics: Dict[str, Dict[str, float]] = defaultdict(
            lambda: defaultdict(float)
        )

        # Alert thresholds
        self.alert_thresholds = {
            "response_time_ms": 5000,  # 5 seconds
            "error_rate_percent": 5.0,  # 5%
            "cost_per_hour": 10.0,  # $10/hour
            "cache_hit_rate_percent": 50.0,  # Below 50%
        }

        # Performance baselines
        self.performance_baselines = {
            "average_response_time_ms": 1500,
            "cache_hit_rate_percent": 75.0,
            "error_rate_percent": 1.0,
        }

    async def record_metric(
        self,
        metric_type: MetricType,
        name: str,
        value: float,
        metadata: Optional[Dict[str, Any]] = None,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> None:
        """Record a new metric event."""

        event = MetricEvent(
            timestamp=datetime.utcnow(),
            metric_type=metric_type,
            name=name,
            value=value,
            metadata=metadata or {},
            user_id=user_id,
            session_id=session_id,
        )

        self.metrics.append(event)

        # Update aggregated metrics
        await self._update_aggregated_metrics(event)

        # Check for alerts
        await self._check_alert_conditions(event)

        logger.debug(f"Recorded metric: {metric_type.value}.{name} = {value}")

    async def _update_aggregated_metrics(self, event: MetricEvent) -> None:
        """Update hourly and daily aggregated metrics."""

        # Get time keys
        hour_key = event.timestamp.strftime("%Y-%m-%d-%H")
        day_key = event.timestamp.strftime("%Y-%m-%d")

        # Update hourly metrics
        self.hourly_metrics[hour_key][
            f"{event.metric_type.value}.{event.name}"
        ] += event.value
        self.hourly_metrics[hour_key]["count"] += 1

        # Update daily metrics
        self.daily_metrics[day_key][
            f"{event.metric_type.value}.{event.name}"
        ] += event.value
        self.daily_metrics[day_key]["count"] += 1

    async def _check_alert_conditions(self, event: MetricEvent) -> None:
        """Check if the metric event triggers any alerts."""

        # Response time alerts
        if (
            event.name == "response_time_ms"
            and event.value > self.alert_thresholds["response_time_ms"]
        ):
            await self._create_alert(
                AlertLevel.WARNING,
                "High Response Time",
                f"AI response time of {event.value}ms exceeds threshold of {self.alert_thresholds['response_time_ms']}ms",
                {
                    "metric_value": event.value,
                    "threshold": self.alert_thresholds["response_time_ms"],
                },
            )

        # Cost alerts
        if (
            event.name == "cost_estimate"
            and event.value > self.alert_thresholds["cost_per_hour"]
        ):
            await self._create_alert(
                AlertLevel.WARNING,
                "High Cost Usage",
                f"Hourly cost of ${event.value:.2f} exceeds threshold of ${self.alert_thresholds['cost_per_hour']:.2f}",
                {
                    "metric_value": event.value,
                    "threshold": self.alert_thresholds["cost_per_hour"],
                },
            )

    async def _create_alert(
        self,
        level: AlertLevel,
        title: str,
        description: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Create a new alert."""

        alert = Alert(
            id=f"alert_{int(datetime.utcnow().timestamp() * 1000)}",
            level=level,
            title=title,
            description=description,
            timestamp=datetime.utcnow(),
            metadata=metadata or {},
        )

        self.alerts.append(alert)
        logger.warning(f"Alert created: {level.value} - {title}")

    async def get_real_time_metrics(self) -> Dict[str, Any]:
        """Get real-time system metrics."""

        now = datetime.utcnow()
        last_hour = now - timedelta(hours=1)

        # Filter recent metrics
        recent_metrics = [m for m in self.metrics if m.timestamp >= last_hour]

        if not recent_metrics:
            return {
                "status": "no_data",
                "message": "No metrics available for the last hour",
            }

        # Calculate real-time statistics
        response_times = [
            m.value for m in recent_metrics if m.name == "response_time_ms"
        ]
        error_count = len(
            [m for m in recent_metrics if m.metric_type == MetricType.ERROR]
        )
        cache_hits = len([m for m in recent_metrics if m.name == "cache_hit"])
        cache_misses = len([m for m in recent_metrics if m.name == "cache_miss"])

        total_requests = len(recent_metrics)
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else 0
        )
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        cache_hit_rate = (
            (cache_hits / (cache_hits + cache_misses) * 100)
            if (cache_hits + cache_misses) > 0
            else 0
        )

        return {
            "timestamp": now.isoformat(),
            "period": "last_hour",
            "metrics": {
                "total_requests": total_requests,
                "average_response_time_ms": round(avg_response_time, 2),
                "error_rate_percent": round(error_rate, 2),
                "cache_hit_rate_percent": round(cache_hit_rate, 2),
                "active_sessions": len(self.active_sessions),
            },
            "health_status": self._calculate_health_status(
                avg_response_time, error_rate, cache_hit_rate
            ),
            "active_alerts": len([a for a in self.alerts if not a.resolved]),
        }

    def _calculate_health_status(
        self, avg_response_time: float, error_rate: float, cache_hit_rate: float
    ) -> str:
        """Calculate overall system health status."""

        issues = 0

        if avg_response_time > self.performance_baselines["average_response_time_ms"]:
            issues += 1
        if error_rate > self.performance_baselines["error_rate_percent"]:
            issues += 1
        if cache_hit_rate < self.performance_baselines["cache_hit_rate_percent"]:
            issues += 1

        if issues == 0:
            return "excellent"
        elif issues == 1:
            return "good"
        elif issues == 2:
            return "fair"
        else:
            return "poor"

    async def get_usage_analytics(self, days: int = 7) -> Dict[str, Any]:
        """Get comprehensive usage analytics for the specified period."""

        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)

        # Filter metrics for the period
        period_metrics = [
            m for m in self.metrics if start_time <= m.timestamp <= end_time
        ]

        # Framework usage analysis
        framework_usage = defaultdict(int)
        content_type_usage = defaultdict(int)
        user_activity = defaultdict(int)

        for metric in period_metrics:
            if "framework" in metric.metadata:
                framework_usage[metric.metadata["framework"]] += 1
            if "content_type" in metric.metadata:
                content_type_usage[metric.metadata["content_type"]] += 1
            if metric.user_id:
                user_activity[metric.user_id] += 1

        # Daily usage trends
        daily_usage = defaultdict(int)
        for metric in period_metrics:
            day_key = metric.timestamp.strftime("%Y-%m-%d")
            daily_usage[day_key] += 1

        return {
            "period": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "days": days,
            },
            "total_requests": len(period_metrics),
            "framework_usage": dict(
                sorted(framework_usage.items(), key=lambda x: x[1], reverse=True)
            ),
            "content_type_usage": dict(
                sorted(content_type_usage.items(), key=lambda x: x[1], reverse=True)
            ),
            "daily_usage_trend": dict(sorted(daily_usage.items())),
            "active_users": len(user_activity),
            "top_users": dict(
                sorted(user_activity.items(), key=lambda x: x[1], reverse=True)[:10]
            ),
        }

    async def get_cost_analytics(self, days: int = 30) -> Dict[str, Any]:
        """Get detailed cost analytics and optimization insights."""

        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)

        # Filter cost-related metrics
        cost_metrics = [
            m
            for m in self.metrics
            if m.timestamp >= start_time and m.name in ["cost_estimate", "token_usage"]
        ]

        # Calculate cost statistics
        total_cost = sum(m.value for m in cost_metrics if m.name == "cost_estimate")
        total_tokens = sum(m.value for m in cost_metrics if m.name == "token_usage")

        # Daily cost breakdown
        daily_costs = defaultdict(float)
        for metric in cost_metrics:
            if metric.name == "cost_estimate":
                day_key = metric.timestamp.strftime("%Y-%m-%d")
                daily_costs[day_key] += metric.value

        # Cost by content type
        cost_by_type = defaultdict(float)
        for metric in cost_metrics:
            if metric.name == "cost_estimate" and "content_type" in metric.metadata:
                content_type = metric.metadata["content_type"]
                cost_by_type[content_type] += metric.value

        # Calculate optimization opportunities
        cache_savings = await self._calculate_cache_savings()
        optimization_savings = await self._calculate_optimization_savings()

        return {
            "period": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "days": days,
            },
            "cost_summary": {
                "total_cost": round(total_cost, 4),
                "average_daily_cost": round(total_cost / days, 4),
                "total_tokens": int(total_tokens),
                "cost_per_token": (
                    round(total_cost / total_tokens, 6) if total_tokens > 0 else 0
                ),
            },
            "daily_cost_trend": dict(sorted(daily_costs.items())),
            "cost_by_content_type": dict(
                sorted(cost_by_type.items(), key=lambda x: x[1], reverse=True)
            ),
            "optimization_opportunities": {
                "cache_savings": cache_savings,
                "optimization_savings": optimization_savings,
                "total_potential_savings": cache_savings + optimization_savings,
            },
        }

    async def _calculate_cache_savings(self) -> float:
        """Calculate estimated savings from caching."""
        cache_hits = len([m for m in self.metrics if m.name == "cache_hit"])
        estimated_cost_per_request = 0.001  # $0.001 per request estimate
        return cache_hits * estimated_cost_per_request

    async def _calculate_optimization_savings(self) -> float:
        """Calculate estimated savings from optimization."""
        optimized_requests = len(
            [m for m in self.metrics if "optimization_metadata" in m.metadata]
        )
        estimated_savings_per_optimization = 0.0005  # $0.0005 per optimization
        return optimized_requests * estimated_savings_per_optimization

    async def get_quality_metrics(self) -> Dict[str, Any]:
        """Get AI response quality metrics and feedback analysis."""

        # This would integrate with actual quality scoring in production
        # For now, return simulated quality metrics

        return {
            "overall_quality_score": 8.5,
            "quality_trends": {
                "recommendations": 8.7,
                "policies": 8.9,
                "workflows": 8.3,
                "analysis": 8.1,
            },
            "feedback_summary": {
                "total_feedback_items": 150,
                "positive_feedback": 85,
                "negative_feedback": 15,
                "neutral_feedback": 50,
                "satisfaction_rate": 85.0,
            },
            "improvement_areas": [
                "Response specificity for small organizations",
                "Industry-specific terminology accuracy",
                "Workflow step granularity",
            ],
        }

    async def get_alerts(self, resolved: Optional[bool] = None) -> List[Dict[str, Any]]:
        """Get system alerts, optionally filtered by resolution status."""

        alerts = self.alerts
        if resolved is not None:
            alerts = [a for a in alerts if a.resolved == resolved]

        return [
            {
                "id": alert.id,
                "level": alert.level.value,
                "title": alert.title,
                "description": alert.description,
                "timestamp": alert.timestamp.isoformat(),
                "resolved": alert.resolved,
                "metadata": alert.metadata,
            }
            for alert in sorted(alerts, key=lambda x: x.timestamp, reverse=True)
        ]

    async def resolve_alert(self, alert_id: str) -> bool:
        """Mark an alert as resolved."""

        for alert in self.alerts:
            if alert.id == alert_id:
                alert.resolved = True
                logger.info(f"Alert {alert_id} marked as resolved")
                return True

        return False

    async def get_dashboard_data(self) -> Dict[str, Any]:
        """Get comprehensive dashboard data for monitoring interface."""

        real_time_metrics = await self.get_real_time_metrics()
        usage_analytics = await self.get_usage_analytics(7)
        cost_analytics = await self.get_cost_analytics(30)
        quality_metrics = await self.get_quality_metrics()
        active_alerts = await self.get_alerts(resolved=False)

        return {
            "real_time": real_time_metrics,
            "usage_analytics": usage_analytics,
            "cost_analytics": cost_analytics,
            "quality_metrics": quality_metrics,
            "alerts": active_alerts[:10],  # Latest 10 unresolved alerts
            "system_health": {
                "status": real_time_metrics.get("health_status", "unknown"),
                "uptime_hours": 24,  # This would be calculated from actual uptime
                "last_updated": datetime.utcnow().isoformat(),
            },
        }


# Global analytics monitor instance
analytics_monitor = AIAnalyticsMonitor()


async def get_analytics_monitor() -> AIAnalyticsMonitor:
    """Get the global analytics monitor instance."""
    return analytics_monitor
