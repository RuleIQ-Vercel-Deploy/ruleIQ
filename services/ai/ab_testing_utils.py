"""
A/B Testing Utilities and Integration Helpers

Provides convenient utilities for integrating A/B testing into the ruleIQ
compliance system, including AI model testing, prompt optimization,
and compliance effectiveness measurement.
"""

from typing import Any, Dict, List, Optional
from enum import Enum

from .ab_testing_framework import (
    ExperimentConfig,
    ExperimentType,
    MetricType,
    StatisticalResult,
    get_ab_testing_framework,
    create_ai_model_experiment,
)
from .analytics_monitor import get_analytics_monitor
from config.logging_config import get_logger

logger = get_logger(__name__)


class ComplianceMetric(Enum):
    """Compliance-specific metrics for A/B testing."""

    ASSESSMENT_ACCURACY = "assessment_accuracy"
    COMPLIANCE_SCORE_IMPROVEMENT = "compliance_score_improvement"
    EVIDENCE_COLLECTION_RATE = "evidence_collection_rate"
    USER_SATISFACTION = "user_satisfaction"
    TASK_COMPLETION_TIME = "task_completion_time"
    ERROR_RATE = "error_rate"
    RECOMMENDATION_RELEVANCE = "recommendation_relevance"


class AIModelTester:
    """A/B testing utilities for AI model comparisons."""

    def __init__(self):
        self.framework = get_ab_testing_framework()
        self.analytics = get_analytics_monitor()
        self.active_experiments: Dict[str, str] = {}  # model_pair -> experiment_id

    def setup_model_comparison(
        self,
        control_model: str,
        treatment_model: str,
        metric: ComplianceMetric = ComplianceMetric.ASSESSMENT_ACCURACY,
        min_effect_size: float = 0.05,
    ) -> str:
        """
        Set up A/B test for comparing two AI models.

        Args:
            control_model: Current/baseline model
            treatment_model: New model to test
            metric: Primary metric to compare
            min_effect_size: Minimum meaningful difference

        Returns:
            Experiment ID
        """
        experiment_id = create_ai_model_experiment(
            model_a=control_model,
            model_b=treatment_model,
            metric=metric.value,
            min_effect_size=min_effect_size,
        )

        model_pair = f"{control_model}_vs_{treatment_model}"
        self.active_experiments[model_pair] = experiment_id

        logger.info(
            f"Set up model comparison: {control_model} vs {treatment_model} (exp: {experiment_id})"
        )
        return experiment_id

    def record_model_performance(
        self,
        experiment_id: str,
        model_name: str,
        user_id: str,
        metric_value: float,
        context: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Record performance metric for a model in an experiment.

        Args:
            experiment_id: ID of the experiment
            model_name: Name of the model (determines variant)
            user_id: User who triggered the evaluation
            metric_value: Performance metric value
            context: Additional context

        Returns:
            True if recorded successfully
        """
        # Determine variant based on model name
        variant = "treatment" if "treatment" in model_name.lower() else "control"

        return self.framework.record_metric(
            experiment_id=experiment_id,
            variant=variant,
            user_id=user_id,
            primary_metric_value=metric_value,
            context=context or {},
        )

    def analyze_model_performance(self, experiment_id: str) -> StatisticalResult:
        """
        Analyze A/B test results for model comparison.

        Args:
            experiment_id: ID of the experiment to analyze

        Returns:
            Statistical analysis results
        """
        result = self.framework.analyze_experiment(experiment_id)

        # Log significant findings
        if result.is_significant:
            logger.info(
                f"Model experiment {experiment_id} shows significant difference: "
                f"p={result.p_value:.4f}, effect size={result.effect_size:.3f}"
            )

        return result


class PromptOptimizationTester:
    """A/B testing utilities for prompt optimization."""

    def __init__(self):
        self.framework = get_ab_testing_framework()
        self.analytics = get_analytics_monitor()

    def setup_prompt_test(
        self,
        original_prompt: str,
        optimized_prompt: str,
        task_type: str,
        metric: ComplianceMetric = ComplianceMetric.TASK_COMPLETION_TIME,
    ) -> str:
        """
        Set up A/B test for prompt optimization.

        Args:
            original_prompt: Current prompt (control)
            optimized_prompt: New prompt to test (treatment)
            task_type: Type of task (assessment, evidence, etc.)
            metric: Primary metric to compare

        Returns:
            Experiment ID
        """
        config = ExperimentConfig(
            name=f"Prompt Optimization: {task_type}",
            description=f"Compare prompt effectiveness for {task_type} tasks",
            experiment_type=ExperimentType.PROMPT_OPTIMIZATION,
            metric_type=MetricType.CONTINUOUS
            if metric
            in [ComplianceMetric.TASK_COMPLETION_TIME, ComplianceMetric.ASSESSMENT_ACCURACY]
            else MetricType.BINARY,
            primary_metric=metric.value,
            secondary_metrics=["user_satisfaction", "response_relevance"],
            min_effect_size=0.1,
            traffic_split={"control": 0.5, "treatment": 0.5},
            min_sample_size=100,
            tags=["prompt", "optimization", task_type],
        )

        experiment_id = self.framework.create_experiment(config)

        logger.info(f"Set up prompt optimization test for {task_type} (exp: {experiment_id})")
        return experiment_id

    def get_prompt_variant(self, experiment_id: str, user_id: str) -> str:
        """
        Get which prompt variant to use for a user.

        Args:
            experiment_id: ID of the experiment
            user_id: User identifier

        Returns:
            Variant name ("control" or "treatment")
        """
        return self.framework.assign_variant(experiment_id, user_id)

    def record_prompt_result(
        self,
        experiment_id: str,
        variant: str,
        user_id: str,
        metric_value: float,
        task_context: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Record result of using a prompt variant.

        Args:
            experiment_id: ID of the experiment
            variant: Which prompt variant was used
            user_id: User who completed the task
            metric_value: Task performance metric
            task_context: Additional task context

        Returns:
            True if recorded successfully
        """
        return self.framework.record_metric(
            experiment_id=experiment_id,
            variant=variant,
            user_id=user_id,
            primary_metric_value=metric_value,
            context=task_context or {},
        )


class ComplianceEffectivenessTester:
    """A/B testing for compliance methodology effectiveness."""

    def __init__(self):
        self.framework = get_ab_testing_framework()
        self.analytics = get_analytics_monitor()

    def setup_methodology_test(
        self,
        methodology_name: str,
        control_approach: str,
        treatment_approach: str,
        metric: ComplianceMetric = ComplianceMetric.COMPLIANCE_SCORE_IMPROVEMENT,
    ) -> str:
        """
        Set up A/B test for compliance methodology effectiveness.

        Args:
            methodology_name: Name of the methodology being tested
            control_approach: Current approach
            treatment_approach: New approach to test
            metric: Primary effectiveness metric

        Returns:
            Experiment ID
        """
        config = ExperimentConfig(
            name=f"Compliance Methodology: {methodology_name}",
            description=f"Compare {control_approach} vs {treatment_approach}",
            experiment_type=ExperimentType.COMPLIANCE_EFFECTIVENESS,
            metric_type=MetricType.CONTINUOUS,
            primary_metric=metric.value,
            secondary_metrics=[
                "evidence_collection_rate",
                "user_satisfaction",
                "task_completion_time",
            ],
            min_effect_size=0.15,  # 15% improvement in compliance effectiveness
            traffic_split={"control": 0.5, "treatment": 0.5},
            min_sample_size=50,  # Smaller sample for business impact tests
            max_duration_days=60,  # Longer duration for business changes
            tags=["compliance", "methodology", methodology_name],
        )

        experiment_id = self.framework.create_experiment(config)

        logger.info(
            f"Set up compliance methodology test: {methodology_name} (exp: {experiment_id})"
        )
        return experiment_id

    def record_compliance_outcome(
        self,
        experiment_id: str,
        variant: str,
        organization_id: str,
        improvement_score: float,
        baseline_score: float,
        context: Optional[Dict[str, Any]] = None,
    ) -> bool:
        """
        Record compliance improvement outcome.

        Args:
            experiment_id: ID of the experiment
            variant: Which methodology was used
            organization_id: Organization identifier
            improvement_score: Final compliance score
            baseline_score: Starting compliance score
            context: Additional context

        Returns:
            True if recorded successfully
        """
        # Calculate improvement percentage
        improvement_percentage = ((improvement_score - baseline_score) / baseline_score) * 100

        enhanced_context = context or {}
        enhanced_context.update(
            {
                "baseline_score": baseline_score,
                "final_score": improvement_score,
                "improvement_percentage": improvement_percentage,
            }
        )

        return self.framework.record_metric(
            experiment_id=experiment_id,
            variant=variant,
            user_id=organization_id,
            primary_metric_value=improvement_percentage,
            context=enhanced_context,
        )


class ABTestingManager:
    """Central manager for all A/B testing activities in ruleIQ."""

    def __init__(self):
        self.framework = get_ab_testing_framework()
        self.model_tester = AIModelTester()
        self.prompt_tester = PromptOptimizationTester()
        self.compliance_tester = ComplianceEffectivenessTester()

        # Track active experiments by type
        self.active_experiments: Dict[str, List[str]] = {
            "model_comparison": [],
            "prompt_optimization": [],
            "compliance_effectiveness": [],
            "feature_rollout": [],
        }

    def start_ai_model_experiment(
        self,
        control_model: str,
        treatment_model: str,
        metric: ComplianceMetric = ComplianceMetric.ASSESSMENT_ACCURACY,
    ) -> str:
        """Start an AI model comparison experiment."""
        experiment_id = self.model_tester.setup_model_comparison(
            control_model, treatment_model, metric
        )

        self.framework.start_experiment(experiment_id)
        self.active_experiments["model_comparison"].append(experiment_id)

        return experiment_id

    def start_prompt_experiment(
        self, original_prompt: str, optimized_prompt: str, task_type: str
    ) -> str:
        """Start a prompt optimization experiment."""
        experiment_id = self.prompt_tester.setup_prompt_test(
            original_prompt, optimized_prompt, task_type
        )

        self.framework.start_experiment(experiment_id)
        self.active_experiments["prompt_optimization"].append(experiment_id)

        return experiment_id

    def start_compliance_experiment(
        self, methodology_name: str, control_approach: str, treatment_approach: str
    ) -> str:
        """Start a compliance methodology experiment."""
        experiment_id = self.compliance_tester.setup_methodology_test(
            methodology_name, control_approach, treatment_approach
        )

        self.framework.start_experiment(experiment_id)
        self.active_experiments["compliance_effectiveness"].append(experiment_id)

        return experiment_id

    def get_experiment_results(self, experiment_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get results for all experiments of a given type."""
        experiments_to_analyze = []

        if experiment_type and experiment_type in self.active_experiments:
            experiments_to_analyze = self.active_experiments[experiment_type]
        else:
            # Get all active experiments
            for exp_list in self.active_experiments.values():
                experiments_to_analyze.extend(exp_list)

        results = []
        for experiment_id in experiments_to_analyze:
            try:
                summary = self.framework.get_experiment_summary(experiment_id)

                # Add latest analysis if available
                if summary["data_summary"]["total_observations"] >= 30:
                    latest_result = self.framework.analyze_experiment(experiment_id)
                    summary["latest_analysis"] = {
                        "p_value": latest_result.p_value,
                        "effect_size": latest_result.effect_size,
                        "is_significant": latest_result.is_significant,
                        "recommendation": latest_result.recommendation,
                        "confidence_interval": latest_result.confidence_interval,
                    }

                results.append(summary)

            except Exception as e:
                logger.error(f"Error analyzing experiment {experiment_id}: {e}")

        return results

    def get_statistical_power_analysis(self, experiment_id: str) -> Dict[str, Any]:
        """Get detailed statistical power analysis for an experiment."""
        summary = self.framework.get_experiment_summary(experiment_id)

        if not summary["results"]:
            return {"error": "No analysis results available"}

        latest_result = summary["results"][-1]

        power_analysis = {
            "current_power": latest_result["power"],
            "sample_size": sum(summary["data_summary"]["variant_counts"].values()),
            "effect_size": latest_result["effect_size"],
            "p_value": latest_result["p_value"],
            "recommendation": latest_result["recommendation"],
            "statistical_significance": latest_result["is_significant"],
            "practical_significance": latest_result["practical_significance"],
        }

        # Add sample size recommendations
        if latest_result["power"] < 0.8:
            required_additional_samples = self._estimate_additional_samples_needed(
                latest_result["power"], latest_result["effect_size"]
            )
            power_analysis["additional_samples_needed"] = required_additional_samples

        return power_analysis

    def _estimate_additional_samples_needed(self, current_power: float, effect_size: float) -> int:
        """Estimate additional samples needed to reach 80% power."""
        # Simplified estimation - in practice would use more sophisticated power calculations
        if current_power >= 0.8:
            return 0

        # Rule of thumb: double sample size to increase power significantly
        current_total = sum(
            summary["data_summary"]["variant_counts"].values()
            for summary in [
                self.framework.get_experiment_summary(exp_id)
                for exp_list in self.active_experiments.values()
                for exp_id in exp_list
            ]
            if summary["data_summary"]["total_observations"] > 0
        )

        return max(100, int(current_total * (0.8 - current_power) / current_power))


# Global instance
_ab_testing_manager: Optional[ABTestingManager] = None


def get_ab_testing_manager() -> ABTestingManager:
    """Get global A/B testing manager instance."""
    global _ab_testing_manager
    if _ab_testing_manager is None:
        _ab_testing_manager = ABTestingManager()
    return _ab_testing_manager


# Convenience functions for common use cases
def start_model_comparison(control_model: str, treatment_model: str) -> str:
    """Quick start for AI model A/B testing."""
    manager = get_ab_testing_manager()
    return manager.start_ai_model_experiment(control_model, treatment_model)


def record_ai_performance(
    experiment_id: str, model_name: str, user_id: str, accuracy_score: float
) -> bool:
    """Quick record for AI model performance."""
    manager = get_ab_testing_manager()
    return manager.model_tester.record_model_performance(
        experiment_id, model_name, user_id, accuracy_score
    )


def analyze_experiment_results(experiment_id: str) -> Dict[str, Any]:
    """Quick analysis for any experiment."""
    manager = get_ab_testing_manager()
    return manager.framework.get_experiment_summary(experiment_id)
