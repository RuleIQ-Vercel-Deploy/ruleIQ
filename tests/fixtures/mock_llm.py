"""
Mock LLM for deterministic testing in LangGraph workflows.

Provides consistent, predictable responses for testing without API calls.
"""

from typing import List, Dict, Any, Optional, Union
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage, BaseMessage
from langchain_core.outputs import LLMResult, Generation
from langchain_core.language_models import BaseChatModel
from langchain_core.callbacks import CallbackManagerForLLMRun
import json


class MockLLM(BaseChatModel):
    """
    Deterministic mock LLM for testing.

    Provides predictable responses based on input patterns.
    Temperature is always 0 for deterministic outputs.
    """

    # Class attributes for LangChain compatibility
    temperature: float = 0.0
    model_name: str = "mock-llm"

    def __init__(self, responses: Optional[Dict[str, str]] = None, **kwargs):
        """
        Initialize mock LLM with optional response mappings.

        Args:
            responses: Dict mapping input patterns to responses
        """
        super().__init__(**kwargs)
        self.responses = responses or self._get_default_responses()
        self.call_history: List[Dict[str, Any]] = []

    def _get_default_responses(self) -> Dict[str, str]:
        """Get default response mappings for common patterns."""
        return {
            # Compliance-related patterns
            "compliance": json.dumps(
                {
                    "assessment": "compliant",
                    "confidence": 0.95,
                    "evidence": ["Document reviewed", "Controls validated"],
                }
            ),
            # Risk assessment patterns
            "risk": json.dumps(
                {
                    "level": "medium",
                    "factors": ["Data exposure", "Third-party dependencies"],
                    "mitigation": "Implement additional controls",
                }
            ),
            # State transition patterns
            "next_step": "data_collection",
            "transition": "approved",
            # Generic patterns
            "test": "Test response for validation",
            "error": "Error simulation response",
            # JSON response pattern
            "json": json.dumps({"status": "success", "data": "test"}),
            # Workflow decisions
            "decision": "proceed",
            "validation": "valid",
            # Data processing
            "extract": json.dumps(
                {
                    "entities": ["Entity1", "Entity2"],
                    "relationships": [
                        {"from": "Entity1", "to": "Entity2", "type": "relates_to"}
                    ],
                }
            ),
        }

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any
    ) -> LLMResult:
        """
        Generate response based on input messages.

        Synchronous version for LangChain compatibility.
        """
        # Extract the last user message
        user_message = ""
        for msg in reversed(messages):
            if isinstance(msg, (HumanMessage, str)):
                user_message = str(msg.content if hasattr(msg, "content") else msg)
                break

        # Record the call
        self.call_history.append(
            {
                "timestamp": "2025-08-27T00:00:00Z",
                "messages": [
                    {
                        "role": m.__class__.__name__,
                        "content": str(m.content if hasattr(m, "content") else m),
                    }
                    for m in messages
                ],
                "kwargs": kwargs,
            }
        )

        # Find matching response pattern
        response = self._get_response_for_input(user_message.lower())

        # Create generation
        generation = Generation(
            text=response, generation_info={"mock": True, "pattern_matched": True}
        )

        return LLMResult(generations=[[generation]])

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any
    ) -> LLMResult:
        """
        Async version of generate for LangChain compatibility.
        """
        # Simply call the sync version as this is a mock
        return self._generate(messages, stop, run_manager, **kwargs)

    def _get_response_for_input(self, input_text: str) -> str:
        """
        Get response based on input text patterns.

        Args:
            input_text: Lowercase input text to match against patterns

        Returns:
            Matched response or default response
        """
        # Check for exact matches first
        if input_text in self.responses:
            return self.responses[input_text]

        # Check for pattern matches
        for pattern, response in self.responses.items():
            if pattern in input_text:
                return response

        # Default structured response
        return json.dumps(
            {
                "response": "Default mock response",
                "confidence": 0.8,
                "metadata": {"mock": True},
            }
        )

    def add_response(self, pattern: str, response: str):
        """
        Add a custom response pattern for testing.

        Args:
            pattern: Input pattern to match (will be lowercased)
            response: Response to return for this pattern
        """
        self.responses[pattern.lower()] = response

    def get_call_history(self) -> List[Dict[str, Any]]:
        """Get the history of all calls made to this mock."""
        return self.call_history

    def reset_history(self):
        """Reset the call history."""
        self.call_history = []

    @property
    def _llm_type(self) -> str:
        """Return identifier for this LLM."""
        return "mock"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Return parameters for identifying this LLM."""
        return {
            "model_name": self.model_name,
            "temperature": self.temperature,
            "mock": True,
        }


class MockLLMWithStreaming(MockLLM):
    """Mock LLM with streaming support for testing streaming workflows."""

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any
    ):
        """Stream tokens for testing streaming functionality."""
        response = self._get_response_for_input(
            str(messages[-1].content if messages else "")
        )

        # Simulate streaming by yielding response in chunks
        words = response.split()
        for i, word in enumerate(words):
            if i > 0:
                yield " "
            yield word

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any
    ):
        """Async stream tokens for testing streaming functionality."""
        response = self._get_response_for_input(
            str(messages[-1].content if messages else "")
        )

        # Simulate streaming by yielding response in chunks
        words = response.split()
        for i, word in enumerate(words):
            if i > 0:
                yield " "
            yield word


def create_deterministic_llm(scenario: str = "default") -> MockLLM:
    """
    Factory function to create mock LLMs for specific test scenarios.

    Args:
        scenario: Test scenario name

    Returns:
        Configured MockLLM instance
    """
    scenarios = {
        "default": {},
        "compliance_assessment": {
            "assess": json.dumps(
                {
                    "compliant": True,
                    "gaps": [],
                    "recommendations": ["Continue monitoring"],
                    "confidence": 0.98,
                }
            ),
            "validate": json.dumps({"valid": True, "issues": []}),
        },
        "error_simulation": {
            "process": "ERROR: Simulated failure",
            "validate": json.dumps(
                {"valid": False, "errors": ["Invalid input", "Missing data"]}
            ),
        },
        "state_transitions": {
            "next": "review",
            "decide": "approve",
            "route": "human_review",
        },
        "data_extraction": {
            "extract": json.dumps(
                {
                    "controls": [
                        {"id": "CTRL-001", "name": "Access Control"},
                        {"id": "CTRL-002", "name": "Data Encryption"},
                    ],
                    "obligations": [
                        {
                            "id": "OBL-001",
                            "regulation": "GDPR",
                            "requirement": "Data protection",
                        }
                    ],
                }
            )
        },
    }

    responses = scenarios.get(scenario, {})
    return MockLLM(responses=responses)


# Fixture function for pytest
def mock_llm_fixture():
    """Pytest fixture for mock LLM."""
    return create_deterministic_llm("default")


# Factory function for creating mock LLMs
def mock_llm_factory(scenario: str = "default"):
    """Factory function for creating mock LLMs with different scenarios."""
    return create_deterministic_llm(scenario)
