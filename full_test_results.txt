============================= test session starts ==============================
platform linux -- Python 3.12.3, pytest-8.4.1, pluggy-1.6.0
benchmark: 5.1.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/omar/Documents/ruleIQ
configfile: pytest.ini
testpaths: tests
plugins: anyio-4.10.0, langsmith-0.4.20, cov-6.2.1, dotenv-0.5.2, asyncio-1.1.0, benchmark-5.1.0
asyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2549 items

tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_basic_questions_accuracy ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_intermediate_questions_accuracy ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_advanced_questions_accuracy ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_source_citation_accuracy ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_response_completeness ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_framework_specific_terminology ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_consistency_across_similar_questions ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_regulatory_compliance_alignment ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestFrameworkCoverage::test_framework_identification_accuracy ERROR [  0%]
tests/ai/test_compliance_accuracy.py::TestFrameworkCoverage::test_cross_framework_guidance ERROR [  0%]
tests/api/test_admin_endpoints.py::TestSystemSettingsEndpoints::test_get_system_settings PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemSettingsEndpoints::test_update_system_settings PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemSettingsEndpoints::test_update_feature_flags PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemSettingsEndpoints::test_update_security_settings PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemMetricsEndpoints::test_get_system_metrics PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemMetricsEndpoints::test_get_usage_statistics PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemMetricsEndpoints::test_get_performance_metrics PASSED [  0%]
tests/api/test_admin_endpoints.py::TestSystemMetricsEndpoints::test_get_resource_utilization PASSED [  0%]
tests/api/test_admin_endpoints.py::TestAuditLogEndpoints::test_get_audit_logs PASSED [  0%]
tests/api/test_admin_endpoints.py::TestAuditLogEndpoints::test_filter_audit_logs_by_user PASSED [  0%]
tests/api/test_admin_endpoints.py::TestAuditLogEndpoints::test_filter_audit_logs_by_action PASSED [  0%]
tests/api/test_admin_endpoints.py::TestAuditLogEndpoints::test_export_audit_logs PASSED [  0%]
tests/api/test_admin_endpoints.py::TestLicenseManagementEndpoints::test_get_license_info PASSED [  0%]
tests/api/test_admin_endpoints.py::TestLicenseManagementEndpoints::test_update_license PASSED [  0%]
tests/api/test_admin_endpoints.py::TestLicenseManagementEndpoints::test_check_license_expiry PASSED [  0%]
tests/api/test_admin_endpoints.py::TestLicenseManagementEndpoints::test_get_license_usage PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemMaintenanceEndpoints::test_backup_system PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemMaintenanceEndpoints::test_restore_system PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemMaintenanceEndpoints::test_get_system_health PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemMaintenanceEndpoints::test_run_maintenance_tasks PASSED [  1%]
tests/api/test_admin_endpoints.py::TestIntegrationConfigurationEndpoints::test_configure_sso_integration PASSED [  1%]
tests/api/test_admin_endpoints.py::TestIntegrationConfigurationEndpoints::test_configure_smtp_settings PASSED [  1%]
tests/api/test_admin_endpoints.py::TestIntegrationConfigurationEndpoints::test_configure_storage_backend PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemNotificationEndpoints::test_send_system_announcement PASSED [  1%]
tests/api/test_admin_endpoints.py::TestSystemNotificationEndpoints::test_configure_alert_rules PASSED [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_create_assessment ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_create_assessment_invalid_framework ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_not_found ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_list_assessments ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_list_assessments_with_filters ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_update_assessment ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_delete_assessment ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_submit_assessment_response ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_bulk_submit_responses ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_score ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_gaps ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_generate_assessment_report ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_complete_assessment ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_ai_assessment_assistance ERROR [  1%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_evidence_upload ERROR [  2%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_collaboration ERROR [  2%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_history ERROR [  2%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_export ERROR [  2%]
tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_import ERROR [  2%]
tests/api/test_assessment_endpoints.py::TestAssessmentIntegration::test_complete_assessment_workflow ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_duplicate_email ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_invalid_email 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [ WARNING] app.core.monitoring.error_handler: Application error: Request validation failed
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_weak_password 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [ WARNING] app.core.monitoring.error_handler: Application error: Request validation failed
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_success ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_invalid_password ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_nonexistent_user 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_inactive_user ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user_invalid_token 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user_expired_token ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_refresh_token ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_logout ERROR   [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_password_reset_request ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_password_reset_confirm ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_change_password ERROR [  2%]
tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_verify_email ERROR [  2%]
tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_login_redirect 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [  2%]
tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_callback_success FAILED [  2%]
tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_callback_invalid_code FAILED [  2%]
tests/api/test_auth_endpoints.py::TestRBACAuth::test_admin_access ERROR  [  3%]
tests/api/test_auth_endpoints.py::TestRBACAuth::test_regular_user_denied_admin ERROR [  3%]
tests/api/test_auth_endpoints.py::TestRBACAuth::test_role_based_content ERROR [  3%]
tests/api/test_auth_endpoints.py::TestAuthIntegration::test_complete_auth_flow ERROR [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_business_profile_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_business_profile_not_found FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_create_business_profile_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_create_business_profile_duplicate FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_business_profile_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_business_profile_not_found FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_delete_business_profile_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_delete_business_profile_not_found FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_compliance_frameworks_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_compliance_frameworks_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_risk_assessment_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_validate_business_profile_success FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_export_business_profile_json FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_business_profile_history FAILED [  3%]
tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_business_profile_analytics FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_send_chat_message FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_get_chat_history FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_create_conversation FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_list_conversations FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_delete_conversation FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_chat_streaming FAILED [  3%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_suggestions FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_search FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_export_conversation FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_feedback FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_context_management FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_compliance_advisor FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_rate_limiting FAILED [  4%]
tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_with_attachments FAILED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceScoreEndpoints::test_calculate_compliance_score_success PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceScoreEndpoints::test_get_compliance_scores_history PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceScoreEndpoints::test_get_compliance_by_framework PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceControlEndpoints::test_get_all_controls PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceControlEndpoints::test_update_control_status PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceControlEndpoints::test_bulk_update_controls PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceControlEndpoints::test_filter_controls_by_status PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceViolationEndpoints::test_create_violation PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceViolationEndpoints::test_get_open_violations PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceViolationEndpoints::test_get_violations_by_severity PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceViolationEndpoints::test_update_violation_status PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceRecommendations::test_get_recommendations PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceRecommendations::test_get_risk_matrix PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceRecommendations::test_get_compliance_roadmap PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceReportEndpoints::test_generate_compliance_report PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceReportEndpoints::test_schedule_recurring_report PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceReportEndpoints::test_export_compliance_data PASSED [  4%]
tests/api/test_compliance_endpoints.py::TestComplianceIntegrationEndpoints::test_sync_with_grc_platform PASSED [  4%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_dashboard_metrics ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_compliance_overview ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_assessment_summary ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_trend_data ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_risk_matrix ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_activity_feed ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_upcoming_deadlines ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_team_performance ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_export_dashboard_report ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_compliance_gaps ERROR [  5%]
tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_ai_insights ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_list_evidence_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_list_evidence_with_filters ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_by_id_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_invalid_file_type ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_file_too_large ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_metadata ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_validate_evidence_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_download_evidence_success ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_bulk_upload_evidence ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_link_evidence_to_requirement ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_by_assessment ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_search_evidence ERROR [  5%]
tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_evidence_expiry_check ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_list_frameworks ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_by_id ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_not_found ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_create_custom_framework 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:18 [ WARNING] app.core.monitoring.error_handler: Application error: Method Not Allowed
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_create_framework_non_admin ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_update_framework 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Method Not Allowed
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_delete_framework 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Method Not Allowed
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_requirements ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_requirement_detail ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_add_requirement 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_update_requirement 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_delete_requirement 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_categories ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_statistics ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_mapping ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_export ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_import 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:19 [ WARNING] app.core.monitoring.error_handler: Application error: Method Not Allowed
FAILED                                                                   [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_search ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_recommendations ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkIntegration::test_framework_assessment_integration ERROR [  6%]
tests/api/test_framework_endpoints.py::TestFrameworkIntegration::test_multi_framework_compliance ERROR [  6%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_list_frameworks_success ERROR [  6%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_list_frameworks_with_filters ERROR [  6%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_by_id_success ERROR [  6%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_not_found ERROR [  6%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_create_framework_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_create_framework_duplicate_name ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_update_framework_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_delete_framework_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_requirements ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_controls ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_map_frameworks_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_import_framework_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_export_framework_success ERROR [  7%]
tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_statistics ERROR [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_list_integrations_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_by_id_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_integration_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_integration_duplicate FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_update_integration_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_delete_integration_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_test_integration_connection_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_test_integration_connection_failure FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_sync_integration_data_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_logs_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_oauth_callback_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_refresh_integration_token_success FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_available_integrations FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_webhooks FAILED [  7%]
tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_webhook_success FAILED [  7%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_list_policies_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_list_policies_with_filters ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_get_policy_by_id_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_create_policy_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_generate_policy_with_ai ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_update_policy_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_submit_policy_for_approval ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_approve_policy_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_reject_policy_with_comments ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_get_policy_versions ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_clone_policy_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_delete_policy_success ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_export_policy_to_pdf ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_check_policy_compliance ERROR [  8%]
tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_search_policies ERROR [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_success PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_without_optional_fields FAILED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_service_error PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_list_policies_success PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_list_policies_empty PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_get_policy_success PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_get_policy_not_found PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_get_policy_unauthorized PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_with_validation_error FAILED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_database_error PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_list_policies_database_error PASSED [  8%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_with_large_requirements PASSED [  9%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_response_structure PASSED [  9%]
tests/api/test_policies_router.py::TestPoliciesRouter::test_list_policies_with_pagination PASSED [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_generate_policy_ai ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_list_policy_templates ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_get_policy_template ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_create_policy_from_template ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_list_user_policies ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_get_policy_by_id ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_update_policy ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_delete_policy ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_validation ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_comparison ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_export ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_version_history ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_approval_workflow ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_publish ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_batch_policy_generation ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_ai_enhancement ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_translation ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_search ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_compliance_monitoring ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyIntegration::test_complete_policy_workflow ERROR [  9%]
tests/api/test_policy_endpoints.py::TestPolicyIntegration::test_multi_framework_policy_generation ERROR [  9%]
tests/api/test_reports_endpoints.py::TestReportGenerationEndpoints::test_generate_compliance_report_success PASSED [  9%]
tests/api/test_reports_endpoints.py::TestReportGenerationEndpoints::test_generate_assessment_report PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportGenerationEndpoints::test_generate_executive_summary PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportGenerationEndpoints::test_generate_audit_trail_report PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportGenerationEndpoints::test_generate_risk_assessment_report PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportRetrievalEndpoints::test_get_report_by_id PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportRetrievalEndpoints::test_list_all_reports PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportRetrievalEndpoints::test_filter_reports_by_type PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportRetrievalEndpoints::test_filter_reports_by_date_range PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportRetrievalEndpoints::test_search_reports PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportTemplateEndpoints::test_get_report_templates PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportTemplateEndpoints::test_create_custom_template PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportTemplateEndpoints::test_update_template PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportTemplateEndpoints::test_preview_template PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportSchedulingEndpoints::test_schedule_recurring_report PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportSchedulingEndpoints::test_update_report_schedule PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportSchedulingEndpoints::test_pause_report_schedule PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportSchedulingEndpoints::test_get_scheduled_reports PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportExportEndpoints::test_export_report_as_pdf PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportExportEndpoints::test_export_report_as_excel PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportExportEndpoints::test_export_report_as_json PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportExportEndpoints::test_batch_export_reports PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportDeliveryEndpoints::test_email_report PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportDeliveryEndpoints::test_share_report_link PASSED [ 10%]
tests/api/test_reports_endpoints.py::TestReportDeliveryEndpoints::test_webhook_delivery PASSED [ 10%]
tests/api/test_reports_router.py::TestReportsRouter::test_generate_report_success FAILED [ 10%]
tests/api/test_reports_router.py::TestReportsRouter::test_generate_report_async_processing FAILED [ 10%]
tests/api/test_reports_router.py::TestReportsRouter::test_get_report_by_id_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_get_report_not_found FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_list_reports_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_list_reports_with_filters FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_download_report_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_delete_report_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_schedule_report_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_get_report_status_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_export_report_csv FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_get_report_templates_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_preview_report_success FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_batch_generate_reports FAILED [ 11%]
tests/api/test_reports_router.py::TestReportsRouter::test_share_report_success FAILED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_create_user_success PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_create_user_duplicate_email PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_get_user_by_id PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_update_user_details PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_delete_user PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_list_all_users PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_filter_users_by_role PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserCRUDEndpoints::test_search_users PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserProfileEndpoints::test_get_user_profile PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserProfileEndpoints::test_update_user_profile PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserProfileEndpoints::test_update_notification_preferences PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserProfileEndpoints::test_upload_avatar PASSED [ 11%]
tests/api/test_user_management_endpoints.py::TestUserAuthenticationEndpoints::test_change_password PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserAuthenticationEndpoints::test_reset_password_request PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserAuthenticationEndpoints::test_enable_mfa PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserAuthenticationEndpoints::test_disable_mfa PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserAuthenticationEndpoints::test_verify_mfa_code PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserPermissionEndpoints::test_get_user_permissions PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserPermissionEndpoints::test_update_user_permissions PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserPermissionEndpoints::test_assign_role_to_user PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserPermissionEndpoints::test_check_user_permission PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserTeamEndpoints::test_get_user_teams PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserTeamEndpoints::test_add_user_to_team PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserTeamEndpoints::test_remove_user_from_team PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserTeamEndpoints::test_make_team_lead PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserActivityEndpoints::test_get_user_activity_log PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserActivityEndpoints::test_get_user_login_history PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserActivityEndpoints::test_get_user_sessions PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserActivityEndpoints::test_terminate_user_session PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserBulkOperations::test_bulk_create_users PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserBulkOperations::test_bulk_update_users PASSED [ 12%]
tests/api/test_user_management_endpoints.py::TestUserBulkOperations::test_bulk_deactivate_users PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_create_webhook_success PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_get_webhook_by_id PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_update_webhook PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_delete_webhook PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_list_webhooks PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookCRUDEndpoints::test_filter_active_webhooks PASSED [ 12%]
tests/api/test_webhook_endpoints.py::TestWebhookConfigurationEndpoints::test_update_webhook_headers PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookConfigurationEndpoints::test_update_retry_configuration PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookConfigurationEndpoints::test_regenerate_webhook_secret PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookConfigurationEndpoints::test_validate_webhook_url PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookTestingEndpoints::test_send_test_webhook PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookTestingEndpoints::test_test_webhook_failure PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookTestingEndpoints::test_simulate_webhook_events PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookDeliveryEndpoints::test_get_webhook_deliveries PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookDeliveryEndpoints::test_filter_failed_deliveries PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookDeliveryEndpoints::test_retry_failed_delivery PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookDeliveryEndpoints::test_bulk_retry_failed_deliveries PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookDeliveryEndpoints::test_get_delivery_statistics PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookEventSubscriptionEndpoints::test_get_available_events PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookEventSubscriptionEndpoints::test_subscribe_to_events PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookEventSubscriptionEndpoints::test_unsubscribe_from_events PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookEventSubscriptionEndpoints::test_get_event_history PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookSecurityEndpoints::test_validate_webhook_signature PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookSecurityEndpoints::test_generate_webhook_signature PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookSecurityEndpoints::test_webhook_ip_whitelist FAILED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookSecurityEndpoints::test_webhook_rate_limiting PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookMonitoringEndpoints::test_get_webhook_health PASSED [ 13%]
tests/api/test_webhook_endpoints.py::TestWebhookMonitoringEndpoints::test_set_webhook_alerts PASSED [ 13%]
tests/database/test_freemium_models.py::TestAssessmentLead::test_create_assessment_lead_minimal ERROR [ 13%]
tests/database/test_freemium_models.py::TestAssessmentLead::test_create_assessment_lead_with_utm ERROR [ 13%]
tests/database/test_freemium_models.py::TestAssessmentLead::test_assessment_lead_email_unique_constraint ERROR [ 13%]
tests/database/test_freemium_models.py::TestAssessmentLead::test_assessment_lead_score_update ERROR [ 14%]
tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_create_assessment_session ERROR [ 14%]
tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_assessment_session_ai_responses_storage ERROR [ 14%]
tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_assessment_session_expiration ERROR [ 14%]
tests/database/test_freemium_models.py::TestAIQuestionBank::test_create_ai_question ERROR [ 14%]
tests/database/test_freemium_models.py::TestAIQuestionBank::test_ai_question_weighting_and_difficulty ERROR [ 14%]
tests/database/test_freemium_models.py::TestLeadScoringEvent::test_create_lead_scoring_event ERROR [ 14%]
tests/database/test_freemium_models.py::TestLeadScoringEvent::test_lead_scoring_with_metadata ERROR [ 14%]
tests/database/test_freemium_models.py::TestConversionEvent::test_create_conversion_event ERROR [ 14%]
tests/database/test_freemium_models.py::TestFreemiumModelRelationships::test_lead_to_sessions_relationship ERROR [ 14%]
tests/database/test_freemium_models.py::TestFreemiumModelRelationships::test_cascade_delete_behavior ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_create_entity ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_get_by_id ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_get_all_with_pagination ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_update_entity ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_delete_entity ERROR [ 14%]
tests/database/test_repositories.py::TestBaseRepository::test_count_entities ERROR [ 14%]
tests/database/test_repositories.py::TestAssessmentRepository::test_get_by_organization ERROR [ 14%]
tests/database/test_repositories.py::TestAssessmentRepository::test_get_by_status ERROR [ 14%]
tests/database/test_repositories.py::TestAssessmentRepository::test_get_with_framework ERROR [ 14%]
tests/database/test_repositories.py::TestAssessmentRepository::test_update_progress ERROR [ 14%]
tests/database/test_repositories.py::TestPolicyRepository::test_get_approved_policies ERROR [ 14%]
tests/database/test_repositories.py::TestPolicyRepository::test_get_by_category ERROR [ 14%]
tests/database/test_repositories.py::TestPolicyRepository::test_search_policies ERROR [ 14%]
tests/database/test_repositories.py::TestPolicyRepository::test_clone_policy ERROR [ 14%]
tests/database/test_repositories.py::TestEvidenceRepository::test_get_by_assessment ERROR [ 14%]
tests/database/test_repositories.py::TestEvidenceRepository::test_get_validated_evidence ERROR [ 15%]
tests/database/test_repositories.py::TestEvidenceRepository::test_validate_evidence ERROR [ 15%]
tests/database/test_repositories.py::TestEvidenceRepository::test_check_expiry ERROR [ 15%]
tests/database/test_repositories.py::TestUserRepository::test_get_by_email ERROR [ 15%]
tests/database/test_repositories.py::TestUserRepository::test_get_active_users ERROR [ 15%]
tests/database/test_repositories.py::TestUserRepository::test_update_last_login ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_create_user_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_create_user_duplicate_email ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_id_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_id_not_found ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_email_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_email_case_insensitive ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_update_user_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_update_user_password ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_delete_user_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_delete_user_not_found ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_verify_user_email ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_list_users_with_pagination ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_list_users_with_filter ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_authenticate_user_success ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_authenticate_user_wrong_password ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_authenticate_inactive_user ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_update_last_login ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_search_users_by_name ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_count_users ERROR [ 15%]
tests/database/test_user_repository.py::TestUserRepository::test_bulk_create_users ERROR [ 16%]
tests/database/test_user_repository.py::TestUserRepository::test_database_error_handling ERROR [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_complete_user_onboarding_workflow ERROR [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_with_assessment_restart 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:20 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_with_minimal_data 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:20 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_error_recovery 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:20 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_triggers_background_tasks 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:20 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_creates_audit_trail ERROR [ 16%]
tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_sets_user_preferences 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:20 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_authentication_required ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_invalid_framework ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_followup_questions_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_analysis_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_recommendations_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_feedback_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_metrics_endpoint_success ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_service_timeout_handling ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_quota_exceeded_handling ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_content_filter_handling ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_invalid_request_data_validation ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_business_profile_not_found ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_ai_help_rate_limiting ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_ai_analysis_rate_limiting ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_regular_endpoints_higher_rate_limit ERROR [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIErrorHandling::test_ai_service_unavailable SKIPPED [ 16%]
tests/integration/api/test_ai_assessments.py::TestAIErrorHandling::test_malformed_ai_response_handling SKIPPED [ 17%]
tests/integration/api/test_ai_assessments.py::TestAIErrorHandling::test_ai_response_validation_errors SKIPPED [ 17%]
tests/integration/api/test_ai_assessments.py::TestAIErrorHandling::test_concurrent_ai_requests_handling ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_analytics_dashboard_success ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_usage_analytics_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_cost_analytics_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_system_alerts_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_resolve_alert_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_resolve_nonexistent_alert ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestPerformanceEndpoints::test_performance_metrics_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestPerformanceEndpoints::test_optimize_performance_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_cache_metrics_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_clear_cache_endpoint ERROR [ 17%]
tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_analytics_error_handling ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_create_conversation ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_send_message_to_conversation ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_get_evidence_recommendations ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_compliance_analysis ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_get_conversations_list ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_conversation_not_found ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_compliance_analysis_missing_business_profile ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_invalid_framework_for_analysis ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_missing_message_content ERROR [ 17%]
tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_ai_assistant_error_handling ERROR [ 17%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_context_aware_recommendations_success ERROR [ 17%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_evidence_collection_workflow_generation ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_policy_generation ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_smart_compliance_guidance ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_missing_business_profile_error ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_ai_service_error_handling ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_invalid_framework_parameter ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_workflow_generation_parameter_validation ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_policy_generation_parameter_validation ERROR [ 18%]
tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_smart_guidance_parameter_validation ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_single_evidence ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_evidence_force_reclassify ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_bulk_classify_evidence ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_get_control_mapping_suggestions ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_get_classification_statistics ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_nonexistent_evidence ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_bulk_classify_with_invalid_evidence ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_bulk_classify_too_many_items ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_invalid_confidence_threshold ERROR [ 18%]
tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_classification_ai_service_error ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_success ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_validation_error ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_unauthenticated ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_success ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_empty ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_with_filtering ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_by_id_success ERROR [ 18%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_by_id_not_found ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_unauthorized_access ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_item_success ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_item_partial ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_item_success ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_item_unauthorized ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_bulk_update_evidence_status ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_statistics ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_search_evidence_items ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_validate_evidence_quality ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_identify_evidence_requirements ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_configure_evidence_automation ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidencePaginationAndSorting::test_evidence_pagination ERROR [ 19%]
tests/integration/api/test_evidence_endpoints.py::TestEvidencePaginationAndSorting::test_evidence_sorting ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_success ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_invalid_format 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_missing_consent 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_duplicate ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_rate_limiting 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_performance 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_success ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_invalid_token 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:21 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_expired_token FAILED [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_ai_service_error ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_resume_existing ERROR [ 19%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_success ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_invalid_question_id ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_assessment_complete ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_validation_error ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_ai_error_fallback ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_success ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_invalid_token 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_incomplete_assessment ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_cached ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_performance ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_success ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_duplicate_event ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_invalid_event_type ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_sql_injection_prevention 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_xss_prevention ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_oversized_payload_rejection ERROR [ 20%]
tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_token_expiration_security FAILED [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_invalid_request 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_unauthenticated 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_rate_limiting 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_store_memory_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_retrieve_memories_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_graph_initialization_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_health_check_healthy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_health_check_degraded 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 20%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_status_endpoint 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_status_endpoint_degraded 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_iq_agent_service_unavailable 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_ai_service_error 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_memory_storage_error_handling 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_concurrent_queries 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_large_query_handling 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_query_with_special_characters 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_background_task_execution 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 21%]
tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentLoadTesting::test_sustained_query_load ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_evidence_quality_analysis ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_detect_evidence_duplicates ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_batch_duplicate_detection ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_quality_benchmark ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_quality_trends ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_quality_analysis_nonexistent_evidence ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_duplicate_detection_insufficient_evidence ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_duplicate_detection_invalid_threshold ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_batch_duplicate_detection_too_many_items ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_quality_trends_invalid_days ERROR [ 21%]
tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_quality_analysis_ai_service_error ERROR [ 21%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_timeout_fallback ERROR [ 21%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_quota_exceeded_fallback ERROR [ 21%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_content_filter_handling ERROR [ 21%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_model_error_fallback ERROR [ 21%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_parsing_error_handling ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_network_error_fallback ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_multiple_ai_service_failures ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_partial_ai_service_degradation ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_recovery_after_failure ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_logging_and_monitoring ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_fallback_to_mock_data ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_context_preservation ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_circuit_breaker_pattern ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_graceful_shutdown ERROR [ 22%]
tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_rate_monitoring ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_analysis_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_recommendations_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_help_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_circuit_breaker_status_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_selection_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_with_circuit_breaker_open ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_fallback_chain ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_error_handling ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_performance_metrics_endpoint ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_concurrent_streaming_requests ERROR [ 22%]
tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_health_check_endpoint ERROR [ 22%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_create_assessment_workflow PASSED [ 22%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_with_ai_recommendations PASSED [ 22%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_failure_handling PASSED [ 22%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_concurrent_assessments PASSED [ 22%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_state_transitions PASSED [ 23%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_rollback_on_error FAILED [ 23%]
tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_caching PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_user_registration_flow PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_login_flow_with_jwt PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_token_refresh_flow PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_rbac_permission_check FAILED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_password_reset_flow PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_multi_factor_authentication PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_session_management PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_role_based_access_control PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_oauth_integration PASSED [ 23%]
tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_api_key_authentication PASSED [ 23%]
tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_endpoint_success ERROR [ 23%]
tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_endpoint_authentication_required ERROR [ 23%]
tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_includes_performance_data ERROR [ 23%]
tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_with_ai_activity ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_complete_authentication_workflow 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_compliance_assessment_pipeline_integration ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_evidence_collection_workflow_integration ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_ai_service_circuit_breaker_integration ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_cross_service_data_consistency ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_concurrent_api_operations_integration ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_rate_limiting_integration_across_endpoints ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_error_handling_integration_across_services ERROR [ 23%]
tests/integration/test_comprehensive_api_workflows.py::TestAPIWorkflowPerformance::test_assessment_workflow_performance ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_openapi_schema_generation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_assessment_endpoint_contract_validation ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_authentication_endpoint_contract_validation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_iq_agent_endpoint_contract_validation ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_evidence_upload_contract_validation ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_error_response_contract_consistency 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:22 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_field_mapper_contract_validation ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_pagination_contract_validation ERROR [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_api_versioning_contract 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 24%]
tests/integration/test_contract_validation.py::TestAPIContractValidation::test_cors_contract_validation PASSED [ 24%]
tests/integration/test_contract_validation.py::TestContractPerformance::test_schema_validation_performance ERROR [ 24%]
tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_authentication_contract_security 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 24%]
tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_input_sanitization_contract ERROR [ 24%]
tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_rate_limiting_contract_headers ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_environment_is_test PASSED [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_connection ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_transaction_rollback ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_no_persistence_between_tests ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_table_creation ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_connection ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_expiration ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_data_types ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_async_postgres_connection ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_connection_pool_settings ERROR [ 24%]
tests/integration/test_database_connections.py::TestDatabaseConnections::test_fixtures_work_together ERROR [ 24%]
tests/integration/test_database_connections.py::TestConnectionResilience::test_postgres_connection_recovery ERROR [ 25%]
tests/integration/test_database_connections.py::TestConnectionResilience::test_redis_connection_recovery ERROR [ 25%]
tests/integration/test_database_connections.py::TestMockFixtures::test_mock_redis_client ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_full_evidence_and_reporting_flow ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_ai_assistant_evidence_query ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_scheduled_report_generation ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestAPIEndpointsIntegration::test_business_profile_to_evidence_workflow ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestAPIEndpointsIntegration::test_framework_to_readiness_assessment ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestErrorHandlingAndResilience::test_integration_failure_handling ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestErrorHandlingAndResilience::test_report_generation_with_no_data ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestAsyncOperations::test_async_evidence_collection ERROR [ 25%]
tests/integration/test_evidence_flow.py::TestAsyncOperations::test_ai_assistant_async_processing ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_gemini_integration_with_circuit_breaker ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_ai_service_circuit_breaker_states ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_ai_service_timeout_handling ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_multiple_ai_provider_fallback ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_connection_pool_behavior ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_transaction_consistency ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_failover_behavior ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_session_management 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 25%]
tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_caching_behavior ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_cache_invalidation ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_connection_health SKIPPED [ 25%]
tests/integration/test_external_service_integration.py::TestEmailServiceIntegration::test_password_reset_email_integration 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 25%]
tests/integration/test_external_service_integration.py::TestEmailServiceIntegration::test_assessment_completion_notification ERROR [ 25%]
tests/integration/test_external_service_integration.py::TestFileStorageIntegration::test_file_upload_storage_integration ERROR [ 26%]
tests/integration/test_external_service_integration.py::TestFileStorageIntegration::test_file_processing_integration ERROR [ 26%]
tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_companies_house_api_integration ERROR [ 26%]
tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_external_api_timeout_handling ERROR [ 26%]
tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_external_api_rate_limiting_respect ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_user_registration_flow ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_user_login_flow ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_access ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_token_refresh_flow ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_logout_flow ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_invalid_credentials_rejection ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_nonexistent_user_rejection ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_duplicate_registration_rejection ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_without_token ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_with_invalid_token ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestBusinessProfileIntegration::test_business_profile_access_with_auth ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestBusinessProfileIntegration::test_business_profile_access_without_auth ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestRAGSystemIntegration::test_chat_endpoint_with_auth ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestRAGSystemIntegration::test_chat_endpoint_without_auth ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestAuthenticationSystemIntegration::test_complete_user_journey ERROR [ 26%]
tests/integration/test_jwt_auth_integration.py::TestAuthenticationSystemIntegration::test_authentication_system_health ERROR [ 26%]
tests/integration/test_state_integration.py::TestStateWithLangGraph::test_state_as_graph_state PASSED [ 26%]
tests/integration/test_state_integration.py::TestStateWithLangGraph::test_state_with_conditional_edges PASSED [ 26%]
tests/integration/test_state_integration.py::TestStateWithLangGraph::test_parallel_node_execution PASSED [ 26%]
tests/integration/test_state_integration.py::TestReducerIntegration::test_evidence_accumulation_reducer PASSED [ 26%]
tests/integration/test_state_integration.py::TestReducerIntegration::test_decision_merge_reducer PASSED [ 26%]
tests/integration/test_state_integration.py::TestReducerIntegration::test_cost_tracker_reducer PASSED [ 27%]
tests/integration/test_state_integration.py::TestReducerIntegration::test_memory_append_reducer PASSED [ 27%]
tests/integration/test_state_integration.py::TestComplexWorkflow::test_full_compliance_workflow PASSED [ 27%]
tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_complete_registration_flow FAILED [ 27%]
tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_registration_with_existing_email FAILED [ 27%]
tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_registration_with_weak_password 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 27%]
tests/integration/test_user_workflows.py::TestComplianceAssessmentWorkflow::test_complete_assessment_flow 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 27%]
tests/integration/test_user_workflows.py::TestPolicyGenerationWorkflow::test_complete_policy_workflow 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:23 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 27%]
tests/integration/test_user_workflows.py::TestIntegrationManagementWorkflow::test_slack_integration_workflow FAILED [ 27%]
tests/integration/test_user_workflows.py::TestReportingWorkflow::test_scheduled_report_workflow FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_security_headers FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_csp_header FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_hsts_header FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_referrer_policy_header FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_permissions_policy_header FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_remove_server_header FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_cors_headers_for_api FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_rate_limiting FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_rate_limit_exceeded FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_xss_protection FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_sql_injection_protection FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_path_traversal_protection FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_request_size_limit FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_jwt_validation_success FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_jwt_validation_failure FAILED [ 27%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_ip_whitelist_check FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_ip_blacklist_check FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_csrf_token_validation FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_csrf_token_mismatch FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_secure_cookie_handling FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_request_logging FAILED [ 28%]
tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_security_headers_cascade FAILED [ 28%]
tests/models/test_compliance_state.py::TestComplianceStateInitialization::test_minimal_state_creation PASSED [ 28%]
tests/models/test_compliance_state.py::TestComplianceStateInitialization::test_full_state_creation PASSED [ 28%]
tests/models/test_compliance_state.py::TestActorValidation::test_valid_actor_types PASSED [ 28%]
tests/models/test_compliance_state.py::TestActorValidation::test_invalid_actor_type_raises_error PASSED [ 28%]
tests/models/test_compliance_state.py::TestActorValidation::test_actor_type_case_sensitive PASSED [ 28%]
tests/models/test_compliance_state.py::TestEvidenceAccumulation::test_evidence_initialization_empty PASSED [ 28%]
tests/models/test_compliance_state.py::TestEvidenceAccumulation::test_evidence_accumulation_preserves_existing FAILED [ 28%]
tests/models/test_compliance_state.py::TestEvidenceAccumulation::test_evidence_item_validation PASSED [ 28%]
tests/models/test_compliance_state.py::TestCostTracking::test_cost_tracker_initialization PASSED [ 28%]
tests/models/test_compliance_state.py::TestCostTracking::test_cost_tracker_update FAILED [ 28%]
tests/models/test_compliance_state.py::TestCostTracking::test_cost_accumulation FAILED [ 28%]
tests/models/test_compliance_state.py::TestMemoryPersistence::test_memory_initialization PASSED [ 28%]
tests/models/test_compliance_state.py::TestMemoryPersistence::test_episodic_memory_append PASSED [ 28%]
tests/models/test_compliance_state.py::TestMemoryPersistence::test_semantic_memory_update PASSED [ 28%]
tests/models/test_compliance_state.py::TestDecisionTracking::test_decisions_initialization PASSED [ 28%]
tests/models/test_compliance_state.py::TestDecisionTracking::test_decision_structure FAILED [ 28%]
tests/models/test_compliance_state.py::TestDecisionTracking::test_decision_accumulation PASSED [ 28%]
tests/models/test_compliance_state.py::TestStateSerialization::test_state_to_json PASSED [ 28%]
tests/models/test_compliance_state.py::TestStateSerialization::test_state_from_json PASSED [ 28%]
tests/models/test_compliance_state.py::TestStateSerialization::test_datetime_serialization PASSED [ 29%]
tests/models/test_compliance_state.py::TestTraceIdGeneration::test_trace_id_required PASSED [ 29%]
tests/models/test_compliance_state.py::TestTraceIdGeneration::test_trace_id_format_validation PASSED [ 29%]
tests/models/test_compliance_state.py::TestTraceIdGeneration::test_trace_id_uniqueness PASSED [ 29%]
tests/models/test_compliance_state.py::TestContextValidation::test_context_structure FAILED [ 29%]
tests/models/test_compliance_state.py::TestContextValidation::test_context_optional PASSED [ 29%]
tests/models/test_compliance_state.py::TestContextValidation::test_context_framework_validation PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateTransitions::test_workflow_status_values PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateTransitions::test_invalid_workflow_status PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateTransitions::test_state_transition_tracking PASSED [ 29%]
tests/models/test_compliance_state.py::TestPerformanceMetrics::test_node_execution_times FAILED [ 29%]
tests/models/test_compliance_state.py::TestPerformanceMetrics::test_retry_and_error_counts PASSED [ 29%]
tests/models/test_compliance_state.py::TestPerformanceMetrics::test_counter_defaults PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateValidation::test_case_id_required PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateValidation::test_objective_required PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateValidation::test_empty_strings_validation PASSED [ 29%]
tests/models/test_compliance_state.py::TestStateValidation::test_type_coercion FAILED [ 29%]
tests/models/test_compliance_state.py::TestLangGraphIntegration::test_state_with_langgraph_typeddict PASSED [ 29%]
tests/models/test_compliance_state.py::TestLangGraphIntegration::test_state_reducer_functions PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_execution_start PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_execution_completion PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_execution_failure PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_execution_timeout PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_concurrent_node_executions PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_metrics_aggregation PASSED [ 29%]
tests/monitoring/test_langgraph_metrics.py::TestWorkflowMetricsTracker::test_workflow_lifecycle PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestWorkflowMetricsTracker::test_workflow_failure_tracking PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestWorkflowMetricsTracker::test_concurrent_workflow_tracking PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestWorkflowMetricsTracker::test_workflow_performance_metrics PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestStateTransitionTracker::test_basic_state_transition PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestStateTransitionTracker::test_state_transition_history PASSED [ 30%]
tests/monitoring/test_langgraph_metrics.py::TestStateTransitionTracker::test_complex_state_machine FAILED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsCollector::test_basic_functionality PASSED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsCollector::test_system_metrics_collection PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_metrics_collection_functionality PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_test_statistics_calculation PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_resource_monitoring_during_test PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_performance_metric_tracking PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_error_rate_tracking PASSED [ 30%]
tests/monitoring/test_metrics.py::TestObservability::test_test_duration_trends PASSED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsReporting::test_generate_test_report PASSED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsReporting::test_metrics_export_format PASSED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsReporting::test_performance_threshold_detection PASSED [ 30%]
tests/monitoring/test_metrics.py::TestMetricsReporting::test_trend_analysis PASSED [ 30%]
tests/monitoring/test_metrics.py::TestCoverageTracking::test_coverage_metric_structure PASSED [ 30%]
tests/monitoring/test_metrics.py::TestCoverageTracking::test_coverage_trend_tracking PASSED [ 30%]
tests/monitoring/test_metrics.py::TestCoverageTracking::test_coverage_threshold_compliance PASSED [ 30%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_collector_initialization PASSED [ 30%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_counter_creation_and_increment 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 30%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_histogram_creation_and_recording 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 30%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_updown_counter_creation 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 30%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_observable_gauge_creation 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_batch_metric_recording 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestOpenTelemetryMetricsCollector::test_metric_labels_and_attributes 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_exporter_initialization 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
FAILED                                                                   [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_exporter_initialization ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_prometheus_format_conversion ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_prometheus_http_endpoint ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_histogram_buckets_configuration ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_metric_name_sanitization ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_multi_collector_aggregation ERROR [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_node_execution_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_workflow_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_error_tracking_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_state_transition_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_checkpoint_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:31 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_message_queue_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:32 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_memory_usage_metrics 
-------------------------------- live log setup --------------------------------
2025-09-05 22:29:32 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
ERROR                                                                    [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_bridge_initialization FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_counter_synchronization FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_histogram_synchronization FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_gauge_synchronization FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_export_to_prometheus FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_automatic_langgraph_instrumentation FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_end_to_end_metrics_pipeline FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_backwards_compatibility FAILED [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_metrics_persistence_across_restarts 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:32 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
FAILED                                                                   [ 31%]
tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_performance_overhead 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:32 [ WARNING] opentelemetry.metrics._internal: Overriding of current MeterProvider is not allowed
PASSED                                                                   [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusFormatter::test_counter_formatting PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusFormatter::test_gauge_formatting PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusFormatter::test_histogram_formatting PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusMetricsExporter::test_register_counter PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusMetricsExporter::test_register_gauge PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusMetricsExporter::test_register_histogram PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestPrometheusMetricsExporter::test_export_metrics PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestMetricsHTTPServer::test_server_startup PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestMetricsHTTPServer::test_metrics_endpoint PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestLabelSanitizer::test_valid_label PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestLabelSanitizer::test_invalid_characters PASSED [ 32%]
tests/monitoring/test_prometheus_exporter.py::TestLabelSanitizer::test_sanitize_label_dict PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_model_selection_performance PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_circuit_breaker_performance PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_streaming_performance PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_concurrent_streaming_performance PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_model_fallback_performance FAILED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_memory_usage_streaming FAILED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_circuit_breaker_overhead PASSED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_cost_optimization_simulation FAILED [ 32%]
tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_end_to_end_performance PASSED [ 32%]
tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_help_response_time_under_threshold ERROR [ 32%]
tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_help_concurrent_requests_performance ERROR [ 32%]
tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_analysis_performance_with_large_dataset ERROR [ 32%]
tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_service_timeout_handling ERROR [ 32%]
tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_caching_improves_performance ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAIRateLimiting::test_ai_help_rate_limit_enforcement ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAIRateLimiting::test_ai_analysis_stricter_rate_limit ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAIRateLimiting::test_regular_endpoints_higher_rate_limits ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAIRateLimiting::test_rate_limit_headers_present ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAIRateLimiting::test_rate_limit_reset_after_window ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAILoadTesting::test_ai_endpoint_load_capacity ERROR [ 33%]
tests/performance/test_ai_performance.py::TestAILoadTesting::test_ai_memory_usage_under_load ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_authentication_performance 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:32 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_creation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_creation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_search_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_search_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_dashboard_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_dashboard_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_ai_chat_performance SKIPPED [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_concurrent_request_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_bulk_operation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestAPIPerformance::test_bulk_operation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestMemoryPerformance::test_large_dataset_handling ERROR [ 33%]
tests/performance/test_api_performance.py::TestMemoryPerformance::test_concurrent_memory_usage ERROR [ 33%]
tests/performance/test_api_performance.py::TestDatabasePerformance::test_complex_query_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestDatabasePerformance::test_complex_query_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestDatabasePerformance::test_aggregation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestDatabasePerformance::test_aggregation_performance ERROR [ 33%]
tests/performance/test_api_performance.py::TestEndToEndPerformance::test_complete_onboarding_performance 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 33%]
tests/performance/test_api_performance.py::TestRealWorldScenarios::test_daily_user_workflow ERROR [ 33%]
tests/performance/test_api_performance.py::TestRealWorldScenarios::test_peak_usage_simulation ERROR [ 33%]
tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_evidence_query_scaling ERROR [ 33%]
tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_full_text_search_performance ERROR [ 33%]
tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_aggregation_query_performance ERROR [ 33%]
tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_join_query_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_connection_pool_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_transaction_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_bulk_operation_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseIndexPerformance::test_indexed_query_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseIndexPerformance::test_unindexed_query_performance ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseConcurrencyPerformance::test_concurrent_read_performance SKIPPED [ 34%]
tests/performance/test_database_performance.py::TestDatabaseConcurrencyPerformance::test_concurrent_write_performance SKIPPED [ 34%]
tests/performance/test_database_performance.py::TestDatabaseResourceUsage::test_memory_usage_optimization ERROR [ 34%]
tests/performance/test_database_performance.py::TestDatabaseResourceUsage::test_connection_cleanup ERROR [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimizer_initialization FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_n1_query_prevention FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_database_indexes_migration FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_caching_implementation FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_batch_operations FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_memory_leak_prevention FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_performance_monitoring FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_connection_pooling FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_slow_query_detection FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[select_related-JOIN] FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[prefetch_related-IN] FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[batch_select-BATCH] FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_index_coverage FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_composite_indexes FAILED [ 34%]
tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_index_performance_impact FAILED [ 34%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_unauthenticated_access_denied ERROR [ 34%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_invalid_token_rejected ERROR [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_expired_token_handling ERROR [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_token_without_bearer_prefix ERROR [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_malformed_authorization_header ERROR [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_password_strength_validation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_strong_password_acceptance 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_account_lockout_protection SKIPPED [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_email_enumeration_protection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_timing_attack_protection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:33 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_session_management_security 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_password_change_security 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthenticationSecurity::test_concurrent_session_limits 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestTokenSecurity::test_token_contains_no_sensitive_data 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestTokenSecurity::test_token_expiry_enforcement 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestTokenSecurity::test_token_signature_validation ERROR [ 35%]
tests/security/test_authentication.py::TestTokenSecurity::test_token_algorithm_confusion ERROR [ 35%]
tests/security/test_authentication.py::TestAuthorizationSecurity::test_user_data_isolation ERROR [ 35%]
tests/security/test_authentication.py::TestAuthorizationSecurity::test_privilege_escalation_protection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthorizationSecurity::test_role_based_access_control 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthorizationSecurity::test_api_rate_limiting_by_user 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_authentication.py::TestAuthorizationSecurity::test_cross_origin_request_security 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 35%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_jwt_secret_key_protection FAILED [ 35%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_input_sanitization FAILED [ 35%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_file_upload_security FAILED [ 35%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_rate_limiting 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:34 [   ERROR] api.main: Database error: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
FAILED                                                                   [ 35%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_sensitive_data_redaction FAILED [ 36%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_admin_authentication 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 36%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_credential_encryption 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:34 [ WARNING] core.security.credential_encryption: Generated new deployment salt for default-deployment - store securely in DEPLOYMENT_SALT env var
FAILED                                                                   [ 36%]
tests/security/test_security_fixes.py::TestSecurityFixes::test_sql_injection_prevention FAILED [ 36%]
tests/security/test_security_fixes.py::TestPerformanceFixes::test_database_indexes PASSED [ 36%]
tests/security/test_security_fixes.py::TestPerformanceFixes::test_n1_query_prevention FAILED [ 36%]
tests/security/test_security_fixes.py::TestPerformanceFixes::test_caching_implementation PASSED [ 36%]
tests/security/test_security_fixes.py::TestIntegrationFixes::test_google_workspace_integration FAILED [ 36%]
tests/security/test_security_fixes.py::TestIntegrationFixes::test_error_handling_consistency PASSED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_init_assistant PASSED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_get_task_appropriate_model_simple FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_get_task_appropriate_model_complex FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_get_task_appropriate_model_with_tools PASSED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_circuit_breaker_protection FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_compliance FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_assessment FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_with_cache FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_with_safety_check FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_unsafe_content FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_with_tools FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_stream_response FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_analyze_compliance_gap FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_performance_optimization FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_quality_monitoring FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_analytics_tracking FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_context_management FAILED [ 36%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_error_handling_model_unavailable FAILED [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_error_handling_database_error FAILED [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_batch_processing FAILED [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistant::test_retry_mechanism FAILED [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_full_workflow_assessment ERROR [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_caching_performance ERROR [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_concurrent_requests ERROR [ 37%]
tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_rate_limiting ERROR [ 37%]
tests/services/ai/test_tools.py::TestBaseTool::test_base_tool_interface FAILED [ 37%]
tests/services/ai/test_tools.py::TestBaseTool::test_custom_tool_implementation PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_register_tool PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_register_duplicate_tool PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_get_tool PASSED  [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_get_nonexistent_tool PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_list_tools PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_list_tools_by_type PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_get_schemas PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolRegistry::test_clear_registry PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_tool_success PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_nonexistent_tool PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_tool_with_error PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_with_validation PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_batch PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolExecutor::test_execute_with_timeout PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolResult::test_success_result PASSED [ 37%]
tests/services/ai/test_tools.py::TestToolResult::test_error_result PASSED [ 38%]
tests/services/ai/test_tools.py::TestToolResult::test_result_to_dict PASSED [ 38%]
tests/services/ai/test_tools.py::TestToolResult::test_result_from_dict PASSED [ 38%]
tests/services/ai/test_tools.py::TestModuleFunctions::test_register_tool_function FAILED [ 38%]
tests/services/ai/test_tools.py::TestModuleFunctions::test_execute_tool_function FAILED [ 38%]
tests/services/ai/test_tools.py::TestModuleFunctions::test_get_tool_schemas_function FAILED [ 38%]
tests/services/ai/test_tools.py::TestToolsIntegration::test_complete_tool_workflow PASSED [ 38%]
tests/services/ai/test_tools.py::TestToolsIntegration::test_concurrent_tool_execution PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_calculate_compliance_score PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_calculate_compliance_score_weighted PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_identify_compliance_gaps FAILED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_generate_recommendations PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_create_assessment FAILED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_update_assessment_response FAILED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_validate_evidence PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_get_framework_requirements FAILED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_complete_assessment PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_export_assessment_report PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_benchmark_compliance PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_assessment_history PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_compliance_trends PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_requirement_prioritization PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceService::test_automated_evidence_mapping PASSED [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_full_assessment_workflow ERROR [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_concurrent_assessments ERROR [ 38%]
tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_performance_large_assessment ERROR [ 38%]
tests/services/test_email_service.py::TestEmailService::test_send_email FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_send_email_with_attachments FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_send_templated_email PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_send_bulk_emails PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_validate_email_address PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_validate_email_domain FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_create_email_campaign PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_track_email_open PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_track_email_click PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_handle_bounce PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_handle_soft_bounce PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_unsubscribe_user PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_check_subscription_status PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_resubscribe_user PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_queue_management PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_retry_logic FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_template_rendering FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_analytics FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_personalization PASSED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_spam_score_check FAILED [ 39%]
tests/services/test_email_service.py::TestEmailService::test_email_scheduling PASSED [ 39%]
tests/services/test_integration_service.py::TestIntegrationService::test_register_integration FAILED [ 39%]
tests/services/test_integration_service.py::TestIntegrationService::test_send_webhook FAILED [ 39%]
tests/services/test_integration_service.py::TestIntegrationService::test_webhook_retry_logic FAILED [ 39%]
tests/services/test_integration_service.py::TestIntegrationService::test_api_connector FAILED [ 39%]
tests/services/test_integration_service.py::TestIntegrationService::test_data_sync_service FAILED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_event_bus_publish PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_event_bus_subscribe PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_message_queue_operations PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_oauth_provider_authentication PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_oauth_token_exchange PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_websocket_connection FAILED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_websocket_message_broadcast PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_integration_health_check PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_batch_webhook_send PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_integration_rate_limiting FAILED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_transform_data_format PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_external_api_pagination PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_integration_error_handling PASSED [ 40%]
tests/services/test_integration_service.py::TestIntegrationService::test_integration_metrics_collection PASSED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_send_email_notification FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_send_sms_notification FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_send_push_notification FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_create_in_app_notification FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_get_notification_template FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_render_template FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_queue_notification FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_get_user_preferences FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_update_user_preferences FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_check_quiet_hours FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_log_notification_history FAILED [ 40%]
tests/services/test_notification_service.py::TestNotificationService::test_get_notification_history FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_bulk_send_notifications FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_retry_failed_notifications FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_notification_rate_limiting FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_notification_channel_selection FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_format_notification_content FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_schedule_notification FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_cancel_scheduled_notification FAILED [ 41%]
tests/services/test_notification_service.py::TestNotificationService::test_notification_analytics FAILED [ 41%]
tests/services/test_policy_service.py::TestPolicyService::test_generate_compliance_policy PASSED [ 41%]
tests/services/test_policy_service.py::TestPolicyService::test_get_policy_by_id PASSED [ 41%]
tests/services/test_policy_service.py::TestPolicyService::test_update_policy PASSED [ 41%]
tests/services/test_policy_service.py::TestPolicyService::test_delete_policy PASSED [ 41%]
tests/services/test_policy_service.py::TestPolicyService::test_validate_policy PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_generate_compliance_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_generate_assessment_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_export_report_pdf PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_export_report_excel PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_get_report_by_id PASSED [ 41%]
tests/services/test_report_service.py::TestReportService::test_get_nonexistent_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportScheduler::test_schedule_daily_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportScheduler::test_schedule_weekly_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportScheduler::test_cancel_scheduled_report PASSED [ 41%]
tests/services/test_report_service.py::TestReportScheduler::test_get_all_scheduled_reports PASSED [ 41%]
tests/services/test_security_service.py::TestSecurityService::test_run_security_audit PASSED [ 41%]
tests/services/test_security_service.py::TestSecurityService::test_report_vulnerability PASSED [ 42%]
tests/services/test_security_service.py::TestSecurityService::test_get_vulnerabilities_by_severity PASSED [ 42%]
tests/services/test_security_service.py::TestSecurityService::test_get_audit_results PASSED [ 42%]
tests/services/test_security_service.py::TestEncryptionService::test_encrypt_decrypt PASSED [ 42%]
tests/services/test_security_service.py::TestEncryptionService::test_password_hashing PASSED [ 42%]
tests/services/test_security_service.py::TestEncryptionService::test_decrypt_invalid_data PASSED [ 42%]
tests/services/test_security_service.py::TestAuditLogger::test_log_security_event PASSED [ 42%]
tests/services/test_security_service.py::TestAuditLogger::test_get_user_audit_logs PASSED [ 42%]
tests/services/test_security_service.py::TestAuditLogger::test_get_logs_by_event_type PASSED [ 42%]
tests/test-utility-scripts/debug_test.py::test_model_selection PASSED    [ 42%]
tests/test-utility-scripts/test_final_verification.py::test_database_connection ERROR [ 42%]
tests/test-utility-scripts/test_final_verification.py::test_user_creation ERROR [ 42%]
tests/test-utility-scripts/test_final_verification.py::test_business_profile ERROR [ 42%]
tests/test-utility-scripts/test_final_verification.py::test_authenticated_client 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:36 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 42%]
tests/test-utility-scripts/test_final_verification.py::test_evidence_service PASSED [ 42%]
tests/test_agentic_rag.py::test_agentic_rag PASSED                       [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIHelpEndpoint::test_ai_help_endpoint_integration PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIHelpEndpoint::test_ai_help_with_context PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIHelpEndpoint::test_ai_help_error_handling PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIAnalysisEndpoint::test_analyze_assessment_response PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIAnalysisEndpoint::test_batch_analysis PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIReportEndpoint::test_generate_ai_report PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIReportEndpoint::test_report_with_recommendations PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIIntegrationFlow::test_complete_ai_workflow PASSED [ 42%]
tests/test_ai_assessment_endpoints_integration.py::TestAIIntegrationFlow::test_ai_fallback_mechanism PASSED [ 42%]
tests/test_ai_cost_management.py::TestAIUsageMetrics::test_metrics_initialization FAILED [ 42%]
tests/test_ai_cost_management.py::TestAIUsageMetrics::test_metrics_aggregation FAILED [ 43%]
tests/test_ai_cost_management.py::TestAIUsageMetrics::test_cost_per_token_calculation FAILED [ 43%]
tests/test_ai_cost_management.py::TestAIUsageMetrics::test_efficiency_score_calculation FAILED [ 43%]
tests/test_ai_cost_management.py::TestModelCostConfig::test_gemini_cost_calculation FAILED [ 43%]
tests/test_ai_cost_management.py::TestModelCostConfig::test_openai_cost_calculation FAILED [ 43%]
tests/test_ai_cost_management.py::TestModelCostConfig::test_custom_model_config FAILED [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_track_usage ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_get_usage_by_service ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_get_usage_by_time_range ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_calculate_daily_costs ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_get_cost_trends ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostTrackingService::test_identify_cost_anomalies ERROR [ 43%]
tests/test_ai_cost_management.py::TestBudgetAlertService::test_set_daily_budget ERROR [ 43%]
tests/test_ai_cost_management.py::TestBudgetAlertService::test_budget_usage_tracking ERROR [ 43%]
tests/test_ai_cost_management.py::TestBudgetAlertService::test_budget_exceeded_alert ERROR [ 43%]
tests/test_ai_cost_management.py::TestBudgetAlertService::test_cost_spike_detection ERROR [ 43%]
tests/test_ai_cost_management.py::TestBudgetAlertService::test_service_specific_budgets ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostOptimizationService::test_model_efficiency_analysis ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostOptimizationService::test_caching_optimization ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostOptimizationService::test_batch_processing_optimization ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostOptimizationService::test_prompt_optimization_analysis ERROR [ 43%]
tests/test_ai_cost_management.py::TestCostOptimizationService::test_comprehensive_optimization_report ERROR [ 43%]
tests/test_ai_cost_management.py::TestAICostManager::test_track_api_call FAILED [ 43%]
tests/test_ai_cost_management.py::TestAICostManager::test_daily_cost_summary FAILED [ 43%]
tests/test_ai_cost_management.py::TestAICostManager::test_budget_monitoring_integration FAILED [ 43%]
tests/test_ai_cost_management.py::TestAICostManager::test_optimization_recommendations FAILED [ 44%]
tests/test_ai_cost_management.py::TestAICostManager::test_cost_reporting_endpoints FAILED [ 44%]
tests/test_ai_cost_management.py::TestAICostManager::test_real_time_cost_monitoring SKIPPED [ 44%]
tests/test_ai_cost_management.py::TestAICostManager::test_user_cost_limits FAILED [ 44%]
tests/test_ai_cost_management.py::TestAICostManager::test_cost_forecasting SKIPPED [ 44%]
tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_intelligent_model_routing FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_dynamic_caching_strategy FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_prompt_compression_optimization FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_batch_request_optimization FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_executive_cost_dashboard FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_cost_attribution_analysis FAILED [ 44%]
tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_predictive_cost_modeling FAILED [ 44%]
tests/test_ai_cost_management.py::TestIntegrationWithAIServices::test_budget_alert_integration FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_success FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_with_cache_hit FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_with_error FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_set_budget_success FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_get_budget_status FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_budget_alert_triggered FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestUsageAnalytics::test_get_usage_analytics_daily FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestUsageAnalytics::test_get_usage_analytics_by_service FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostOptimization::test_get_optimization_insights FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostOptimization::test_simulate_optimization FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostSummary::test_get_cost_summary_current_month FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestCostSummary::test_get_detailed_cost_breakdown FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestErrorHandling::test_track_usage_invalid_tokens FAILED [ 44%]
tests/test_ai_cost_monitoring.py::TestErrorHandling::test_set_budget_invalid_limits FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestErrorHandling::test_unauthorized_access FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestRateLimiting::test_rate_limit_exceeded FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostAlerts::test_webhook_alert_triggered FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostAlerts::test_alert_history FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostExport::test_export_cost_data_csv FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostExport::test_export_cost_data_json FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostForecasting::test_forecast_monthly_costs FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostMonitoringIntegration::test_full_cost_tracking_workflow FAILED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostMonitoringSmoke::test_health_check PASSED [ 45%]
tests/test_ai_cost_monitoring.py::TestCostMonitoringSmoke::test_basic_cost_tracking FAILED [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketConnection::test_connect_success 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ff07168c-b703-4208-ae85-25b31b2fdcff: 'AICostManager' object has no attribute 'get_daily_summary'
PASSED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketConnection::test_disconnect 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 14b0b55b-8d11-4b63-a613-f47a13b8c0e0: 'AICostManager' object has no attribute 'get_daily_summary'
PASSED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketConnection::test_multiple_connections_same_user 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to c50a7aaa-26b2-4ad1-b496-54b03824fbdb: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 1b7c7344-afa9-4f26-b81d-de5ba13c6735: 'AICostManager' object has no attribute 'get_daily_summary'
PASSED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestCostUpdates::test_broadcast_cost_update 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to bf26b879-4f46-4ca4-8c83-bcc14b72c966: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestCostUpdates::test_send_personal_message 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to bff4e16d-9d0f-4750-ab55-b21f6c5b4d84: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestCostUpdates::test_broadcast_to_all 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 4d3334cd-a940-453f-9e65-65c72e3d8a32: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 22a4d96d-7ad2-4f1b-be75-e23c314cd74f: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestBudgetAlerts::test_send_budget_alert 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 98841932-b3bb-4462-9771-e89e13088de1: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestBudgetAlerts::test_critical_budget_alert 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 44799775-32c3-4625-ac22-d7d187653e2e: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestLiveDashboard::test_dashboard_initial_data FAILED [ 45%]
tests/test_ai_cost_websocket.py::TestLiveDashboard::test_dashboard_periodic_updates 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 6c670ea3-cae4-4a39-ba3f-ea4a8655fa53: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_client_message 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 27a9f260-1cd6-4133-ba68-983b8a6106f5: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_query_message 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 4ebb97b6-8700-4738-b7b1-233ff9eece3e: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_invalid_message 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to c15394b6-7741-4b2d-bcdc-5c784e2a0f78: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 45%]
tests/test_ai_cost_websocket.py::TestConnectionResilience::test_connection_timeout FAILED [ 45%]
tests/test_ai_cost_websocket.py::TestConnectionResilience::test_connection_lost 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 480e11a9-59ba-4b7d-ade1-7079bd8e574f: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 46%]
tests/test_ai_cost_websocket.py::TestConnectionResilience::test_reconnection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to cd4b5360-1a4c-419b-bd68-b1c97e3351e1: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 013156d0-c75c-4589-aa71-291eba4ea807: 'AICostManager' object has no attribute 'get_daily_summary'
PASSED                                                                   [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketSecurity::test_authentication_required PASSED [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketSecurity::test_rate_limiting 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 5362429a-6035-4f03-a375-7eba6088b701: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketIntegration::test_full_websocket_flow FAILED [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketPerformance::test_concurrent_connections 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 192cb6e8-c134-4a61-a8f2-1ff7b1a400bc: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e11f41ae-8b41-4b60-b315-41476c4b0f53: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 28348179-2178-4558-a9f1-29f4f908a30e: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ea9c2314-4f4d-4055-8f12-d3d16233c1c8: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 2f1cc8af-06a4-47c4-93f9-36a89d453367: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to d39cc01b-3f87-432d-a522-f84d65eb7b66: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e3ff51b4-5e60-4489-b05b-ba79cd6e8856: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 44fec528-d9a8-4936-a72e-9fbb6f433031: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 8f208fe0-586d-40b0-bdcb-95d315414a68: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 6746fe10-df1c-4cb4-8ccc-ee022c12394f: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to f1351cd0-6dcb-45f7-9ab9-941998b715f0: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ab3af7ee-68d6-4049-9908-8f1a37631fc4: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a3c7324f-d4e5-4b72-a0b7-91ab7ef8319d: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 86fe13cf-58f5-4dfb-8301-724f43ead0fe: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to d86e314f-ebb8-4188-96f9-30ebd8f54ee3: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a4b96da6-c5d1-41ed-b63d-f78402757b72: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 3b5a74e7-85af-4e90-b98d-7b995db0406f: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 5e434028-1a05-4b06-b564-7ced99597195: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 52606074-07e9-4ec4-9e86-d1841b4d0521: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 89a89807-b927-4e29-9f51-c8dd1106254b: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 6bfab066-e2f4-4359-8caa-e6b44a8378a5: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 499eca22-bc35-46f6-ab27-35f323045280: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to f83cfcea-9e6a-4130-b265-ce6c4249b5aa: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b61a17d6-44c3-415a-bed9-01b7e6f4315b: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ac483da3-b2b1-4d5e-8ee1-f64cc319a8f9: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 35f23cdb-0675-4f15-b9b9-b77660f088ba: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 2f5951d6-95f6-434f-b4ec-d73d4906cdb1: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 1d4ea81f-cbce-4a9f-b209-13497841d093: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 4ab5848d-7539-41f5-8b69-06218ce20aee: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 2f3485e3-8a31-4a94-8f89-f2c829b88ad2: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 82e8d1ae-4210-4c61-bf6a-0afc3817b825: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 1c1a64fc-5caf-444d-9d3a-b2dad2b724d9: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to fea917c0-d021-4804-a861-47d136aa5c7b: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ffe21a7c-bea5-40e8-a40f-40b2074e88da: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to d35f58ec-cc2f-441d-b509-112bb77c0f82: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 6045e701-d51a-43bd-a2fa-c20c73a860d8: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 75404da8-fb53-4b6d-b1f7-b6b4553244ba: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to f466bbd9-bd21-49f4-88d5-4ae93e6cf50f: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 8fb805f1-b5d8-4d58-b688-b1c417770223: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 797ea1e0-0f03-46bd-8edd-e219379bc33d: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to c251d40c-2bd0-4f2f-90b9-2e4765f8f950: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a2c7ef83-3b21-477c-9bc6-75477ae1244a: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e2d0c2da-8aca-404a-889b-d5e9390714b9: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 496ec2d7-6d8f-4323-82cf-25276600fb85: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e1844649-10a1-464f-967b-bd8e0552f6af: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 9c1b57b8-edc6-4ef7-bccd-d291ace58b92: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 7b88e791-d73a-45f5-97d5-1cdb30e296f1: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ae937910-355a-4f83-a718-8d4ec8f659b4: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 664600f9-a547-4622-9491-123d228dd019: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b5a67386-c078-4601-a8e5-a3fa547fea56: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 7320eed2-bb7f-4226-9c05-3c3765e3396c: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 413aef02-30fb-469a-baec-5de61d77ab89: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 8bd40271-5f6a-45c9-ba3f-51f8f9a5f1a8: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to afdf04db-4af0-4209-9852-ec5ce103e14c: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b2877022-0016-48cc-9e22-bdd51d57e6a6: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a59bc333-b3bf-4e72-8484-8de7b9aaa89d: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 2d8e2dd4-c782-4f03-ace9-cfa20286d437: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e3839731-1327-439e-bc90-f0d86ae8e7a8: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 891d310c-0a50-4b5b-9e91-95b60b55dc78: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 8712c254-817e-4b10-9f69-e76c82bd762f: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 19f5c4e9-ccb6-41a5-b28c-4d5adb075230: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 9bc17478-630d-4017-8990-38e0cf0fd2f8: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to f1f01b43-0d2f-442d-8db7-9694f1966178: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a8c486b3-bd36-42d5-abe0-7e60703a2d25: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 06907335-e144-4e72-8a24-12343f530ae7: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 60409a76-5445-4467-8d1d-8f4bce418435: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to c24e4b96-f15c-496f-8852-6cd6ad827fba: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ffb7edf2-08b7-487d-9daf-3f50e8735c30: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a1ba4757-fba9-442c-b146-3599a02a8a31: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a20f898d-bbfd-414c-9b7e-296d39f7cd90: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to eb853c51-635b-4d4e-97c8-5d73daaedc42: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a2480c0c-9132-458a-8928-b741aa278c69: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 634de7e8-7293-4c4d-b5bd-9ac9473558d3: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 0dbdd5a1-4ba8-49f9-8f67-ec9041125e68: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 8dc5b864-fd2d-4699-8f74-3160ac6196aa: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 3c0ed0d6-ebde-4590-8f06-98e810416be3: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 6613f377-bef4-4438-b096-5deec2c91f56: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 81f722b8-1deb-4124-9cdd-aeff09adf3fc: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b9caac44-6a7b-4dd4-83e5-e8ce97675747: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 0ff220c6-2a9c-4db1-8a0c-b9ad1ef57dc3: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 45b097be-6ed4-404e-af0f-33ba0021e61d: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 773489eb-fa45-4bd4-9232-e0ff03b9f984: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 14b86166-15cf-4f47-812b-5b0d48b23126: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to aab9b031-e474-43df-8d47-8505c2a8af60: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 34e20027-a1cc-4c1c-9581-beaaab27cd5f: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to bcacd011-ab9c-4e78-aed1-d0b80f0b18cc: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to a1460bd9-ecd0-4e27-b225-78d999986f79: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to ce168554-ce1e-44dd-8a02-22029c2633f4: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to efe67a11-d468-453c-999c-72e47719246c: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 26078d4b-2242-43f4-8534-992675558ccf: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b9e2a14a-2ef2-48f9-b7d7-d9e47e8914c5: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 7f6f346d-8284-4d4d-bca2-fc76ac6ed972: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 95c1e1d4-aaba-4de9-8a13-d9917632e6d2: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 4f773553-dd1a-43cc-95b2-728666b9cf00: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to b58b589b-4c3d-447d-b2da-d3d4bbb3898e: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to fb5d40b6-7ec1-44e0-bfdf-1d229c494e53: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to bda17421-6438-4ac4-92c7-ab93d743d2b0: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 04fe298f-2ab0-4420-9677-69f77dc9cd6d: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e8a651b1-a48b-45e0-81be-41702d829148: 'AICostManager' object has no attribute 'get_daily_summary'
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to e117a0a8-ab4f-42dd-946b-e891b34e0e88: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketPerformance::test_message_throughput 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [   ERROR] api.routers.ai_cost_websocket: Failed to send initial data to 1b92e43b-c560-4e5c-8dca-29aa683ae594: 'AICostManager' object has no attribute 'get_daily_summary'
FAILED                                                                   [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketSmoke::test_websocket_endpoint_exists PASSED [ 46%]
tests/test_ai_cost_websocket.py::TestWebSocketSmoke::test_connection_manager_initialization PASSED [ 46%]
tests/test_ai_ethics.py::TestBiasDetection::test_gender_bias_in_compliance_advice ERROR [ 46%]
tests/test_ai_ethics.py::TestBiasDetection::test_company_size_bias ERROR [ 46%]
tests/test_ai_ethics.py::TestBiasDetection::test_industry_fairness ERROR [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[GDPR] ERROR [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[ISO 27001] ERROR [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[SOX] ERROR [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[HIPAA] ERROR [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_unknown_regulation_handling 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestHallucinationPrevention::test_date_and_version_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestAdversarialRobustness::test_prompt_injection_resistance ERROR [ 46%]
tests/test_ai_ethics.py::TestAdversarialRobustness::test_out_of_scope_question_handling ERROR [ 46%]
tests/test_ai_ethics.py::TestAdversarialRobustness::test_malicious_input_sanitization ERROR [ 46%]
tests/test_ai_ethics.py::TestExplainability::test_reasoning_transparency 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestExplainability::test_confidence_scoring 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestExplainability::test_source_attribution 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestResponsibleAI::test_harmful_advice_prevention 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:37 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:38 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestResponsibleAI::test_ethical_language_usage 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 46%]
tests/test_ai_ethics.py::TestResponsibleAI::test_uncertainty_acknowledgment 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 47%]
tests/test_ai_neon.py::test_ai_functionality 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] database.db_setup: Database initialization failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/omar/Documents/ruleIQ/database/db_setup.py", line 176, in init_db
    Base.metadata.create_all(bind=_ENGINE)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/sql/schema.py", line 5924, in create_all
    bind._run_ddl_visitor(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3247, in _run_ddl_visitor
    with self.begin() as conn:
  File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3237, in begin
    with self.connect() as conn:
         ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3273, in connect
    return self._connection_cls(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2436, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
PASSED                                                                   [ 47%]
tests/test_ai_policy_functional.py::test_ai_policy_generation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] tests.test_ai_policy_functional: Test failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
PASSED                                                                   [ 47%]
tests/test_ai_policy_functional.py::test_frameworks_availability 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] tests.test_ai_policy_functional: Framework test failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
PASSED                                                                   [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_generator_initialization SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_circuit_breaker_configuration FAILED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_generation_success_google SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_generation_fallback_to_openai SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_generation_both_providers_fail SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_template_processor_iso27001_integration SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_customization_levels SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_validation_uk_specific SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_cost_optimization_caching SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_policy_refinement_iterative SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGeneratorAPIIntegration::test_generate_policy_endpoint SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGeneratorAPIIntegration::test_refine_policy_endpoint SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGeneratorAPIIntegration::test_policy_templates_endpoint SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGeneratorPerformance::test_policy_generation_response_time SKIPPED [ 47%]
tests/test_ai_policy_generator.py::TestPolicyGeneratorPerformance::test_concurrent_policy_generation SKIPPED [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_policy_generator_initialization 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
PASSED                                                                   [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_template_processor_initialization FAILED [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_mock_policy_generation PASSED [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_cache_key_generation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_fallback_content_generation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorIntegration::test_circuit_breaker_integration 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 47%]
tests/test_ai_policy_simple.py::TestPolicyGeneratorIntegration::test_template_processor_integration 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_generate_policy_stream_metadata_chunk ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_google_success ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_openai_success ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_error_handling ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_circuit_breaker_open ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_progress_updates ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_sse_format ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_headers ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_error_handling ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_authentication ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_rate_limiting ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingIntegration::test_end_to_end_streaming_google 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingIntegration::test_end_to_end_streaming_openai 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingPerformance::test_streaming_latency ERROR [ 48%]
tests/test_ai_policy_streaming.py::TestPolicyStreamingPerformance::test_streaming_memory_usage ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_timeout_handling ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_partial_failure_recovery ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_empty_response_handling ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_unicode_handling ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_large_chunk_handling ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_rapid_chunks ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_cancellation ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_duplicate_chunk_ids ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_malformed_json_in_metadata ERROR [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingConcurrency::test_multiple_concurrent_streams 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 48%]
tests/test_ai_policy_streaming_edge_cases.py::TestStreamingConcurrency::test_stream_isolation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
FAILED                                                                   [ 49%]
tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingSimple::test_generate_policy_stream_basic 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
PASSED                                                                   [ 49%]
tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingSimple::test_streaming_error_handling 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
2025-09-05 22:29:38 [ WARNING] services.ai.policy_generator: Google streaming failed: Test error during streaming, trying fallback
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: OpenAI fallback streaming failed: Test error during streaming
2025-09-05 22:29:38 [ WARNING] services.ai.policy_generator: Google AI failed: 'Mock' object is not subscriptable
2025-09-05 22:29:38 [ WARNING] services.ai.circuit_breaker: Recorded failure for google: 'Mock' object is not subscriptable (recent_failures: 1/5)
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: OpenAI fallback failed: 'Mock' object is not subscriptable
2025-09-05 22:29:38 [ WARNING] services.ai.circuit_breaker: Recorded failure for openai: 'Mock' object is not subscriptable (recent_failures: 1/5)
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: All policy generation methods failed: 1 validation error for PolicyGenerationResponse
fallback_content
  Input should be a valid string [type=string_type, input_value=<Mock name='mock.policy_t...)' id='132057844721008'>, input_type=Mock]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
PASSED                                                                   [ 49%]
tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingSimple::test_streaming_with_fallback 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
2025-09-05 22:29:38 [ WARNING] services.ai.policy_generator: Google streaming failed: Google API failed, trying fallback
PASSED                                                                   [ 49%]
tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingSimple::test_streaming_progress_tracking 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] services.ai.policy_generator: Failed to import AI clients: No module named 'services.ai.google_client'
PASSED                                                                   [ 49%]
tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingAPI::test_stream_endpoint_format FAILED [ 49%]
tests/test_ai_quick.py::test_ai_direct PASSED                            [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_allows_requests_within_limit FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_blocks_requests_over_limit FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_burst_allowance FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_window_reset FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_different_users FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_get_remaining_requests FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_help_limiter_configuration PASSED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_followup_limiter_configuration FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_analysis_limiter_configuration FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_recommendations_limiter_configuration FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimitStats::test_get_ai_rate_limit_stats_structure PASSED [ 49%]
tests/test_ai_rate_limiting.py::TestAIRateLimitStats::test_rate_limit_stats_initial_values PASSED [ 49%]
tests/test_ai_rate_limiting.py::TestRateLimitingIntegration::test_rate_limiting_with_mock_endpoint FAILED [ 49%]
tests/test_ai_rate_limiting.py::TestRateLimitingIntegration::test_concurrent_rate_limiting FAILED [ 49%]
tests/test_ai_simple.py::test_neon_and_ai 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [   ERROR] database.db_setup: Database initialization failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/omar/Documents/ruleIQ/database/db_setup.py", line 176, in init_db
    Base.metadata.create_all(bind=_ENGINE)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/sql/schema.py", line 5924, in create_all
    bind._run_ddl_visitor(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3247, in _run_ddl_visitor
    with self.begin() as conn:
  File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3237, in begin
    with self.connect() as conn:
         ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3273, in connect
    return self._connection_cls(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2436, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
PASSED                                                                   [ 49%]
tests/test_app.py::test_lifespan ERROR                                   [ 49%]
tests/test_assessments_ownership.py::TestAssessmentOwnership::test_assessment_session_ownership PASSED [ 49%]
tests/test_assessments_ownership.py::TestAssessmentOwnership::test_create_assessment_for_user PASSED [ 49%]
tests/test_assessments_ownership.py::TestAssessmentOwnership::test_assessment_questions_require_user PASSED [ 50%]
tests/test_assessments_ownership.py::TestAssessmentOwnership::test_update_assessment_response_ownership PASSED [ 50%]
tests/test_assessments_ownership.py::TestAssessmentOwnership::test_complete_assessment_ownership PASSED [ 50%]
tests/test_auth_fix.py::TestAuthenticationFix::test_fixed_authentication_flow 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:38 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 50%]
tests/test_auth_flow.py::test_auth_flow PASSED                           [ 50%]
tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_penalty_amounts_accuracy ERROR [ 50%]
tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_data_subject_rights_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_breach_notification_timeline_accuracy ERROR [ 50%]
tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_lawful_basis_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestISO27001Accuracy::test_iso27001_security_domains_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestISO27001Accuracy::test_iso27001_isms_requirements_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestUKSpecificRegulations::test_data_protection_act_2018_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestUKSpecificRegulations::test_ico_guidance_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestSectorSpecificCompliance::test_financial_services_compliance_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestSectorSpecificCompliance::test_healthcare_compliance_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestComplianceContentValidation::test_policy_content_structure_validation 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestComplianceContentValidation::test_implementation_plan_completeness 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestComplianceContentValidation::test_risk_assessment_accuracy 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestSMEValidationFramework::test_gdpr_expert_validation_checklist 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestSMEValidationFramework::test_compliance_content_versioning 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_accuracy.py::TestSMEValidationFramework::test_automated_content_flagging 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 50%]
tests/test_compliance_assistant_assessment.py::TestAssessmentHelp::test_get_assessment_help_success FAILED [ 50%]
tests/test_compliance_assistant_assessment.py::TestAssessmentHelp::test_get_assessment_help_with_fallback FAILED [ 50%]
tests/test_compliance_assistant_assessment.py::TestAssessmentFollowup::test_generate_assessment_followup_success FAILED [ 50%]
tests/test_compliance_assistant_assessment.py::TestAssessmentAnalysis::test_analyze_assessment_results_success FAILED [ 50%]
tests/test_compliance_assistant_assessment.py::TestAssessmentRecommendations::test_get_assessment_recommendations_success PASSED [ 51%]
tests/test_compliance_assistant_assessment.py::TestIntentClassification::test_classify_assessment_intent PASSED [ 51%]
tests/test_compliance_assistant_assessment.py::TestEntityExtraction::test_extract_assessment_entities PASSED [ 51%]
tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_successful 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [ WARNING] langgraph_agent.nodes.compliance_nodes: Found 1 compliance violations for company-456
FAILED                                                                   [ 51%]
tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_with_errors 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:42 [   ERROR] langgraph_agent.nodes.compliance_nodes: Error in compliance check: Compliance service unavailable
2025-09-05 22:29:42 [   ERROR] langgraph_agent.utils.cost_tracking: Error in cost tracking for compliance_check: Compliance service unavailable
FAILED                                                                   [ 51%]
tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_empty_documents FAILED [ 51%]
tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_successful FAILED [ 51%]
tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_no_documents FAILED [ 51%]
tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_with_metadata FAILED [ 51%]
tests/test_compliance_nodes.py::TestCheckComplianceStatus::test_check_compliance_with_neo4j PASSED [ 51%]
tests/test_compliance_nodes.py::TestCheckComplianceStatus::test_check_compliance_no_obligations PASSED [ 51%]
tests/test_compliance_nodes.py::TestCheckComplianceStatus::test_check_compliance_neo4j_unavailable 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] langgraph_agent.nodes.compliance_nodes: Neo4j service not available
PASSED                                                                   [ 51%]
tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_high FAILED [ 51%]
tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_medium FAILED [ 51%]
tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_low FAILED [ 51%]
tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_with_recommendations FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_batch_compliance_update_success FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_single_compliance_check_success FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_compliance_monitoring_periodic_check 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] langgraph_agent.nodes.compliance_nodes_real: Failed to monitor compliance: password authentication failed for user "postgres"
PASSED                                                                   [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_update_compliance_for_profile 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] langgraph_agent.nodes.compliance_nodes_real: Failed to update compliance for profile <Mock name='mock.id' id='132057787727456'>: 'dict' object has no attribute 'overall_score'
FAILED                                                                   [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_get_default_framework FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_get_user_for_profile FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_batch_compliance_error_handling FAILED [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_single_compliance_check_no_profile 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] langgraph_agent.nodes.compliance_nodes_real: Failed to check compliance for company company-456: type object 'BusinessProfile' has no attribute 'company_id'
FAILED                                                                   [ 51%]
tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_compliance_monitoring_disabled 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] langgraph_agent.nodes.compliance_nodes_real: Failed to monitor compliance: password authentication failed for user "postgres"
FAILED                                                                   [ 51%]
tests/test_config_env.py::test_environment_loading PASSED                [ 51%]
tests/test_config_env.py::test_environment_override FAILED               [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_requires_credentials 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_with_invalid_credentials 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_with_valid_credentials FAILED [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_protected_endpoint_requires_auth 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_protected_endpoint_with_valid_token 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_token_expiry_handling 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_token_refresh_flow FAILED [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_logout_invalidates_token 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_rate_limiting_on_login 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_sql_injection_protection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_xss_protection_in_responses 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_password_reset_flow FAILED [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_session_timeout 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
FAILED                                                                   [ 52%]
tests/test_critical_auth.py::TestAuthenticationSecurity::test_concurrent_login_sessions 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
2025-09-05 22:29:43 [ WARNING] app.core.monitoring.error_handler: Application error: Not Found
PASSED                                                                   [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_connection_pool_stress FAILED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_query_performance_baseline FAILED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_n_plus_one_detection FAILED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_connection_pool_metrics FAILED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_query_optimization_suggestions SKIPPED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_bulk_operations_performance FAILED [ 52%]
tests/test_database_performance.py::TestDatabasePerformance::test_cache_performance_impact 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [ WARNING] config.cache: Failed to connect to Redis: Error 111 connecting to localhost:6380. Connect call failed ('127.0.0.1', 6380).. Using in-memory cache fallback
FAILED                                                                   [ 52%]
tests/test_database_performance.py::TestDatabaseOptimizationRecommendations::test_generate_performance_report PASSED [ 52%]
tests/test_db_connection.py::test_connection PASSED                      [ 52%]
tests/test_db_init.py::test_database_connection 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] database.db_setup: Database connection test failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
PASSED                                                                   [ 52%]
tests/test_db_init.py::test_sync_initialization 
-------------------------------- live log call ---------------------------------
2025-09-05 22:29:43 [   ERROR] database.db_setup: Database initialization failed: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/omar/Documents/ruleIQ/database/db_setup.py", line 176, in init_db
    Base.metadata.create_all(bind=_ENGINE)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/sql/schema.py", line 5924, in create_all
    bind._run_ddl_visitor(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3247, in _run_ddl_visitor
    with self.begin() as conn:
  File "/usr/lib/python3.12/contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3237, in begin
    with self.connect() as conn:
         ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3273, in connect
    return self._connection_cls(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 147, in __init__
    Connection._handle_dbapi_exception_noconnection(
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 2436, in _handle_dbapi_exception_noconnection
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 145, in __init__
    self._dbapi_connection = engine.raw_connection()
                             ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py", line 3297, in raw_connection
    return self.pool.connect()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 449, in connect
    return _ConnectionFairy._checkout(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 1264, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 713, in checkout
    rec = pool._do_get()
          ^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 179, in _do_get
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/impl.py", line 177, in _do_get
    return self._create_connection()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 390, in _create_connection
    return _ConnectionRecord(self)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 675, in __init__
    self.__connect()
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 901, in __connect
    with util.safe_reraise():
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/pool/base.py", line 897, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/create.py", line 646, in connect
    return dialect.connect(*cargs, **cparams)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py", line 625, in connect
    return self.loaded_dbapi.connect(*cargs, **cparams)  # type: ignore[no-any-return]  # NOQA: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/Documents/ruleIQ/tests/mock_service_proxy.py", line 59, in psycopg2_connect_wrapper
    return original_psycopg2_connect(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/omar/.local/lib/python3.12/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5433 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-09-05 22:29:43 [   ERROR] tests.test_db_init:  init_db() failed
PASSED                                                                   [ 53%]
tests/test_db_init.py::test_async_operations PASSED                      [ 53%]
tests/test_e2e_workflows.py::TestCompleteComplianceJourney::test_new_business_complete_gdpr_journey ERROR [ 53%]
tests/test_e2e_workflows.py::TestCompleteComplianceJourney::test_existing_business_framework_migration_journey ERROR [ 53%]
tests/test_e2e_workflows.py::TestErrorStateHandling::test_network_interruption_recovery ERROR [ 53%]
tests/test_e2e_workflows.py::TestErrorStateHandling::test_invalid_data_graceful_handling ERROR [ 53%]
tests/test_e2e_workflows.py::TestErrorStateHandling::test_concurrent_user_conflict_resolution ERROR [ 53%]
tests/test_e2e_workflows.py::TestErrorStateHandling::test_external_service_failure_fallback ERROR [ 53%]
tests/test_e2e_workflows.py::TestAuditWorkflows::test_comprehensive_audit_trail ERROR [ 53%]
tests/test_e2e_workflows.py::TestAuditWorkflows::test_compliance_report_generation ERROR [ 53%]
tests/test_e2e_workflows.py::TestAuditWorkflows::test_regulatory_submission_preparation ERROR [ 53%]
tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_data_backup_and_recovery ERROR [ 53%]
tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_compliance_deadline_management ERROR [ 53%]
tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_multi_framework_coordination ERROR [ 53%]
tests/test_error.py::TestErrorHandling::test_error_handler_node PASSED   [ 53%]
tests/test_error.py::TestErrorHandling::test_error_recovery_flow PASSED  [ 53%]
tests/test_error.py::TestErrorHandling::test_max_retries_exceeded PASSED [ 53%]
tests/test_error.py::TestErrorHandling::test_error_logging PASSED        [ 53%]
tests/test_error.py::TestErrorHandling::test_error_aggregation PASSED    [ 53%]
tests/test_error.py::TestErrorHandling::test_error_handler_with_custom_recovery PASSED [ 53%]
tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_success FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_no_items FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_database_error FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestEvidenceProcessing::test_process_evidence_success FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestEvidenceProcessing::test_process_evidence_validation_failure FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestDuplicateDetection::test_detect_duplicate_evidence FAILED [ 53%]
tests/test_evidence_migration_tdd.py::TestDuplicateDetection::test_no_duplicates_found FAILED [ 54%]
tests/test_evidence_migration_tdd.py::TestStatusSync::test_sync_evidence_status PASSED [ 54%]
tests/test_evidence_migration_tdd.py::TestStatusSync::test_sync_status_with_error PASSED [ 54%]
tests/test_evidence_migration_tdd.py::TestErrorHandling::test_handle_processing_error FAILED [ 54%]
tests/test_evidence_migration_tdd.py::TestErrorHandling::test_error_recovery FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestDuplicateDetection::test_process_evidence_detects_duplicate FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestDuplicateDetection::test_missing_evidence_data_handling FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestDatabaseOperations::test_database_commit_failure PASSED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestDatabaseOperations::test_database_connection_lost FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestEvidenceProcessingEdgeCases::test_processor_initialization_failure FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestEvidenceProcessingEdgeCases::test_evidence_node_wrapper_function FAILED [ 54%]
tests/test_evidence_nodes_80_percent.py::TestConcurrentOperations::test_concurrent_evidence_collection FAILED [ 54%]
tests/test_evidence_nodes_additional.py::TestSyncEvidenceStatusFull::test_sync_evidence_status_with_updates PASSED [ 54%]
tests/test_evidence_nodes_additional.py::TestSyncEvidenceStatusFull::test_sync_evidence_status_database_error PASSED [ 54%]
tests/test_evidence_nodes_additional.py::TestProcessPendingEvidence::test_process_pending_items FAILED [ 54%]
tests/test_evidence_nodes_additional.py::TestValidationLogic::test_validation_with_multiple_criteria FAILED [ 54%]
tests/test_evidence_nodes_additional.py::TestErrorRecoveryMechanisms::test_retry_with_exponential_backoff PASSED [ 54%]
tests/test_evidence_nodes_additional.py::TestErrorRecoveryMechanisms::test_graceful_degradation FAILED [ 54%]
tests/test_evidence_nodes_additional.py::TestMessageHandling::test_evidence_node_with_messages FAILED [ 54%]
tests/test_evidence_nodes_additional.py::TestBatchOperations::test_batch_evidence_update PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_process_pending_evidence PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_aggregate_evidence_correct_field PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_aggregate_evidence_deduplication PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_retry_with_correct_signature PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_circuit_breaker_state_transitions PASSED [ 54%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_sync_evidence_with_text_wrapper PASSED [ 55%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_check_evidence_expiry_with_frequencies PASSED [ 55%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_collect_integrations_with_config PASSED [ 55%]
tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeSync::test_merge_evidence_preserves_data PASSED [ 55%]
tests/test_evidence_nodes_final.py::TestProcessEvidenceAdditionalPaths::test_process_evidence_with_existing_processor PASSED [ 55%]
tests/test_evidence_nodes_final.py::TestProcessEvidenceAdditionalPaths::test_process_evidence_database_exception_reraise FAILED [ 55%]
tests/test_evidence_nodes_final.py::TestProcessPendingEvidenceEdgeCases::test_process_pending_with_multiple_items PASSED [ 55%]
tests/test_evidence_nodes_final.py::TestProcessPendingEvidenceEdgeCases::test_process_pending_with_database_error PASSED [ 55%]
tests/test_evidence_nodes_final.py::TestRetryWithBackoffEdgeCases::test_retry_exhausted PASSED [ 55%]
tests/test_evidence_nodes_final.py::TestStaleEvidenceCleanup::test_process_evidence_stale_evidence_cleanup FAILED [ 55%]
tests/test_evidence_nodes_final.py::TestValidateEvidenceScoring::test_validate_evidence_low_score FAILED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_init_without_dependencies PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_init_with_dependencies PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_process_evidence_backward_compatibility PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_process_evidence_with_state PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_sync_evidence_status PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_check_evidence_expiry PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_collect_all_integrations PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_validate_evidence_method PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_aggregate_evidence_method PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_merge_evidence_method PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_duplicate_detection PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_evidence_node_instance PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_circuit_breaker_functionality PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_retry_with_backoff PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_concurrent_evidence_processing PASSED [ 55%]
tests/test_evidence_nodes_integration.py::TestErrorScenariosIntegration::test_database_error_handling PASSED [ 56%]
tests/test_evidence_nodes_integration.py::TestErrorScenariosIntegration::test_validation_error_handling PASSED [ 56%]
tests/test_evidence_nodes_integration.py::TestErrorScenariosIntegration::test_network_timeout_simulation PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestCleanupStaleEvidence::test_cleanup_stale_evidence_default_cutoff PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestCleanupStaleEvidence::test_cleanup_stale_evidence_custom_cutoff PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestCleanupStaleEvidence::test_cleanup_stale_evidence_no_stale_items PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestCleanupStaleEvidence::test_cleanup_stale_evidence_database_error PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestSyncEvidenceStatusRefactored::test_sync_with_custom_cutoff PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestSyncEvidenceStatusRefactored::test_sync_with_cleanup_error PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestSyncEvidenceStatusRefactored::test_sync_with_typed_dict_state PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestSyncEvidenceStatusRefactored::test_sync_with_no_database_connection PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestIntegrationWithRefactoredMethods::test_full_sync_workflow PASSED [ 56%]
tests/test_evidence_nodes_refactored.py::TestIntegrationWithRefactoredMethods::test_sync_idempotency PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceSourceDiscovery::test_register_evidence_source PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceSourceDiscovery::test_discover_available_sources PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceSourceDiscovery::test_source_health_check PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceSourceDiscovery::test_source_priority_management PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestParallelCollectionMechanisms::test_parallel_collection_basic PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestParallelCollectionMechanisms::test_collection_with_timeout PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestParallelCollectionMechanisms::test_collection_concurrency_limit PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestParallelCollectionMechanisms::test_collection_error_handling PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceValidationAndScoring::test_validate_evidence_structure PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceValidationAndScoring::test_evidence_quality_scoring PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceValidationAndScoring::test_evidence_relevance_scoring PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceValidationAndScoring::test_evidence_deduplication_scoring PASSED [ 56%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceAggregationAlgorithms::test_aggregate_by_regulation PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceAggregationAlgorithms::test_merge_duplicate_evidence PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceAggregationAlgorithms::test_hierarchical_aggregation PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceAggregationAlgorithms::test_weighted_aggregation PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestCachingAndDeduplication::test_evidence_caching PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestCachingAndDeduplication::test_cache_invalidation PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestCachingAndDeduplication::test_deduplication_fingerprinting PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestCachingAndDeduplication::test_incremental_deduplication PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceConfidenceCalculations::test_source_confidence_calculation PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceConfidenceCalculations::test_temporal_confidence_decay PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceConfidenceCalculations::test_corroboration_confidence PASSED [ 57%]
tests/test_evidence_orchestrator_v2.py::TestEvidenceConfidenceCalculations::test_composite_confidence_score PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackModels::test_feedback_item_creation PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackModels::test_response_feedback_validation PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackModels::test_feedback_type_enum_values PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackModels::test_quality_score_with_dimensions FAILED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_collect_single_feedback PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_invalid_feedback_type_rejected PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_rating_validation PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_batch_feedback_collection PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_retrieve_feedback_by_id PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackCollectionAPIs::test_retrieve_run_feedback PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackStorage::test_store_feedback_in_memory FAILED [ 57%]
tests/test_feedback_system.py::TestFeedbackStorage::test_feedback_persistence_to_file FAILED [ 57%]
tests/test_feedback_system.py::TestFeedbackStorage::test_feedback_queue_management FAILED [ 57%]
tests/test_feedback_system.py::TestFeedbackStorage::test_batch_submission_to_langsmith PASSED [ 57%]
tests/test_feedback_system.py::TestFeedbackAggregation::test_aggregate_ratings PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackAggregation::test_sentiment_aggregation PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackAggregation::test_time_based_aggregation PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackAggregation::test_user_feedback_patterns PASSED [ 58%]
tests/test_feedback_system.py::TestFineTuningTriggers::test_correction_threshold_trigger PASSED [ 58%]
tests/test_feedback_system.py::TestFineTuningTriggers::test_low_rating_trigger PASSED [ 58%]
tests/test_feedback_system.py::TestFineTuningTriggers::test_negative_sentiment_trigger PASSED [ 58%]
tests/test_feedback_system.py::TestFineTuningTriggers::test_composite_trigger_conditions PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackLoopMetrics::test_response_time_metrics PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackLoopMetrics::test_feedback_incorporation_rate PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackLoopMetrics::test_model_improvement_metrics PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackLoopMetrics::test_user_satisfaction_trend PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackUIComponents::test_feedback_widget_props PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackUIComponents::test_feedback_dashboard_metrics PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackUIComponents::test_inline_feedback_buttons PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackUIComponents::test_correction_dialog_component PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackSystemIntegration::test_end_to_end_feedback_flow FAILED [ 58%]
tests/test_feedback_system.py::TestFeedbackSystemIntegration::test_feedback_triggers_model_update PASSED [ 58%]
tests/test_feedback_system.py::TestFeedbackSystemIntegration::test_feedback_export_import_cycle PASSED [ 58%]
tests/test_fixture_isolation.py::test_authenticated_client_works ERROR   [ 58%]
tests/test_fixture_isolation.py::test_unauthenticated_client_fails ERROR [ 58%]
tests/test_fixture_isolation.py::test_authenticated_client_works_again ERROR [ 58%]
tests/test_fixtures_validation.py::TestDatabaseFixtures::test_db_session_fixture ERROR [ 58%]
tests/test_fixtures_validation.py::TestDatabaseFixtures::test_sample_user_fixture ERROR [ 58%]
tests/test_fixtures_validation.py::TestDatabaseFixtures::test_sample_business_profile_fixture ERROR [ 58%]
tests/test_fixtures_validation.py::TestDatabaseFixtures::test_authenticated_user_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestDatabaseFixtures::test_async_db_session_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestRedisFixtures::test_mock_redis_client_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_openai_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_anthropic_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_google_ai_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_s3_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_stripe_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_sendgrid_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestExternalServiceMocks::test_mock_celery_task_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_authenticated_headers_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_admin_headers_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_client_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_authenticated_client_fixture ERROR [ 59%]
tests/test_fixtures_validation.py::TestSampleDataFixtures::test_sample_user_data_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestSampleDataFixtures::test_sample_framework_data_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestSampleDataFixtures::test_sample_assessment_data_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestUtilityFixtures::test_event_loop_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestUtilityFixtures::test_api_headers_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestUtilityFixtures::test_cleanup_uploads_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestUtilityFixtures::test_temporary_env_var_context PASSED [ 59%]
tests/test_fixtures_validation.py::TestAsyncFixtures::test_async_client_fixture PASSED [ 59%]
tests/test_fixtures_validation.py::TestAutoMocking::test_auto_mock_external_services PASSED [ 59%]
tests/test_fixtures_validation.py::TestFixtureIsolation::test_db_rollback_between_tests_1 ERROR [ 59%]
tests/test_fixtures_validation.py::TestFixtureIsolation::test_db_rollback_between_tests_2 ERROR [ 59%]
tests/test_freemium_simple.py::test_assessment_lead_creation ERROR       [ 59%]
tests/test_freemium_simple.py::test_assessment_lead_with_all_fields ERROR [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetVersion::test_version_comparison PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetVersion::test_version_parsing PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetVersion::test_version_validation PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetVersion::test_version_increment PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestVersionManager::test_version_metadata PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestVersionManager::test_version_manager_init PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestVersionManager::test_create_version PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestVersionManager::test_list_versions PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestVersionManager::test_get_latest_version PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestJSONLLoader::test_load_empty_file PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestJSONLLoader::test_load_compliance_scenarios PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestJSONLLoader::test_load_mixed_types PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestJSONLLoader::test_save_to_jsonl PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestGoldenDatasetLoader::test_loader_initialization PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestGoldenDatasetLoader::test_load_dataset_by_version PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestGoldenDatasetLoader::test_parse_dataset_types FAILED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetRegistry::test_registry_initialization PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetRegistry::test_register_dataset PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetRegistry::test_get_dataset PASSED [ 60%]
tests/test_golden_dataset_loaders.py::TestDatasetRegistry::test_list_datasets PASSED [ 60%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_reg_citation_minimal PASSED [ 60%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_reg_citation_full PASSED [ 60%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_source_meta FAILED [ 60%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_temporal_validity_valid PASSED [ 60%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_temporal_validity_no_end PASSED [ 61%]
tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_temporal_validity_invalid PASSED [ 61%]
tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_minimal FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_full FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_missing_triggers FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_missing_outcome_code PASSED [ 61%]
tests/test_golden_dataset_schemas.py::TestEvidenceCase::test_evidence_case_minimal FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestEvidenceCase::test_evidence_case_full FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestRegulatoryQAPair::test_regulatory_qa_minimal FAILED [ 61%]
tests/test_golden_dataset_schemas.py::TestRegulatoryQAPair::test_regulatory_qa_full FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_semantic_layer_valid FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_semantic_layer_missing_description FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_cross_reference_layer_valid FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_cross_reference_layer_orphaned_evidence FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_regulatory_accuracy_layer_valid FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_regulatory_accuracy_layer_invalid_citation FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_temporal_consistency_layer_valid FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_temporal_consistency_layer_overlapping FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_validate_full_dataset FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestDeepValidator::test_validate_compliance_scenario FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_calculate_trust_score_high FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_calculate_trust_score_low FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_trust_subscores FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_scenario FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_with_warnings FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_invalid_source FAILED [ 61%]
tests/test_golden_dataset_validators.py::TestSecurityValidation::test_input_bounds_checking FAILED [ 62%]
tests/test_golden_dataset_validators.py::TestSecurityValidation::test_deeply_nested_structure_protection FAILED [ 62%]
tests/test_golden_dataset_validators.py::TestSecurityValidation::test_rate_limiting FAILED [ 62%]
tests/test_golden_dataset_validators.py::TestSecurityValidation::test_regex_dos_protection FAILED [ 62%]

==================================== ERRORS ====================================
_ ERROR at setup of TestComplianceAccuracy.test_gdpr_basic_questions_accuracy __
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 33
      @pytest.mark.asyncio
      async def test_gdpr_basic_questions_accuracy(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test AI accuracy on basic GDPR questions"""
          assistant = ComplianceAssistant(db_session)

          basic_questions = [q for q in gdpr_golden_dataset if q["difficulty"] == "basic"]
          correct_answers = 0
          total_questions = len(basic_questions)

          for question_data in basic_questions:
              # Mock AI response with realistic compliance content
              mock_response = self._generate_mock_response(question_data)
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  # Check if response contains expected keywords
                  if self._validate_response_accuracy(response, question_data):
                      correct_answers += 1

          # Require 85% accuracy on basic questions
          accuracy = correct_answers / total_questions
          assert (
              accuracy >= 0.85
          ), f"Basic GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:33
_ ERROR at setup of TestComplianceAccuracy.test_gdpr_intermediate_questions_accuracy _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 79
      @pytest.mark.asyncio
      async def test_gdpr_intermediate_questions_accuracy(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test AI accuracy on intermediate GDPR questions"""
          assistant = ComplianceAssistant(db_session)

          intermediate_questions = [
              q for q in gdpr_golden_dataset if q["difficulty"] == "intermediate"
          ]
          correct_answers = 0
          total_questions = len(intermediate_questions)

          for question_data in intermediate_questions:
              mock_response = self._generate_mock_response(question_data)
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.85,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  if self._validate_response_accuracy(response, question_data):
                      correct_answers += 1

          # Require 75% accuracy on intermediate questions
          accuracy = correct_answers / total_questions
          assert (
              accuracy >= 0.75
          ), f"Intermediate GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:79
_ ERROR at setup of TestComplianceAccuracy.test_gdpr_advanced_questions_accuracy _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 125
      @pytest.mark.asyncio
      async def test_gdpr_advanced_questions_accuracy(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test AI accuracy on advanced GDPR questions"""
          assistant = ComplianceAssistant(db_session)

          advanced_questions = [
              q for q in gdpr_golden_dataset if q["difficulty"] == "advanced"
          ]
          correct_answers = 0
          total_questions = len(advanced_questions)

          for question_data in advanced_questions:
              mock_response = self._generate_mock_response(question_data)
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.8,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  if self._validate_response_accuracy(response, question_data):
                      correct_answers += 1

          # Require 65% accuracy on advanced questions
          accuracy = correct_answers / total_questions
          assert (
              accuracy >= 0.65
          ), f"Advanced GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:125
____ ERROR at setup of TestComplianceAccuracy.test_source_citation_accuracy ____
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 171
      @pytest.mark.asyncio
      async def test_source_citation_accuracy(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test that AI responses include accurate source citations"""
          assistant = ComplianceAssistant(db_session)

          questions_with_sources = [q for q in gdpr_golden_dataset if "source" in q]
          correct_citations = 0

          for question_data in questions_with_sources:
              # Generate response that includes source citation
              mock_response = f"""
              {self._generate_mock_response(question_data)}

              **Source:** {question_data["source"]}
              """
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "sources": [question_data["source"]],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  # Check if response includes correct source citation
                  expected_source = question_data["source"]
                  if expected_source.lower() in response.lower():
                      correct_citations += 1

          # Require 80% accurate source citations
          citation_accuracy = correct_citations / len(questions_with_sources)
          assert (
              citation_accuracy >= 0.8
          ), f"Source citation accuracy too low: {citation_accuracy:.2%}"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:171
_____ ERROR at setup of TestComplianceAccuracy.test_response_completeness ______
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 222
      @pytest.mark.asyncio
      async def test_response_completeness(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test that AI responses are comprehensive and complete"""
          assistant = ComplianceAssistant(db_session)

          for question_data in gdpr_golden_dataset[:5]:  # Test subset for performance
              mock_response = self._generate_comprehensive_response(question_data)
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  # Check response completeness criteria
                  assert len(response) >= 100, "Response too short to be comprehensive"
                  assert self._contains_key_information(
                      response, question_data
                  ), "Response missing key information"
                  assert self._has_practical_guidance(
                      response
                  ), "Response lacks practical guidance"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:222
_ ERROR at setup of TestComplianceAccuracy.test_framework_specific_terminology _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 262
      @pytest.mark.asyncio
      async def test_framework_specific_terminology(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test that AI uses correct framework-specific terminology"""
          assistant = ComplianceAssistant(db_session)

          gdpr_terminology = [
              "data subject",
              "controller",
              "processor",
              "supervisory authority",
              "lawful basis",
              "legitimate interests",
              "data protection officer",
          ]

          for question_data in gdpr_golden_dataset[:3]:
              mock_response = self._generate_mock_response(
                  question_data, include_terminology=True,
              )
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  # Check that response uses appropriate GDPR terminology
                  response_lower = response.lower()
                  relevant_terms = [
                      term
                      for term in gdpr_terminology
                      if any(keyword in term for keyword in question_data["keywords"])
                  ]

                  if relevant_terms:
                      found_terms = [
                          term for term in relevant_terms if term in response_lower
                      ]
                      assert (
                          len(found_terms) > 0
                      ), f"Response should use GDPR terminology for question about {question_data['category']}"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:262
_ ERROR at setup of TestComplianceAccuracy.test_consistency_across_similar_questions _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 321
      @pytest.mark.asyncio
      async def test_consistency_across_similar_questions(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test that AI provides consistent answers to similar questions"""
          assistant = ComplianceAssistant(db_session)

          # Group questions by category
          categories = {}
          for question_data in gdpr_golden_dataset:
              category = question_data["category"]
              if category not in categories:
                  categories[category] = []
              categories[category].append(question_data)

          # Test consistency within categories that have multiple questions
          for category, questions in categories.items():
              if len(questions) < 2:
                  continue

              responses = []
              for question_data in questions:
                  mock_response = self._generate_mock_response(question_data)
                  mock_ai_client.generate_content.return_value.text = mock_response

                  with patch.object(assistant, "process_message") as mock_process:
                      mock_process.return_value = (
                          mock_response,
                          {
                              "intent": "compliance_guidance",
                              "framework": question_data["framework"],
                              "confidence": 0.9,
                          },
                      )

                      response, metadata = await assistant.process_message(
                          conversation_id=uuid4(),
                          user=None,
                          message=question_data["question"],
                          business_profile_id=uuid4(),
                      )
                      responses.append(response)

              # Check for consistency in key concepts across responses
              assert self._check_conceptual_consistency(
                  responses, category
              ), f"Inconsistent responses in category: {category}"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:321
_ ERROR at setup of TestComplianceAccuracy.test_regulatory_compliance_alignment _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 372
      @pytest.mark.asyncio
      async def test_regulatory_compliance_alignment(
          self,
          db_session,
          mock_ai_client,
          gdpr_golden_dataset,
      ):
          """Test that AI responses align with current regulatory requirements"""
          assistant = ComplianceAssistant(db_session)

          # Test that responses don't contradict regulatory requirements
          regulatory_violations = [
              "you don't need to comply",
              "gdpr doesn't apply",
              "ignore the regulation",
              "bypass the requirement",
              "this law is optional",
          ]

          for question_data in gdpr_golden_dataset:
              mock_response = self._generate_mock_response(question_data)
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "compliance_guidance",
                          "framework": question_data["framework"],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=question_data["question"],
                      business_profile_id=uuid4(),
                  )

                  # Check that response doesn't contain regulatory violations
                  response_lower = response.lower()
                  violations_found = [
                      v for v in regulatory_violations if v in response_lower
                  ]
                  assert (
                      len(violations_found) == 0
                  ), f"Response contains regulatory violations: {violations_found}"

                  # Check that response encourages compliance
                  compliance_indicators = [
                      "should comply",
                      "required",
                      "mandatory",
                      "must implement",
                  ]
                  has_compliance_guidance = any(
                      indicator in response_lower for indicator in compliance_indicators
                  )
                  assert has_compliance_guidance, "Response should encourage compliance"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, gdpr_golden_dataset, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:372
_ ERROR at setup of TestFrameworkCoverage.test_framework_identification_accuracy _
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 571
      @pytest.mark.asyncio
      async def test_framework_identification_accuracy(self, db_session, mock_ai_client):
          """Test AI accurately identifies relevant compliance frameworks"""
          assistant = ComplianceAssistant(db_session)

          framework_test_cases = [
              {
                  "question": "We process customer personal data in the EU",
                  "expected_frameworks": ["GDPR"],
                  "context": {"location": "EU", "data_type": "personal"},
              },
              {
                  "question": "Our healthcare app needs security compliance",
                  "expected_frameworks": ["HIPAA", "ISO 27001"],
                  "context": {"industry": "healthcare", "focus": "security"},
              },
              {
                  "question": "SOC 2 audit requirements for our SaaS platform",
                  "expected_frameworks": ["SOC 2"],
                  "context": {"industry": "saas", "focus": "audit"},
              },
          ]

          for test_case in framework_test_cases:
              # Mock response that identifies correct frameworks
              frameworks_text = ", ".join(test_case["expected_frameworks"])
              mock_response = f"Based on your requirements, the relevant frameworks are: {frameworks_text}."
              mock_ai_client.generate_content.return_value.text = mock_response

              with patch.object(assistant, "process_message") as mock_process:
                  mock_process.return_value = (
                      mock_response,
                      {
                          "intent": "framework_identification",
                          "identified_frameworks": test_case["expected_frameworks"],
                          "confidence": 0.9,
                      },
                  )

                  response, metadata = await assistant.process_message(
                      conversation_id=uuid4(),
                      user=None,
                      message=test_case["question"],
                      business_profile_id=uuid4(),
                  )

                  # Verify correct frameworks are identified
                  for framework in test_case["expected_frameworks"]:
                      assert (
                          framework.lower() in response.lower()
                      ), f"Response should identify {framework} framework"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:571
____ ERROR at setup of TestFrameworkCoverage.test_cross_framework_guidance _____
file /home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py, line 623
      @pytest.mark.asyncio
      async def test_cross_framework_guidance(self, db_session, mock_ai_client):
          """Test AI provides guidance when multiple frameworks apply"""
          assistant = ComplianceAssistant(db_session)

          multi_framework_question = "We're a fintech company processing EU customer data. What compliance frameworks apply?"

          mock_response = """
          For a fintech company processing EU customer data, multiple compliance frameworks apply:

          1. **GDPR** - Required for processing EU personal data
          2. **PCI DSS** - Required for payment card data processing
          3. **ISO 27001** - Recommended for information security management
          4. **SOC 2** - May be required by business customers

          These frameworks complement each other and share common security requirements.
          """

          mock_ai_client.generate_content.return_value.text = mock_response

          with patch.object(assistant, "process_message") as mock_process:
              mock_process.return_value = (
                  mock_response,
                  {
                      "intent": "multi_framework_guidance",
                      "identified_frameworks": ["GDPR", "PCI DSS", "ISO 27001", "SOC 2"],
                      "confidence": 0.9,
                  },
              )

              response, metadata = await assistant.process_message(
                  conversation_id=uuid4(),
                  user=None,
                  message=multi_framework_question,
                  business_profile_id=uuid4(),
              )

              # Check that multiple frameworks are mentioned
              frameworks = ["GDPR", "PCI DSS", "ISO 27001", "SOC 2"]
              mentioned_frameworks = [fw for fw in frameworks if fw in response]

              assert (
                  len(mentioned_frameworks) >= 3
              ), f"Should mention multiple frameworks, found: {mentioned_frameworks}"

              # Check that response explains relationships between frameworks
              relationship_indicators = ["complement", "overlap", "common", "together"]
              has_relationship_guidance = any(
                  indicator in response.lower() for indicator in relationship_indicators
              )
              assert has_relationship_guidance, "Should explain framework relationships"
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/ai/test_compliance_accuracy.py:623
_______ ERROR at setup of TestAssessmentEndpoints.test_create_assessment _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 40
      @pytest.mark.asyncio
      async def test_create_assessment(self, client, auth_headers, assessment_data):
          """Test creating new assessment"""
          response = client.post(
              "/api/v1/assessments",
              json=assessment_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert data["session_type"] == assessment_data["session_type"]
          assert data["business_profile_id"] == assessment_data["business_profile_id"]
          assert "id" in data
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_ ERROR at setup of TestAssessmentEndpoints.test_create_assessment_invalid_framework _
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 55
      @pytest.mark.asyncio
      async def test_create_assessment_invalid_framework(self, client, auth_headers):
          """Test creating assessment with invalid framework"""
          data = {
              "framework_id": 99999,  # Non-existent framework
              "name": "Invalid Assessment"
          }

          response = client.post(
              "/api/v1/assessments",
              json=data,
              headers=auth_headers
          )

          assert response.status_code == 404
          assert "framework not found" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
________ ERROR at setup of TestAssessmentEndpoints.test_get_assessment _________
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 72
      @pytest.mark.asyncio
      async def test_get_assessment(self, client, auth_headers):
          """Test retrieving assessment by ID"""
          assessment_id = str(uuid4())

          with patch('api.routers.assessments.get_assessment_by_id') as mock_get:
              mock_get.return_value = {
                  "id": assessment_id,
                  "name": "Test Assessment",
                  "status": "in_progress",
                  "score": 75.0
              }

              response = client.get(
                  f"/api/v1/assessments/{assessment_id}",
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert data["id"] == assessment_id
              assert data["score"] == 75.0
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
___ ERROR at setup of TestAssessmentEndpoints.test_get_assessment_not_found ____
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 95
      @pytest.mark.asyncio
      async def test_get_assessment_not_found(self, client, auth_headers):
          """Test retrieving non-existent assessment"""
          response = client.get(
              f"/api/v1/assessments/{uuid4()}",
              headers=auth_headers
          )

          assert response.status_code == 404
          assert "not found" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_______ ERROR at setup of TestAssessmentEndpoints.test_list_assessments ________
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 106
      @pytest.mark.asyncio
      async def test_list_assessments(self, client, auth_headers):
          """Test listing user's assessments"""
          with patch('api.routers.assessments.get_user_assessments') as mock_list:
              mock_list.return_value = [
                  {"id": str(uuid4()), "name": "Assessment 1", "status": "completed"},
                  {"id": str(uuid4()), "name": "Assessment 2", "status": "in_progress"}
              ]

              response = client.get(
                  "/api/v1/assessments",
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert len(data) == 2
              assert data[0]["name"] == "Assessment 1"
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_ ERROR at setup of TestAssessmentEndpoints.test_list_assessments_with_filters _
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 125
      @pytest.mark.asyncio
      async def test_list_assessments_with_filters(self, client, auth_headers):
          """Test listing assessments with filters"""
          response = client.get(
              "/api/v1/assessments?status=completed&framework_id=1",
              headers=auth_headers
          )

          assert response.status_code == 200
          # Results should be filtered
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_______ ERROR at setup of TestAssessmentEndpoints.test_update_assessment _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 136
      @pytest.mark.asyncio
      async def test_update_assessment(self, client, auth_headers):
          """Test updating assessment"""
          assessment_id = str(uuid4())
          update_data = {
              "name": "Updated Assessment Name",
              "description": "Updated description"
          }

          response = client.put(
              f"/api/v1/assessments/{assessment_id}",
              json=update_data,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["name"] == update_data["name"]
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_______ ERROR at setup of TestAssessmentEndpoints.test_delete_assessment _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 155
      @pytest.mark.asyncio
      async def test_delete_assessment(self, client, auth_headers):
          """Test deleting assessment"""
          assessment_id = str(uuid4())

          response = client.delete(
              f"/api/v1/assessments/{assessment_id}",
              headers=auth_headers
          )

          assert response.status_code == 204
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
__ ERROR at setup of TestAssessmentEndpoints.test_submit_assessment_response ___
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 167
      @pytest.mark.asyncio
      async def test_submit_assessment_response(self, client, auth_headers):
          """Test submitting response to assessment question"""
          assessment_id = str(uuid4())
          response_data = {
              "requirement_id": "GDPR-1",
              "compliant": True,
              "evidence": ["privacy_policy.pdf", "data_map.xlsx"],
              "notes": "Fully compliant with lawful basis requirement",
              "implementation_status": "implemented"
          }

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/responses",
              json=response_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert data["requirement_id"] == response_data["requirement_id"]
          assert data["compliant"] == response_data["compliant"]
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_____ ERROR at setup of TestAssessmentEndpoints.test_bulk_submit_responses _____
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 190
      @pytest.mark.asyncio
      async def test_bulk_submit_responses(self, client, auth_headers):
          """Test submitting multiple responses at once"""
          assessment_id = str(uuid4())
          responses_data = {
              "responses": [
                  {
                      "requirement_id": "GDPR-1",
                      "compliant": True,
                      "evidence": ["doc1.pdf"]
                  },
                  {
                      "requirement_id": "GDPR-2",
                      "compliant": False,
                      "notes": "In progress"
                  },
                  {
                      "requirement_id": "GDPR-3",
                      "compliant": True,
                      "evidence": ["doc2.pdf", "doc3.pdf"]
                  }
              ]
          }

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/responses/bulk",
              json=responses_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert len(data["processed"]) == 3
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_____ ERROR at setup of TestAssessmentEndpoints.test_get_assessment_score ______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 224
      @pytest.mark.asyncio
      async def test_get_assessment_score(self, client, auth_headers):
          """Test getting assessment compliance score"""
          assessment_id = str(uuid4())

          with patch('api.routers.assessments.calculate_score') as mock_score:
              mock_score.return_value = {
                  "overall_score": 82.5,
                  "category_scores": {
                      "technical": 90.0,
                      "organizational": 75.0,
                      "legal": 82.5
                  },
                  "compliant_requirements": 33,
                  "total_requirements": 40,
                  "gaps": 7
              }

              response = client.get(
                  f"/api/v1/assessments/{assessment_id}/score",
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert data["overall_score"] == 82.5
              assert data["compliant_requirements"] == 33
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
______ ERROR at setup of TestAssessmentEndpoints.test_get_assessment_gaps ______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 252
      @pytest.mark.asyncio
      async def test_get_assessment_gaps(self, client, auth_headers):
          """Test identifying assessment gaps"""
          assessment_id = str(uuid4())

          response = client.get(
              f"/api/v1/assessments/{assessment_id}/gaps",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "gaps" in data
          assert isinstance(data["gaps"], list)
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
__ ERROR at setup of TestAssessmentEndpoints.test_generate_assessment_report ___
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 267
      @pytest.mark.asyncio
      async def test_generate_assessment_report(self, client, auth_headers):
          """Test generating assessment report"""
          assessment_id = str(uuid4())

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/report",
              json={"format": "pdf"},
              headers=auth_headers
          )

          assert response.status_code in [200, 201]
          # Should return report URL or document
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
______ ERROR at setup of TestAssessmentEndpoints.test_complete_assessment ______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 281
      @pytest.mark.asyncio
      async def test_complete_assessment(self, client, auth_headers):
          """Test marking assessment as complete"""
          assessment_id = str(uuid4())

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/complete",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["status"] == "completed"
          assert "completed_at" in data
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
___ ERROR at setup of TestAssessmentEndpoints.test_ai_assessment_assistance ____
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 296
      @pytest.mark.asyncio
      async def test_ai_assessment_assistance(self, client, auth_headers):
          """Test AI assistance for assessment"""
          assessment_id = str(uuid4())
          ai_request = {
              "requirement_id": "GDPR-1",
              "question": "How do I document lawful basis for processing?",
              "context": {
                  "business_type": "SaaS",
                  "data_types": ["personal", "financial"]
              }
          }

          with patch('services.ai.assistant.ComplianceAssistant') as mock_ai:
              mock_ai.return_value.generate_response = AsyncMock(
                  return_value="To document lawful basis, you should..."
              )

              response = client.post(
                  f"/api/v1/assessments/{assessment_id}/ai-assist",
                  json=ai_request,
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert "response" in data
              assert "lawful basis" in data["response"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
__ ERROR at setup of TestAssessmentEndpoints.test_assessment_evidence_upload ___
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 325
      @pytest.mark.asyncio
      async def test_assessment_evidence_upload(self, client, auth_headers):
          """Test uploading evidence for assessment"""
          assessment_id = str(uuid4())
          requirement_id = "GDPR-1"

          # Simulate file upload
          files = {
              "file": ("privacy_policy.pdf", b"PDF content here", "application/pdf")
          }

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/requirements/{requirement_id}/evidence",
              files=files,
              headers=auth_headers
          )

          assert response.status_code in [200, 201]
          data = response.json()
          assert "file_id" in data
          assert data["filename"] == "privacy_policy.pdf"
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
___ ERROR at setup of TestAssessmentEndpoints.test_assessment_collaboration ____
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 347
      @pytest.mark.asyncio
      async def test_assessment_collaboration(self, client, auth_headers):
          """Test adding collaborators to assessment"""
          assessment_id = str(uuid4())
          collaborator_data = {
              "email": "collaborator@example.com",
              "role": "reviewer",
              "permissions": ["view", "comment"]
          }

          response = client.post(
              f"/api/v1/assessments/{assessment_id}/collaborators",
              json=collaborator_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert data["email"] == collaborator_data["email"]
          assert data["role"] == collaborator_data["role"]
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
______ ERROR at setup of TestAssessmentEndpoints.test_assessment_history _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 368
      @pytest.mark.asyncio
      async def test_assessment_history(self, client, auth_headers):
          """Test getting assessment change history"""
          assessment_id = str(uuid4())

          response = client.get(
              f"/api/v1/assessments/{assessment_id}/history",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "changes" in data
          assert isinstance(data["changes"], list)
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_______ ERROR at setup of TestAssessmentEndpoints.test_assessment_export _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 383
      @pytest.mark.asyncio
      async def test_assessment_export(self, client, auth_headers):
          """Test exporting assessment data"""
          assessment_id = str(uuid4())

          response = client.get(
              f"/api/v1/assessments/{assessment_id}/export?format=json",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "assessment" in data
          assert "responses" in data
          assert "score" in data
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_______ ERROR at setup of TestAssessmentEndpoints.test_assessment_import _______
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 399
      @pytest.mark.asyncio
      async def test_assessment_import(self, client, auth_headers):
          """Test importing assessment from template"""
          import_data = {
              "template_id": "gdpr-standard",
              "customize": {
                  "name": "Imported GDPR Assessment",
                  "exclude_categories": ["marketing"]
              }
          }

          response = client.post(
              "/api/v1/assessments/import",
              json=import_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert data["name"] == import_data["customize"]["name"]
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, assessment_data, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:24
_ ERROR at setup of TestAssessmentIntegration.test_complete_assessment_workflow _
file /home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py, line 425
      @pytest.mark.asyncio
      async def test_complete_assessment_workflow(self, client, auth_headers, db_session):
          """Test complete assessment lifecycle"""
          # 1. Create assessment
          create_data = {
              "framework_id": 1,
              "name": "Integration Test Assessment"
          }

          create_response = client.post(
              "/api/v1/assessments",
              json=create_data,
              headers=auth_headers
          )
          assert create_response.status_code == 201
          assessment_id = create_response.json()["id"]

          # 2. Submit responses
          responses = [
              {"requirement_id": f"REQ-{i}", "compliant": i % 2 == 0}
              for i in range(10)
          ]

          for response in responses:
              resp = client.post(
                  f"/api/v1/assessments/{assessment_id}/responses",
                  json=response,
                  headers=auth_headers
              )
              assert resp.status_code == 201

          # 3. Get score
          score_response = client.get(
              f"/api/v1/assessments/{assessment_id}/score",
              headers=auth_headers
          )
          assert score_response.status_code == 200
          assert score_response.json()["overall_score"] == 50.0  # 5/10 compliant

          # 4. Identify gaps
          gaps_response = client.get(
              f"/api/v1/assessments/{assessment_id}/gaps",
              headers=auth_headers
          )
          assert gaps_response.status_code == 200
          assert len(gaps_response.json()["gaps"]) == 5

          # 5. Complete assessment
          complete_response = client.post(
              f"/api/v1/assessments/{assessment_id}/complete",
              headers=auth_headers
          )
          assert complete_response.status_code == 200

          # 6. Generate report
          report_response = client.post(
              f"/api/v1/assessments/{assessment_id}/report",
              json={"format": "json"},
              headers=auth_headers
          )
          assert report_response.status_code in [200, 201]
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_assessment_endpoints.py:425
______ ERROR at setup of TestAuthEndpoints.test_register_duplicate_email _______
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 62
      @pytest.mark.asyncio
      async def test_register_duplicate_email(self, client, user_data, existing_user):
          """Test registration with duplicate email"""
          user_data["email"] = existing_user.email

          response = client.post("/api/v1/auth/register", json=user_data)

          assert response.status_code == 400
          assert "already registered" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
____________ ERROR at setup of TestAuthEndpoints.test_login_success ____________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 92
      @pytest.mark.asyncio
      async def test_login_success(self, client, existing_user):
          """Test successful login"""
          login_data = {
              "username": existing_user.email,  # OAuth2 uses 'username' field
              "password": "ExistingPassword123!"
          }

          response = client.post(
              "/api/v1/auth/token",
              data=login_data,  # Form data, not JSON
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )

          assert response.status_code == 200
          data = response.json()
          assert "access_token" in data
          assert data["token_type"] == "bearer"

          # Verify token is valid
          token = data["access_token"]
          payload = jwt.decode(token, options={"verify_signature": False})
          assert payload["sub"] == existing_user.email
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
_______ ERROR at setup of TestAuthEndpoints.test_login_invalid_password ________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 116
      @pytest.mark.asyncio
      async def test_login_invalid_password(self, client, existing_user):
          """Test login with incorrect password"""
          login_data = {
              "username": existing_user.email,
              "password": "WrongPassword123!"
          }

          response = client.post(
              "/api/v1/auth/token",
              data=login_data,
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )

          assert response.status_code == 401
          assert "incorrect" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
_________ ERROR at setup of TestAuthEndpoints.test_login_inactive_user _________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 150
      @pytest.mark.asyncio
      async def test_login_inactive_user(self, client, db_session):
          """Test login with inactive user account"""
          # Create inactive user
          user = User(
              email="inactive@example.com",
              full_name="Inactive User",
              hashed_password=get_password_hash("Password123!"),
              is_active=False,
          )
          db_session.add(user)
          db_session.commit()

          login_data = {
              "username": "inactive@example.com",
              "password": "Password123!"
          }

          response = client.post(
              "/api/v1/auth/token",
              data=login_data,
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )

          assert response.status_code == 403
          assert "inactive" in response.json()["detail"].lower()
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:150
__________ ERROR at setup of TestAuthEndpoints.test_get_current_user ___________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 177
      @pytest.mark.asyncio
      async def test_get_current_user(self, client, existing_user):
          """Test getting current user info with valid token"""
          token = create_access_token(data={"sub": existing_user.email})

          response = client.get(
              "/api/v1/users/me",
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 200
          data = response.json()
          assert data["email"] == existing_user.email
          assert data["full_name"] == existing_user.full_name
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
___ ERROR at setup of TestAuthEndpoints.test_get_current_user_expired_token ____
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 203
      @pytest.mark.asyncio
      async def test_get_current_user_expired_token(self, client, existing_user):
          """Test getting current user with expired token"""
          # Create token that expires immediately
          token = create_access_token(
              data={"sub": existing_user.email},
              expires_delta=timedelta(seconds=-1)
          )

          response = client.get(
              "/api/v1/users/me",
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 401
          assert "expired" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
____________ ERROR at setup of TestAuthEndpoints.test_refresh_token ____________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 220
      @pytest.mark.asyncio
      async def test_refresh_token(self, client, existing_user):
          """Test refreshing access token"""
          # Get initial token
          login_data = {
              "username": existing_user.email,
              "password": "ExistingPassword123!"
          }

          login_response = client.post(
              "/api/v1/auth/token",
              data=login_data,
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )

          initial_token = login_response.json()["access_token"]

          # Refresh token
          response = client.post(
              "/api/v1/auth/refresh",
              headers={"Authorization": f"Bearer {initial_token}"}
          )

          assert response.status_code == 200
          data = response.json()
          assert "access_token" in data
          assert data["access_token"] != initial_token  # Should be new token
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
_______________ ERROR at setup of TestAuthEndpoints.test_logout ________________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 248
      @pytest.mark.asyncio
      async def test_logout(self, client, existing_user):
          """Test logout endpoint"""
          token = create_access_token(data={"sub": existing_user.email})

          response = client.post(
              "/api/v1/auth/logout",
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 200
          assert response.json()["message"] == "Successfully logged out"
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
_______ ERROR at setup of TestAuthEndpoints.test_password_reset_request ________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 261
      @pytest.mark.asyncio
      async def test_password_reset_request(self, client, existing_user):
          """Test password reset request"""
          response = client.post(
              "/api/v1/auth/password-reset",
              json={"email": existing_user.email}
          )

          assert response.status_code == 200
          assert "reset link" in response.json()["message"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
_______ ERROR at setup of TestAuthEndpoints.test_password_reset_confirm ________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 272
      @pytest.mark.asyncio
      async def test_password_reset_confirm(self, client, existing_user):
          """Test password reset confirmation"""
          # Generate reset token
          reset_token = create_access_token(
              data={"sub": existing_user.email, "type": "password_reset"},
              expires_delta=timedelta(hours=1)
          )

          response = client.post(
              "/api/v1/auth/password-reset/confirm",
              json={
                  "token": reset_token,
                  "new_password": "NewSecurePassword123!"
              }
          )

          assert response.status_code == 200
          assert "successfully reset" in response.json()["message"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
___________ ERROR at setup of TestAuthEndpoints.test_change_password ___________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 292
      @pytest.mark.asyncio
      async def test_change_password(self, client, existing_user):
          """Test changing password for authenticated user"""
          token = create_access_token(data={"sub": existing_user.email})

          response = client.post(
              "/api/v1/auth/change-password",
              json={
                  "current_password": "ExistingPassword123!",
                  "new_password": "NewSecurePassword456!"
              },
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 200
          assert "successfully changed" in response.json()["message"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 37
      @pytest.fixture
      def existing_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:37
____________ ERROR at setup of TestAuthEndpoints.test_verify_email _____________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 309
      @pytest.mark.asyncio
      async def test_verify_email(self, client, db_session):
          """Test email verification"""
          # Create unverified user
          user = User(
              email="unverified@example.com",
              full_name="Unverified User",
              hashed_password=get_password_hash("Password123!"),
              is_active=True,
          )
          db_session.add(user)
          db_session.commit()

          # Create verification token
          verify_token = create_access_token(
              data={"sub": user.email, "type": "email_verification"},
              expires_delta=timedelta(hours=24)
          )

          response = client.get(
              f"/api/v1/auth/verify-email?token={verify_token}"
          )

          assert response.status_code == 200
          assert "verified" in response.json()["message"].lower()

          # Check user is now verified
          db_session.refresh(user)
          assert user.is_active is True
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, user_data
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:309
_______________ ERROR at setup of TestRBACAuth.test_admin_access _______________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 429
      @pytest.mark.asyncio
      async def test_admin_access(self, client, admin_user):
          """Test admin accessing admin-only endpoint"""
          token = create_access_token(
              data={"sub": admin_user.email, "role": "admin"}
          )

          response = client.get(
              "/api/v1/admin/users",
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 200
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 401
      @pytest.fixture
      def admin_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, admin_user, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, regular_user, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:401
________ ERROR at setup of TestRBACAuth.test_regular_user_denied_admin _________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 443
      @pytest.mark.asyncio
      async def test_regular_user_denied_admin(self, client, regular_user):
          """Test regular user denied from admin endpoint"""
          token = create_access_token(
              data={"sub": regular_user.email, "role": "user"}
          )

          response = client.get(
              "/api/v1/admin/users",
              headers={"Authorization": f"Bearer {token}"}
          )

          assert response.status_code == 403
          assert "permission" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 415
      @pytest.fixture
      def regular_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, admin_user, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, regular_user, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:415
____________ ERROR at setup of TestRBACAuth.test_role_based_content ____________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 458
      @pytest.mark.asyncio
      async def test_role_based_content(self, client, admin_user, regular_user):
          """Test different content based on user role"""
          # Admin sees all data
          admin_token = create_access_token(
              data={"sub": admin_user.email, "role": "admin"}
          )

          admin_response = client.get(
              "/api/v1/dashboard",
              headers={"Authorization": f"Bearer {admin_token}"}
          )

          # Regular user sees filtered data
          user_token = create_access_token(
              data={"sub": regular_user.email, "role": "user"}
          )

          user_response = client.get(
              "/api/v1/dashboard",
              headers={"Authorization": f"Bearer {user_token}"}
          )

          assert admin_response.status_code == 200
          assert user_response.status_code == 200
          # Admin should have more data/features
          assert len(admin_response.json()) >= len(user_response.json())
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 401
      @pytest.fixture
      def admin_user(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, admin_user, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, regular_user, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:401
________ ERROR at setup of TestAuthIntegration.test_complete_auth_flow _________
file /home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py, line 491
      @pytest.mark.asyncio
      async def test_complete_auth_flow(self, client, db_session):
          """Test complete authentication workflow"""
          # 1. Register new user
          register_data = {
              "email": "newuser@example.com",
              "password": "NewUserPass123!",
              "full_name": "New User",
              "company": "New Company"
          }

          register_response = client.post(
              "/api/v1/auth/register",
              json=register_data
          )
          assert register_response.status_code == 201

          # 2. Login
          login_response = client.post(
              "/api/v1/auth/token",
              data={
                  "username": register_data["email"],
                  "password": register_data["password"]
              },
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )
          assert login_response.status_code == 200
          token = login_response.json()["access_token"]

          # 3. Access protected endpoint
          me_response = client.get(
              "/api/v1/users/me",
              headers={"Authorization": f"Bearer {token}"}
          )
          assert me_response.status_code == 200
          assert me_response.json()["email"] == register_data["email"]

          # 4. Change password
          change_pass_response = client.post(
              "/api/v1/auth/change-password",
              json={
                  "current_password": register_data["password"],
                  "new_password": "UpdatedPass456!"
              },
              headers={"Authorization": f"Bearer {token}"}
          )
          assert change_pass_response.status_code == 200

          # 5. Logout
          logout_response = client.post(
              "/api/v1/auth/logout",
              headers={"Authorization": f"Bearer {token}"}
          )
          assert logout_response.status_code == 200

          # 6. Login with new password
          new_login_response = client.post(
              "/api/v1/auth/token",
              data={
                  "username": register_data["email"],
                  "password": "UpdatedPass456!"
              },
              headers={"Content-Type": "application/x-www-form-urlencoded"}
          )
          assert new_login_response.status_code == 200
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:491
_____ ERROR at setup of TestDashboardEndpoints.test_get_dashboard_metrics ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestDashboardEndpoints.test_get_compliance_overview _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestDashboardEndpoints.test_get_assessment_summary _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_________ ERROR at setup of TestDashboardEndpoints.test_get_trend_data _________
E   TypeError: 'organization_id' is an invalid keyword argument for User
________ ERROR at setup of TestDashboardEndpoints.test_get_risk_matrix _________
E   TypeError: 'organization_id' is an invalid keyword argument for User
_______ ERROR at setup of TestDashboardEndpoints.test_get_activity_feed ________
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestDashboardEndpoints.test_get_upcoming_deadlines _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestDashboardEndpoints.test_get_team_performance ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestDashboardEndpoints.test_export_dashboard_report _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestDashboardEndpoints.test_get_compliance_gaps _______
E   TypeError: 'organization_id' is an invalid keyword argument for User
________ ERROR at setup of TestDashboardEndpoints.test_get_ai_insights _________
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestEvidenceEndpoints.test_list_evidence_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestEvidenceEndpoints.test_list_evidence_with_filters ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_by_id_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestEvidenceEndpoints.test_upload_evidence_success _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_ ERROR at setup of TestEvidenceEndpoints.test_upload_evidence_invalid_file_type _
E   TypeError: 'organization_id' is an invalid keyword argument for User
_ ERROR at setup of TestEvidenceEndpoints.test_upload_evidence_file_too_large __
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestEvidenceEndpoints.test_update_evidence_metadata _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestEvidenceEndpoints.test_validate_evidence_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestEvidenceEndpoints.test_delete_evidence_success _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestEvidenceEndpoints.test_download_evidence_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestEvidenceEndpoints.test_bulk_upload_evidence _______
E   TypeError: 'organization_id' is an invalid keyword argument for User
__ ERROR at setup of TestEvidenceEndpoints.test_link_evidence_to_requirement ___
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_by_assessment ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_________ ERROR at setup of TestEvidenceEndpoints.test_search_evidence _________
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestEvidenceEndpoints.test_evidence_expiry_check ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
________ ERROR at setup of TestFrameworkEndpoints.test_list_frameworks _________
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 64
      @pytest.mark.asyncio
      async def test_list_frameworks(self, client, auth_headers):
          """Test listing available frameworks"""
          response = client.get(
              "/api/v1/frameworks",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
          # Should include default frameworks like GDPR, ISO 27001, etc.
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
______ ERROR at setup of TestFrameworkEndpoints.test_get_framework_by_id _______
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 77
      @pytest.mark.asyncio
      async def test_get_framework_by_id(self, client, auth_headers):
          """Test retrieving specific framework"""
          framework_id = 1  # Assuming GDPR is ID 1

          response = client.get(
              f"/api/v1/frameworks/{framework_id}",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "id" in data
          assert "name" in data
          assert "requirements" in data
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
____ ERROR at setup of TestFrameworkEndpoints.test_get_framework_not_found _____
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 93
      @pytest.mark.asyncio
      async def test_get_framework_not_found(self, client, auth_headers):
          """Test retrieving non-existent framework"""
          response = client.get(
              "/api/v1/frameworks/99999",
              headers=auth_headers
          )

          assert response.status_code == 404
          assert "not found" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
___ ERROR at setup of TestFrameworkEndpoints.test_create_framework_non_admin ___
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 118
      @pytest.mark.asyncio
      async def test_create_framework_non_admin(self, client, auth_headers, framework_data):
          """Test non-admin cannot create framework"""
          response = client.post(
              "/api/v1/frameworks",
              json=framework_data,
              headers=auth_headers
          )

          assert response.status_code == 403
          assert "permission" in response.json()["detail"].lower()
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
___ ERROR at setup of TestFrameworkEndpoints.test_get_framework_requirements ___
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 170
      @pytest.mark.asyncio
      async def test_get_framework_requirements(self, client, auth_headers):
          """Test getting framework requirements"""
          framework_id = 1

          response = client.get(
              f"/api/v1/frameworks/{framework_id}/requirements",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
          if len(data) > 0:
              assert "id" in data[0]
              assert "title" in data[0]
              assert "category" in data[0]
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
_____ ERROR at setup of TestFrameworkEndpoints.test_get_requirement_detail _____
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 188
      @pytest.mark.asyncio
      async def test_get_requirement_detail(self, client, auth_headers):
          """Test getting specific requirement details"""
          framework_id = 1
          requirement_id = "GDPR-1"

          response = client.get(
              f"/api/v1/frameworks/{framework_id}/requirements/{requirement_id}",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["id"] == requirement_id
          assert "title" in data
          assert "description" in data
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
______ ERROR at setup of TestFrameworkEndpoints.test_framework_categories ______
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 272
      @pytest.mark.asyncio
      async def test_framework_categories(self, client, auth_headers):
          """Test getting framework categories"""
          framework_id = 1

          response = client.get(
              f"/api/v1/frameworks/{framework_id}/categories",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
          # Should include categories like Technical, Organizational, etc.
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
______ ERROR at setup of TestFrameworkEndpoints.test_framework_statistics ______
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 287
      @pytest.mark.asyncio
      async def test_framework_statistics(self, client, auth_headers):
          """Test getting framework statistics"""
          framework_id = 1

          response = client.get(
              f"/api/v1/frameworks/{framework_id}/statistics",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "total_requirements" in data
          assert "categories" in data
          assert "priority_breakdown" in data
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
_______ ERROR at setup of TestFrameworkEndpoints.test_framework_mapping ________
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 303
      @pytest.mark.asyncio
      async def test_framework_mapping(self, client, auth_headers):
          """Test framework requirement mapping"""
          response = client.get(
              "/api/v1/frameworks/mapping?from=GDPR&to=ISO27001",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "mappings" in data
          assert isinstance(data["mappings"], list)
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
________ ERROR at setup of TestFrameworkEndpoints.test_framework_export ________
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 316
      @pytest.mark.asyncio
      async def test_framework_export(self, client, auth_headers):
          """Test exporting framework"""
          framework_id = 1

          response = client.get(
              f"/api/v1/frameworks/{framework_id}/export?format=json",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "framework" in data
          assert "requirements" in data
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
________ ERROR at setup of TestFrameworkEndpoints.test_framework_search ________
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 353
      @pytest.mark.asyncio
      async def test_framework_search(self, client, auth_headers):
          """Test searching frameworks"""
          response = client.get(
              "/api/v1/frameworks/search?q=privacy",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "results" in data
          # Should return frameworks related to privacy (e.g., GDPR)
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
___ ERROR at setup of TestFrameworkEndpoints.test_framework_recommendations ____
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 366
      @pytest.mark.asyncio
      async def test_framework_recommendations(self, client, auth_headers):
          """Test AI-powered framework recommendations"""
          request_data = {
              "industry": "healthcare",
              "company_size": "medium",
              "regions": ["EU", "US"],
              "data_types": ["health", "personal"]
          }

          response = client.post(
              "/api/v1/frameworks/recommendations",
              json=request_data,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "recommended" in data
          assert "reasoning" in data
          # Should recommend HIPAA, GDPR, etc.
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, framework_data, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:24
_ ERROR at setup of TestFrameworkIntegration.test_framework_assessment_integration _
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 393
      @pytest.mark.asyncio
      async def test_framework_assessment_integration(self, client, auth_headers, db_session):
          """Test framework integration with assessments"""
          # 1. Get framework
          framework_response = client.get(
              "/api/v1/frameworks/1",
              headers=auth_headers
          )
          assert framework_response.status_code == 200
          framework = framework_response.json()

          # 2. Create assessment from framework
          assessment_data = {
              "framework_id": framework["id"],
              "name": f"{framework['name']} Assessment"
          }

          assessment_response = client.post(
              "/api/v1/assessments",
              json=assessment_data,
              headers=auth_headers
          )
          assert assessment_response.status_code == 201
          assessment = assessment_response.json()

          # 3. Verify requirements match
          requirements_response = client.get(
              f"/api/v1/assessments/{assessment['id']}/requirements",
              headers=auth_headers
          )
          assert requirements_response.status_code == 200
          assessment_reqs = requirements_response.json()

          assert len(assessment_reqs) == len(framework.get("requirements", []))
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:393
__ ERROR at setup of TestFrameworkIntegration.test_multi_framework_compliance __
file /home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py, line 428
      @pytest.mark.asyncio
      async def test_multi_framework_compliance(self, client, auth_headers):
          """Test compliance across multiple frameworks"""
          # Get overlapping requirements between GDPR and ISO 27001
          mapping_response = client.get(
              "/api/v1/frameworks/mapping?from=GDPR&to=ISO27001",
              headers=auth_headers
          )
          assert mapping_response.status_code == 200

          mappings = mapping_response.json()["mappings"]

          # Verify mapped requirements have similar themes
          for mapping in mappings[:5]:  # Check first 5 mappings
              assert "source_requirement" in mapping
              assert "target_requirement" in mapping
              assert "similarity_score" in mapping
              assert mapping["similarity_score"] > 0.5  # Reasonable similarity
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:428
____ ERROR at setup of TestFrameworksEndpoints.test_list_frameworks_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_ ERROR at setup of TestFrameworksEndpoints.test_list_frameworks_with_filters __
E   TypeError: 'organization_id' is an invalid keyword argument for User
__ ERROR at setup of TestFrameworksEndpoints.test_get_framework_by_id_success __
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestFrameworksEndpoints.test_get_framework_not_found ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_create_framework_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_ ERROR at setup of TestFrameworksEndpoints.test_create_framework_duplicate_name _
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_update_framework_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_delete_framework_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
__ ERROR at setup of TestFrameworksEndpoints.test_get_framework_requirements ___
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestFrameworksEndpoints.test_get_framework_controls _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestFrameworksEndpoints.test_map_frameworks_success _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_import_framework_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_export_framework_success ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestFrameworksEndpoints.test_get_framework_statistics ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_list_policies_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestPoliciesEndpoints.test_list_policies_with_filters ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
____ ERROR at setup of TestPoliciesEndpoints.test_get_policy_by_id_success _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_create_policy_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestPoliciesEndpoints.test_generate_policy_with_ai _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_update_policy_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestPoliciesEndpoints.test_submit_policy_for_approval ____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestPoliciesEndpoints.test_approve_policy_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
___ ERROR at setup of TestPoliciesEndpoints.test_reject_policy_with_comments ___
E   TypeError: 'organization_id' is an invalid keyword argument for User
_______ ERROR at setup of TestPoliciesEndpoints.test_get_policy_versions _______
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_clone_policy_success _______
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_delete_policy_success ______
E   TypeError: 'organization_id' is an invalid keyword argument for User
______ ERROR at setup of TestPoliciesEndpoints.test_export_policy_to_pdf _______
E   TypeError: 'organization_id' is an invalid keyword argument for User
_____ ERROR at setup of TestPoliciesEndpoints.test_check_policy_compliance _____
E   TypeError: 'organization_id' is an invalid keyword argument for User
_________ ERROR at setup of TestPoliciesEndpoints.test_search_policies _________
E   TypeError: 'organization_id' is an invalid keyword argument for User
________ ERROR at setup of TestPolicyEndpoints.test_generate_policy_ai _________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 52
      @pytest.mark.asyncio
      async def test_generate_policy_ai(self, client, auth_headers, policy_request_data):
          """Test AI-powered policy generation"""
          with patch('services.ai.ai_policy.generate_policy') as mock_generate:
              mock_generate.return_value = {
                  "policy_content": "# Privacy Policy\n\nThis is the generated policy...",
                  "sections": ["Introduction", "Data Collection", "Data Use"],
                  "compliance_score": 95.0,
                  "recommendations": []
              }

              response = client.post(
                  "/api/v1/ai/policies/generate",
                  json=policy_request_data,
                  headers=auth_headers
              )

              assert response.status_code == 201
              data = response.json()
              assert "policy_content" in data
              assert "compliance_score" in data
              assert data["compliance_score"] == 95.0
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_______ ERROR at setup of TestPolicyEndpoints.test_list_policy_templates _______
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 75
      @pytest.mark.asyncio
      async def test_list_policy_templates(self, client, auth_headers):
          """Test listing available policy templates"""
          response = client.get(
              "/api/v1/policies/templates",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
          assert len(data) > 0

          # Verify template structure
          template = data[0]
          assert "id" in template
          assert "name" in template
          assert "description" in template
          assert "framework" in template
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
________ ERROR at setup of TestPolicyEndpoints.test_get_policy_template ________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 95
      @pytest.mark.asyncio
      async def test_get_policy_template(self, client, auth_headers):
          """Test retrieving specific policy template"""
          template_id = "privacy-policy-gdpr"

          response = client.get(
              f"/api/v1/policies/templates/{template_id}",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["id"] == template_id
          assert "content" in data
          assert "variables" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
____ ERROR at setup of TestPolicyEndpoints.test_create_policy_from_template ____
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 111
      @pytest.mark.asyncio
      async def test_create_policy_from_template(self, client, auth_headers):
          """Test creating policy from template"""
          request_data = {
              "template_id": "privacy-policy-gdpr",
              "variables": {
                  "company_name": "TestCorp",
                  "contact_email": "privacy@testcorp.com",
                  "data_controller": "TestCorp Ltd",
                  "dpo_email": "dpo@testcorp.com"
              }
          }

          response = client.post(
              "/api/v1/policies/from-template",
              json=request_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert "id" in data
          assert "content" in data
          assert "TestCorp" in data["content"]
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
________ ERROR at setup of TestPolicyEndpoints.test_list_user_policies _________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 136
      @pytest.mark.asyncio
      async def test_list_user_policies(self, client, auth_headers):
          """Test listing user's generated policies"""
          response = client.get(
              "/api/v1/policies",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_________ ERROR at setup of TestPolicyEndpoints.test_get_policy_by_id __________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 148
      @pytest.mark.asyncio
      async def test_get_policy_by_id(self, client, auth_headers):
          """Test retrieving specific policy"""
          policy_id = str(uuid4())

          with patch('api.routers.policies.get_policy') as mock_get:
              mock_get.return_value = {
                  "id": policy_id,
                  "name": "Privacy Policy",
                  "type": "privacy_policy",
                  "content": "Policy content here...",
                  "created_at": datetime.now(timezone.utc).isoformat()
              }

              response = client.get(
                  f"/api/v1/policies/{policy_id}",
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert data["id"] == policy_id
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
___________ ERROR at setup of TestPolicyEndpoints.test_update_policy ___________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 171
      @pytest.mark.asyncio
      async def test_update_policy(self, client, auth_headers):
          """Test updating policy content"""
          policy_id = str(uuid4())
          update_data = {
              "name": "Updated Privacy Policy",
              "content": "Updated content...",
              "version": "2.0"
          }

          response = client.put(
              f"/api/v1/policies/{policy_id}",
              json=update_data,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["name"] == update_data["name"]
          assert data["version"] == "2.0"
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
___________ ERROR at setup of TestPolicyEndpoints.test_delete_policy ___________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 192
      @pytest.mark.asyncio
      async def test_delete_policy(self, client, auth_headers):
          """Test deleting policy"""
          policy_id = str(uuid4())

          response = client.delete(
              f"/api/v1/policies/{policy_id}",
              headers=auth_headers
          )

          assert response.status_code == 204
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_________ ERROR at setup of TestPolicyEndpoints.test_policy_validation _________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 204
      @pytest.mark.asyncio
      async def test_policy_validation(self, client, auth_headers):
          """Test policy compliance validation"""
          validation_request = {
              "policy_content": "This is our privacy policy...",
              "framework": "GDPR",
              "policy_type": "privacy_policy"
          }

          with patch('services.compliance.validate_policy') as mock_validate:
              mock_validate.return_value = {
                  "is_compliant": False,
                  "score": 65.0,
                  "issues": [
                      {"severity": "high", "issue": "Missing data retention period"},
                      {"severity": "medium", "issue": "No mention of user rights"}
                  ],
                  "suggestions": ["Add section on data retention", "Include GDPR user rights"]
              }

              response = client.post(
                  "/api/v1/policies/validate",
                  json=validation_request,
                  headers=auth_headers
              )

              assert response.status_code == 200
              data = response.json()
              assert data["is_compliant"] is False
              assert data["score"] == 65.0
              assert len(data["issues"]) == 2
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_________ ERROR at setup of TestPolicyEndpoints.test_policy_comparison _________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 236
      @pytest.mark.asyncio
      async def test_policy_comparison(self, client, auth_headers):
          """Test comparing policies for changes"""
          comparison_request = {
              "policy_id_1": str(uuid4()),
              "policy_id_2": str(uuid4())
          }

          response = client.post(
              "/api/v1/policies/compare",
              json=comparison_request,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "differences" in data
          assert "similarity_score" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
___________ ERROR at setup of TestPolicyEndpoints.test_policy_export ___________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 255
      @pytest.mark.asyncio
      async def test_policy_export(self, client, auth_headers):
          """Test exporting policy in different formats"""
          policy_id = str(uuid4())

          # Export as PDF
          response = client.get(
              f"/api/v1/policies/{policy_id}/export?format=pdf",
              headers=auth_headers
          )

          assert response.status_code == 200
          assert response.headers["content-type"] == "application/pdf"

          # Export as DOCX
          response = client.get(
              f"/api/v1/policies/{policy_id}/export?format=docx",
              headers=auth_headers
          )

          assert response.status_code == 200
          assert "application/vnd.openxmlformats" in response.headers["content-type"]
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
______ ERROR at setup of TestPolicyEndpoints.test_policy_version_history _______
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 278
      @pytest.mark.asyncio
      async def test_policy_version_history(self, client, auth_headers):
          """Test retrieving policy version history"""
          policy_id = str(uuid4())

          response = client.get(
              f"/api/v1/policies/{policy_id}/versions",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert isinstance(data, list)
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_____ ERROR at setup of TestPolicyEndpoints.test_policy_approval_workflow ______
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 292
      @pytest.mark.asyncio
      async def test_policy_approval_workflow(self, client, auth_headers):
          """Test policy approval workflow"""
          policy_id = str(uuid4())

          # Submit for approval
          response = client.post(
              f"/api/v1/policies/{policy_id}/submit-approval",
              json={"approvers": ["legal@company.com", "compliance@company.com"]},
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["status"] == "pending_approval"
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
__________ ERROR at setup of TestPolicyEndpoints.test_policy_publish ___________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 308
      @pytest.mark.asyncio
      async def test_policy_publish(self, client, auth_headers):
          """Test publishing policy"""
          policy_id = str(uuid4())

          response = client.post(
              f"/api/v1/policies/{policy_id}/publish",
              json={"effective_date": "2024-02-01"},
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["status"] == "published"
          assert "published_url" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
______ ERROR at setup of TestPolicyEndpoints.test_batch_policy_generation ______
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 324
      @pytest.mark.asyncio
      async def test_batch_policy_generation(self, client, auth_headers):
          """Test generating multiple policies at once"""
          batch_request = {
              "policies": [
                  {"type": "privacy_policy", "framework": "GDPR"},
                  {"type": "cookie_policy", "framework": "GDPR"},
                  {"type": "terms_of_service", "framework": "general"}
              ],
              "business_context": {
                  "company_name": "BatchCorp",
                  "industry": "technology"
              }
          }

          response = client.post(
              "/api/v1/ai/policies/batch-generate",
              json=batch_request,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()
          assert "policies" in data
          assert len(data["policies"]) == 3
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
_______ ERROR at setup of TestPolicyEndpoints.test_policy_ai_enhancement _______
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 350
      @pytest.mark.asyncio
      async def test_policy_ai_enhancement(self, client, auth_headers):
          """Test AI enhancement of existing policy"""
          enhancement_request = {
              "policy_id": str(uuid4()),
              "enhancements": ["improve_clarity", "add_examples", "strengthen_compliance"],
              "target_framework": "GDPR"
          }

          response = client.post(
              "/api/v1/ai/policies/enhance",
              json=enhancement_request,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "enhanced_content" in data
          assert "improvements" in data
          assert "new_compliance_score" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
________ ERROR at setup of TestPolicyEndpoints.test_policy_translation _________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 371
      @pytest.mark.asyncio
      async def test_policy_translation(self, client, auth_headers):
          """Test policy translation"""
          policy_id = str(uuid4())

          response = client.post(
              f"/api/v1/policies/{policy_id}/translate",
              json={"target_language": "es-ES"},
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["language"] == "es-ES"
          assert "translated_content" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
___________ ERROR at setup of TestPolicyEndpoints.test_policy_search ___________
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 387
      @pytest.mark.asyncio
      async def test_policy_search(self, client, auth_headers):
          """Test searching policies"""
          response = client.get(
              "/api/v1/policies/search?q=data+retention&type=privacy_policy",
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert "results" in data
          assert "total_count" in data
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
___ ERROR at setup of TestPolicyEndpoints.test_policy_compliance_monitoring ____
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 400
      @pytest.mark.asyncio
      async def test_policy_compliance_monitoring(self, client, auth_headers):
          """Test policy compliance monitoring setup"""
          policy_id = str(uuid4())
          monitoring_config = {
              "check_frequency": "monthly",
              "frameworks": ["GDPR", "CCPA"],
              "alert_email": "compliance@company.com"
          }

          response = client.post(
              f"/api/v1/policies/{policy_id}/monitoring",
              json=monitoring_config,
              headers=auth_headers
          )

          assert response.status_code == 200
          data = response.json()
          assert data["monitoring_enabled"] is True
          assert data["next_check"] is not None
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 24
      @pytest.fixture
      def auth_headers(self, sample_user):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, policy_request_data, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:24
____ ERROR at setup of TestPolicyIntegration.test_complete_policy_workflow _____
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 426
      @pytest.mark.asyncio
      async def test_complete_policy_workflow(self, client, auth_headers, db_session):
          """Test complete policy generation and management workflow"""
          # 1. Generate policy with AI
          generation_request = {
              "policy_type": "privacy_policy",
              "framework": "GDPR",
              "business_context": {
                  "company_name": "WorkflowTest Inc",
                  "industry": "saas"
              }
          }

          generate_response = client.post(
              "/api/v1/ai/policies/generate",
              json=generation_request,
              headers=auth_headers
          )
          assert generate_response.status_code == 201
          policy = generate_response.json()
          policy_id = policy["id"]

          # 2. Validate policy compliance
          validate_response = client.post(
              "/api/v1/policies/validate",
              json={
                  "policy_content": policy["policy_content"],
                  "framework": "GDPR"
              },
              headers=auth_headers
          )
          assert validate_response.status_code == 200
          validation = validate_response.json()

          # 3. Enhance policy if needed
          if validation["score"] < 90:
              enhance_response = client.post(
                  "/api/v1/ai/policies/enhance",
                  json={
                      "policy_id": policy_id,
                      "enhancements": ["strengthen_compliance"]
                  },
                  headers=auth_headers
              )
              assert enhance_response.status_code == 200

          # 4. Submit for approval
          approval_response = client.post(
              f"/api/v1/policies/{policy_id}/submit-approval",
              json={"approvers": ["legal@test.com"]},
              headers=auth_headers
          )
          assert approval_response.status_code == 200

          # 5. Publish policy
          publish_response = client.post(
              f"/api/v1/policies/{policy_id}/publish",
              json={"effective_date": "2024-01-15"},
              headers=auth_headers
          )
          assert publish_response.status_code == 200

          # 6. Export policy
          export_response = client.get(
              f"/api/v1/policies/{policy_id}/export?format=pdf",
              headers=auth_headers
          )
          assert export_response.status_code == 200
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:426
_ ERROR at setup of TestPolicyIntegration.test_multi_framework_policy_generation _
file /home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py, line 495
      @pytest.mark.asyncio
      async def test_multi_framework_policy_generation(self, client, auth_headers):
          """Test generating policy compliant with multiple frameworks"""
          request_data = {
              "policy_type": "privacy_policy",
              "frameworks": ["GDPR", "CCPA", "LGPD"],
              "business_context": {
                  "company_name": "GlobalCorp",
                  "regions": ["EU", "US", "Brazil"]
              }
          }

          response = client.post(
              "/api/v1/ai/policies/multi-framework",
              json=request_data,
              headers=auth_headers
          )

          assert response.status_code == 201
          data = response.json()

          # Verify policy covers all frameworks
          assert "gdpr" in data["policy_content"].lower()
          assert "ccpa" in data["policy_content"].lower()
          assert "lgpd" in data["policy_content"].lower()

          # Check compliance scores for each framework
          assert "compliance_scores" in data
          assert len(data["compliance_scores"]) == 3
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/api/test_policy_endpoints.py:495
___ ERROR at setup of TestAssessmentLead.test_create_assessment_lead_minimal ___
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 28
      def test_create_assessment_lead_minimal(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:28
__ ERROR at setup of TestAssessmentLead.test_create_assessment_lead_with_utm ___
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 46
      def test_create_assessment_lead_with_utm(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:46
_ ERROR at setup of TestAssessmentLead.test_assessment_lead_email_unique_constraint _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 71
      def test_assessment_lead_email_unique_constraint(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:71
____ ERROR at setup of TestAssessmentLead.test_assessment_lead_score_update ____
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 86
      def test_assessment_lead_score_update(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:86
_ ERROR at setup of TestFreemiumAssessmentSession.test_create_assessment_session _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 108
      def test_create_assessment_session(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:108
_ ERROR at setup of TestFreemiumAssessmentSession.test_assessment_session_ai_responses_storage _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 132
      def test_assessment_session_ai_responses_storage(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:132
_ ERROR at setup of TestFreemiumAssessmentSession.test_assessment_session_expiration _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 163
      def test_assessment_session_expiration(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:163
_________ ERROR at setup of TestAIQuestionBank.test_create_ai_question _________
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 189
      def test_create_ai_question(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:189
_ ERROR at setup of TestAIQuestionBank.test_ai_question_weighting_and_difficulty _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 214
      def test_ai_question_weighting_and_difficulty(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:214
____ ERROR at setup of TestLeadScoringEvent.test_create_lead_scoring_event _____
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 234
      def test_create_lead_scoring_event(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:234
____ ERROR at setup of TestLeadScoringEvent.test_lead_scoring_with_metadata ____
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 258
      def test_lead_scoring_with_metadata(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:258
______ ERROR at setup of TestConversionEvent.test_create_conversion_event ______
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 289
      def test_create_conversion_event(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:289
_ ERROR at setup of TestFreemiumModelRelationships.test_lead_to_sessions_relationship _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 322
      def test_lead_to_sessions_relationship(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:322
_ ERROR at setup of TestFreemiumModelRelationships.test_cascade_delete_behavior _
file /home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py, line 347
      def test_cascade_delete_behavior(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/database/test_freemium_models.py:347
___________ ERROR at setup of TestBaseRepository.test_create_entity ____________
E   NameError: name 'BaseRepository' is not defined
_____________ ERROR at setup of TestBaseRepository.test_get_by_id ______________
E   NameError: name 'BaseRepository' is not defined
______ ERROR at setup of TestBaseRepository.test_get_all_with_pagination _______
E   NameError: name 'BaseRepository' is not defined
___________ ERROR at setup of TestBaseRepository.test_update_entity ____________
E   NameError: name 'BaseRepository' is not defined
___________ ERROR at setup of TestBaseRepository.test_delete_entity ____________
E   NameError: name 'BaseRepository' is not defined
___________ ERROR at setup of TestBaseRepository.test_count_entities ___________
E   NameError: name 'BaseRepository' is not defined
_____ ERROR at setup of TestAssessmentRepository.test_get_by_organization ______
E   NameError: name 'AssessmentRepository' is not defined
________ ERROR at setup of TestAssessmentRepository.test_get_by_status _________
E   NameError: name 'AssessmentRepository' is not defined
______ ERROR at setup of TestAssessmentRepository.test_get_with_framework ______
E   NameError: name 'AssessmentRepository' is not defined
_______ ERROR at setup of TestAssessmentRepository.test_update_progress ________
E   NameError: name 'AssessmentRepository' is not defined
______ ERROR at setup of TestPolicyRepository.test_get_approved_policies _______
E   NameError: name 'PolicyRepository' is not defined
_________ ERROR at setup of TestPolicyRepository.test_get_by_category __________
E   NameError: name 'PolicyRepository' is not defined
_________ ERROR at setup of TestPolicyRepository.test_search_policies __________
E   NameError: name 'PolicyRepository' is not defined
___________ ERROR at setup of TestPolicyRepository.test_clone_policy ___________
E   NameError: name 'PolicyRepository' is not defined
_______ ERROR at setup of TestEvidenceRepository.test_get_by_assessment ________
E   NameError: name 'EvidenceRepository' is not defined
_____ ERROR at setup of TestEvidenceRepository.test_get_validated_evidence _____
E   NameError: name 'EvidenceRepository' is not defined
_______ ERROR at setup of TestEvidenceRepository.test_validate_evidence ________
E   NameError: name 'EvidenceRepository' is not defined
__________ ERROR at setup of TestEvidenceRepository.test_check_expiry __________
E   NameError: name 'EvidenceRepository' is not defined
____________ ERROR at setup of TestUserRepository.test_get_by_email ____________
E   NameError: name 'UserRepository' is not defined
__________ ERROR at setup of TestUserRepository.test_get_active_users __________
E   NameError: name 'UserRepository' is not defined
_________ ERROR at setup of TestUserRepository.test_update_last_login __________
E   NameError: name 'UserRepository' is not defined
________ ERROR at setup of TestUserRepository.test_create_user_success _________
E   NameError: name 'UserRepository' is not defined
____ ERROR at setup of TestUserRepository.test_create_user_duplicate_email _____
E   NameError: name 'UserRepository' is not defined
_______ ERROR at setup of TestUserRepository.test_get_user_by_id_success _______
E   NameError: name 'UserRepository' is not defined
______ ERROR at setup of TestUserRepository.test_get_user_by_id_not_found ______
E   NameError: name 'UserRepository' is not defined
_____ ERROR at setup of TestUserRepository.test_get_user_by_email_success ______
E   NameError: name 'UserRepository' is not defined
_ ERROR at setup of TestUserRepository.test_get_user_by_email_case_insensitive _
E   NameError: name 'UserRepository' is not defined
________ ERROR at setup of TestUserRepository.test_update_user_success _________
E   NameError: name 'UserRepository' is not defined
________ ERROR at setup of TestUserRepository.test_update_user_password ________
E   NameError: name 'UserRepository' is not defined
________ ERROR at setup of TestUserRepository.test_delete_user_success _________
E   NameError: name 'UserRepository' is not defined
_______ ERROR at setup of TestUserRepository.test_delete_user_not_found ________
E   NameError: name 'UserRepository' is not defined
_________ ERROR at setup of TestUserRepository.test_verify_user_email __________
E   NameError: name 'UserRepository' is not defined
_____ ERROR at setup of TestUserRepository.test_list_users_with_pagination _____
E   NameError: name 'UserRepository' is not defined
_______ ERROR at setup of TestUserRepository.test_list_users_with_filter _______
E   NameError: name 'UserRepository' is not defined
_____ ERROR at setup of TestUserRepository.test_authenticate_user_success ______
E   NameError: name 'UserRepository' is not defined
__ ERROR at setup of TestUserRepository.test_authenticate_user_wrong_password __
E   NameError: name 'UserRepository' is not defined
_____ ERROR at setup of TestUserRepository.test_authenticate_inactive_user _____
E   NameError: name 'UserRepository' is not defined
_________ ERROR at setup of TestUserRepository.test_update_last_login __________
E   NameError: name 'UserRepository' is not defined
________ ERROR at setup of TestUserRepository.test_search_users_by_name ________
E   NameError: name 'UserRepository' is not defined
____________ ERROR at setup of TestUserRepository.test_count_users _____________
E   NameError: name 'UserRepository' is not defined
_________ ERROR at setup of TestUserRepository.test_bulk_create_users __________
E   NameError: name 'UserRepository' is not defined
______ ERROR at setup of TestUserRepository.test_database_error_handling _______
E   NameError: name 'UserRepository' is not defined
_ ERROR at setup of TestUserOnboardingFlow.test_complete_user_onboarding_workflow _
file /home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py, line 18
      def test_complete_user_onboarding_workflow(
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:18
_ ERROR at setup of TestOnboardingIntegration.test_onboarding_creates_audit_trail _
file /home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py, line 407
      def test_onboarding_creates_audit_trail(
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:407
__ ERROR at setup of TestAIAssessmentEndpoints.test_ai_help_endpoint_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 123
      def test_ai_help_endpoint_success(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_ai_help_endpoint_authentication_required _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 143
      def test_ai_help_endpoint_authentication_required(self, client):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_ai_help_endpoint_invalid_framework _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 159
      def test_ai_help_endpoint_invalid_framework(self, client,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_followup_questions_endpoint_success _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 171
      def test_followup_questions_endpoint_success(self, client,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
__ ERROR at setup of TestAIAssessmentEndpoints.test_analysis_endpoint_success __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 198
      def test_analysis_endpoint_success(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_recommendations_endpoint_success _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 216
      def test_recommendations_endpoint_success(self, client,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
__ ERROR at setup of TestAIAssessmentEndpoints.test_feedback_endpoint_success __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 228
      def test_feedback_endpoint_success(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
__ ERROR at setup of TestAIAssessmentEndpoints.test_metrics_endpoint_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 240
      def test_metrics_endpoint_success(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_ai_service_timeout_handling _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 252
      def test_ai_service_timeout_handling(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_ai_quota_exceeded_handling __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 264
      def test_ai_quota_exceeded_handling(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_ai_content_filter_handling __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 276
      def test_ai_content_filter_handling(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_invalid_request_data_validation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 292
      def test_invalid_request_data_validation(self, client,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_ ERROR at setup of TestAIAssessmentEndpoints.test_business_profile_not_found __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 300
      def test_business_profile_not_found(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 30
      @pytest.fixture(autouse=True)
      def setup_mocks(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_mocks, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:30
_______ ERROR at setup of TestAIRateLimiting.test_ai_help_rate_limiting ________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 382
      def test_ai_help_rate_limiting(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 325
      @pytest.fixture(autouse=True)
      def setup_auth(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_auth, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:325
_____ ERROR at setup of TestAIRateLimiting.test_ai_analysis_rate_limiting ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 399
      def test_ai_analysis_rate_limiting(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 325
      @pytest.fixture(autouse=True)
      def setup_auth(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_auth, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:325
_ ERROR at setup of TestAIRateLimiting.test_regular_endpoints_higher_rate_limit _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 418
      def test_regular_endpoints_higher_rate_limit(self, client,
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 325
      @pytest.fixture(autouse=True)
      def setup_auth(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, setup_auth, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py:325
__ ERROR at setup of TestAIErrorHandling.test_concurrent_ai_requests_handling __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_ai_assessments.py, line 485
      def test_concurrent_ai_requests_handling(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestAnalyticsEndpoints.test_analytics_dashboard_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 22
      def test_analytics_dashboard_success(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAnalyticsEndpoints.test_usage_analytics_endpoint ____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 77
      def test_usage_analytics_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAnalyticsEndpoints.test_cost_analytics_endpoint _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 103
      def test_cost_analytics_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAnalyticsEndpoints.test_system_alerts_endpoint _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 131
      def test_system_alerts_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAnalyticsEndpoints.test_resolve_alert_endpoint _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 160
      def test_resolve_alert_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAnalyticsEndpoints.test_resolve_nonexistent_alert ____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 177
      def test_resolve_nonexistent_alert(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestPerformanceEndpoints.test_performance_metrics_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 196
      def test_performance_metrics_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestPerformanceEndpoints.test_optimize_performance_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 232
      def test_optimize_performance_endpoint(self, client, authenticated_headers
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_______ ERROR at setup of TestCacheEndpoints.test_cache_metrics_endpoint _______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 263
      def test_cache_metrics_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
________ ERROR at setup of TestCacheEndpoints.test_clear_cache_endpoint ________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 286
      def test_clear_cache_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestCacheEndpoints.test_analytics_error_handling ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_analytics_endpoints.py, line 302
      def test_analytics_error_handling(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_________ ERROR at setup of TestChatEndpoints.test_create_conversation _________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 24
      def test_create_conversation(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestChatEndpoints.test_send_message_to_conversation _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 48
      def test_send_message_to_conversation(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestChatEndpoints.test_get_evidence_recommendations _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 76
      def test_get_evidence_recommendations(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_________ ERROR at setup of TestChatEndpoints.test_compliance_analysis _________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 99
      def test_compliance_analysis(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_______ ERROR at setup of TestChatEndpoints.test_get_conversations_list ________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 131
      def test_get_conversations_list(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_______ ERROR at setup of TestChatEndpoints.test_conversation_not_found ________
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 142
      def test_conversation_not_found(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestChatEndpoints.test_compliance_analysis_missing_business_profile _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 149
      def test_compliance_analysis_missing_business_profile(self, client,
E       fixture 'another_authenticated_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py:149
___ ERROR at setup of TestChatValidation.test_invalid_framework_for_analysis ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 164
      def test_invalid_framework_for_analysis(self, client, authenticated_headers
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestChatValidation.test_missing_message_content _______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 172
      def test_missing_message_content(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestChatValidation.test_ai_assistant_error_handling _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_chat_endpoints.py, line 188
      def test_ai_assistant_error_handling(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatEndpoints.test_context_aware_recommendations_success _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 25
      def test_context_aware_recommendations_success(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatEndpoints.test_evidence_collection_workflow_generation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 64
      def test_evidence_collection_workflow_generation(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestEnhancedChatEndpoints.test_policy_generation ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 117
      def test_policy_generation(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestEnhancedChatEndpoints.test_smart_compliance_guidance __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 176
      def test_smart_compliance_guidance(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatEndpoints.test_missing_business_profile_error _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 215
      def test_missing_business_profile_error(self, client, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py:215
__ ERROR at setup of TestEnhancedChatEndpoints.test_ai_service_error_handling __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 246
      def test_ai_service_error_handling(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatEndpoints.test_invalid_framework_parameter _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 259
      def test_invalid_framework_parameter(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatValidation.test_workflow_generation_parameter_validation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 272
      def test_workflow_generation_parameter_validation(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatValidation.test_policy_generation_parameter_validation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 279
      def test_policy_generation_parameter_validation(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEnhancedChatValidation.test_smart_guidance_parameter_validation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_enhanced_chat_endpoints.py, line 286
      def test_smart_guidance_parameter_validation(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_classify_single_evidence _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 33
      def test_classify_single_evidence(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_classify_evidence_force_reclassify _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 62
      def test_classify_evidence_force_reclassify(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_bulk_classify_evidence __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 96
      def test_bulk_classify_evidence(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_get_control_mapping_suggestions _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 128
      def test_get_control_mapping_suggestions(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_get_classification_statistics _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 156
      def test_get_classification_statistics(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_classify_nonexistent_evidence _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 174
      def test_classify_nonexistent_evidence(self, client, authenticated_headers
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationAPI.test_bulk_classify_with_invalid_evidence _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 184
      def test_bulk_classify_with_invalid_evidence(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationValidation.test_bulk_classify_too_many_items _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 204
      def test_bulk_classify_too_many_items(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationValidation.test_invalid_confidence_threshold _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 213
      def test_invalid_confidence_threshold(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceClassificationValidation.test_classification_ai_service_error _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_classification.py, line 221
      def test_classification_ai_service_error(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestEvidenceEndpoints.test_create_evidence_item_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 30
      def test_create_evidence_item_success(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_create_evidence_item_validation_error _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 58
      def test_create_evidence_item_validation_error(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_create_evidence_item_unauthenticated _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 76
      def test_create_evidence_item_unauthenticated(self,
E       fixture 'sample_compliance_framework' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py:76
___ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_items_success ____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 92
      def test_get_evidence_items_success(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_items_empty _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 107
      def test_get_evidence_items_empty(self, client, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py:107
_ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_items_with_filtering _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 127
      def test_get_evidence_items_with_filtering(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_item_by_id_success _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 147
      def test_get_evidence_item_by_id_success(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_item_by_id_not_found _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 161
      def test_get_evidence_item_by_id_not_found(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_item_unauthorized_access _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 170
      def test_get_evidence_item_unauthorized_access(self, client,
E       fixture 'another_authenticated_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py:170
__ ERROR at setup of TestEvidenceEndpoints.test_update_evidence_item_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 182
      def test_update_evidence_item_success(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestEvidenceEndpoints.test_update_evidence_item_partial ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 199
      def test_update_evidence_item_partial(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestEvidenceEndpoints.test_delete_evidence_item_success ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 212
      def test_delete_evidence_item_success(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceEndpoints.test_delete_evidence_item_unauthorized _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 223
      def test_delete_evidence_item_unauthorized(self, client,
E       fixture 'another_authenticated_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py:223
___ ERROR at setup of TestEvidenceEndpoints.test_bulk_update_evidence_status ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 231
      def test_bulk_update_evidence_status(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestEvidenceEndpoints.test_get_evidence_statistics _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 265
      def test_get_evidence_statistics(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestEvidenceEndpoints.test_search_evidence_items ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 282
      def test_search_evidence_items(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceValidationEndpoints.test_validate_evidence_quality _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 306
      def test_validate_evidence_quality(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceValidationEndpoints.test_identify_evidence_requirements _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 329
      def test_identify_evidence_requirements(self, client, authenticated_headers
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidenceValidationEndpoints.test_configure_evidence_automation _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 351
      def test_configure_evidence_automation(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestEvidencePaginationAndSorting.test_evidence_pagination __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 374
      def test_evidence_pagination(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestEvidencePaginationAndSorting.test_evidence_sorting ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_evidence_endpoints.py, line 416
      def test_evidence_sorting(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestFreemiumEmailCapture.test_capture_email_success _____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py, line 65
      @pytest.mark.asyncio
      async def test_capture_email_success(self, async_client: AsyncClient,
          db_session):
          """Test successful email capture with UTM parameters."""
          payload = {'email': 'test@example.com', 'utm_source': 'google',
              'utm_campaign': 'compliance_assessment', 'utm_medium': 'cpc',
              'utm_term': 'gdpr_compliance', 'utm_content': 'cta_button',
              'consent_marketing': True, 'consent_terms': True}
          response = await async_client.post('/api/v1/freemium/capture-email',
              json=payload)
          assert response.status_code == HTTP_OK
          data = response.json()
          assert data['success'] is True
          assert 'token' in data
          assert len(data['token']) > 20
          assert data['message'] == 'Email captured successfully'
          token_data = verify_freemium_token(data['token'])
          assert token_data['email'] == 'test@example.com'
          assert token_data['utm_source'] == 'google'
          assert token_data['utm_campaign'] == 'compliance_assessment'
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, completed_freemium_session, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, freemium_session, freemium_token, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:65
___ ERROR at setup of TestFreemiumEmailCapture.test_capture_email_duplicate ____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py, line 111
      @pytest.mark.asyncio
      async def test_capture_email_duplicate(self, async_client: AsyncClient,
          db_session):
          """Test duplicate email capture (should return existing token)."""
          payload = {'email': 'duplicate@example.com', 'consent_marketing':
              True, 'consent_terms': True}
          response1 = await async_client.post('/api/v1/freemium/capture-email',
              json=payload)
          assert response1.status_code == HTTP_OK
          token1 = response1.json()['token']
          response2 = await async_client.post('/api/v1/freemium/capture-email',
              json=payload)
          assert response2.status_code == HTTP_OK
          token2 = response2.json()['token']
          assert token1 == token2
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, completed_freemium_session, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, freemium_session, freemium_token, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:111
_ ERROR at setup of TestFreemiumAssessmentStart.test_start_assessment_success __
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAssessmentStart.test_start_assessment_ai_service_error _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAssessmentStart.test_start_assessment_resume_existing _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
__ ERROR at setup of TestFreemiumAnswerQuestion.test_answer_question_success ___
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAnswerQuestion.test_answer_question_invalid_question_id _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAnswerQuestion.test_answer_question_assessment_complete _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAnswerQuestion.test_answer_question_validation_error _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumAnswerQuestion.test_answer_question_ai_error_fallback _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
________ ERROR at setup of TestFreemiumResults.test_get_results_success ________
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumResults.test_get_results_incomplete_assessment _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
________ ERROR at setup of TestFreemiumResults.test_get_results_cached _________
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
______ ERROR at setup of TestFreemiumResults.test_get_results_performance ______
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumConversionTracking.test_track_conversion_success _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumConversionTracking.test_track_conversion_duplicate_event _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumConversionTracking.test_track_conversion_invalid_event_type _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
___ ERROR at setup of TestFreemiumSecurityAndValidation.test_xss_prevention ____
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
_ ERROR at setup of TestFreemiumSecurityAndValidation.test_oversized_payload_rejection _
E   TypeError: create_freemium_token() got an unexpected keyword argument 'utm_source'
______ ERROR at setup of TestIQAgentLoadTesting.test_sustained_query_load ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py, line 581
      async def test_sustained_query_load(
          self, async_client, auth_headers, mock_iq_agent
      ):
          """Test sustained load on compliance queries"""
          import asyncio
          import time

          with patch("api.routers.iq_agent.get_iq_agent", return_value=mock_iq_agent):

              start_time = time.time()
              tasks = []

              # Create sustained load over 10 seconds
              for i in range(50):  # 50 requests
                  task = async_client.post(
                      "/api/v1/iq/query",
                      json={"query": f"Load test query {i}"},
                      headers=auth_headers,
                  )
                  tasks.append(task)

                  # Small delay between requests
                  if i % 10 == 0:
                      await asyncio.sleep(0.1)

              responses = await asyncio.gather(*tasks, return_exceptions=True)
              end_time = time.time()

              # Analyze results
              successful_responses = [
                  r
                  for r in responses
                  if hasattr(r, "status_code") and r.status_code == 200
              ]
              rate_limited_responses = [
                  r
                  for r in responses
                  if hasattr(r, "status_code") and r.status_code == 429
              ]

              total_time = end_time - start_time
              throughput = len(successful_responses) / total_time

              # Should handle reasonable load
              assert len(successful_responses) > 0
              assert throughput > 1.0  # At least 1 query per second

              print("Load test results:")
              print(f"Total requests: {len(responses)}")
              print(f"Successful: {len(successful_responses)}")
              print(f"Rate limited: {len(rate_limited_responses)}")
              print(f"Throughput: {throughput:.2f} queries/second")
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:581
_ ERROR at setup of TestQualityAnalysisAPI.test_get_evidence_quality_analysis __
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 34
      def test_get_evidence_quality_analysis(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestQualityAnalysisAPI.test_detect_evidence_duplicates ___
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 70
      def test_detect_evidence_duplicates(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestQualityAnalysisAPI.test_batch_duplicate_detection ____
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 111
      def test_batch_duplicate_detection(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestQualityAnalysisAPI.test_get_quality_benchmark ______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 151
      def test_get_quality_benchmark(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_______ ERROR at setup of TestQualityAnalysisAPI.test_get_quality_trends _______
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 173
      def test_get_quality_trends(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisAPI.test_quality_analysis_nonexistent_evidence _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 195
      def test_quality_analysis_nonexistent_evidence(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisAPI.test_duplicate_detection_insufficient_evidence _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 204
      def test_duplicate_detection_insufficient_evidence(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_evidence_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisValidation.test_duplicate_detection_invalid_threshold _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 220
      def test_duplicate_detection_invalid_threshold(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisValidation.test_batch_duplicate_detection_too_many_items _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 229
      def test_batch_duplicate_detection_too_many_items(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisValidation.test_quality_trends_invalid_days _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 239
      def test_quality_trends_invalid_days(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestQualityAnalysisValidation.test_quality_analysis_ai_service_error _
file /home/omar/Documents/ruleIQ/tests/integration/api/test_quality_analysis.py, line 245
      def test_quality_analysis_ai_service_error(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAIErrorHandling.test_ai_service_timeout_fallback ____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 29
      def test_ai_service_timeout_fallback(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAIErrorHandling.test_ai_quota_exceeded_fallback _____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 56
      def test_ai_quota_exceeded_fallback(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAIErrorHandling.test_ai_content_filter_handling _____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 79
      def test_ai_content_filter_handling(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestAIErrorHandling.test_ai_model_error_fallback ______
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 104
      def test_ai_model_error_fallback(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAIErrorHandling.test_ai_parsing_error_handling _____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 129
      def test_ai_parsing_error_handling(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestAIErrorHandling.test_network_error_fallback _______
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 156
      def test_network_error_fallback(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAIErrorHandling.test_multiple_ai_service_failures ____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 179
      def test_multiple_ai_service_failures(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestAIErrorHandling.test_partial_ai_service_degradation ___
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 250
      def test_partial_ai_service_degradation(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIErrorHandling.test_ai_service_recovery_after_failure _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 302
      def test_ai_service_recovery_after_failure(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestAIErrorHandling.test_ai_error_logging_and_monitoring __
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 340
      def test_ai_error_logging_and_monitoring(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAIErrorHandling.test_ai_fallback_to_mock_data ______
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 367
      def test_ai_fallback_to_mock_data(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAIErrorHandling.test_ai_error_context_preservation ___
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 403
      def test_ai_error_context_preservation(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAIErrorHandling.test_ai_circuit_breaker_pattern _____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 436
      def test_ai_circuit_breaker_pattern(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAIErrorHandling.test_ai_service_graceful_shutdown ____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 467
      @pytest.mark.asyncio
      async def test_ai_service_graceful_shutdown(self, db_session):
          """Test that AI services handle graceful shutdown properly"""
          assistant = ComplianceAssistant(db_session)

          # Simulate shutdown during AI request
          with patch.object(assistant, "_generate_response") as mock_generate:

              async def interrupted_response(*args, **kwargs):
                  await asyncio.sleep(0.1)
                  raise asyncio.CancelledError("Service shutting down")

              mock_generate.side_effect = interrupted_response

              with pytest.raises(asyncio.CancelledError):
                  await assistant.get_question_help(
                      question_id="shutdown-test",
                      question_text="Test question",
                      framework_id="gdpr",
                  )
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py:467
_____ ERROR at setup of TestAIErrorHandling.test_ai_error_rate_monitoring ______
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_error_handling.py, line 488
      def test_ai_error_rate_monitoring(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIOptimizationEndpoints.test_streaming_analysis_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 39
      @pytest.mark.asyncio
      async def test_streaming_analysis_endpoint(self, async_test_client,
          authenticated_headers, mock_compliance_assistant):
          """Test streaming analysis endpoint with optimization features."""
          request_data = {'assessment_responses': [{'question_id': 'q1',
              'answer': 'yes'}, {'question_id': 'q2', 'answer': 'no'}],
              'framework_id': 'gdpr', 'business_profile_id': 'profile-123'}

          async def mock_stream():
              yield 'Analysis chunk 1'
              yield 'Analysis chunk 2'
              yield 'Analysis complete'
          mock_compliance_assistant.analyze_assessment_results_stream = (
              AsyncMock(return_value=mock_stream()))
          with patch('api.routers.ai_assessments.ComplianceAssistant'
              ) as MockAssistant:
              MockAssistant.return_value = mock_compliance_assistant
              response = await async_test_client.post(
                  '/api/ai/assessments/analysis/stream', json=request_data,
                  headers=authenticated_headers)
              assert response.status_code == HTTP_OK
              assert (response.headers['content-type'] ==
                  'text/event-stream; charset=utf-8',)
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:39
_ ERROR at setup of TestAIOptimizationEndpoints.test_streaming_recommendations_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 63
      @pytest.mark.asyncio
      async def test_streaming_recommendations_endpoint(self,
          async_test_client, authenticated_headers, mock_ai_assistant):
          """Test streaming recommendations endpoint."""
          request_data = {'assessment_gaps': [{'section': 'data_protection',
              'severity': 'high'}, {'section': 'consent', 'severity':
              'medium'}], 'framework_id': 'gdpr', 'business_profile_id':
              'profile-123', 'priority_level': 'high'}

          async def mock_stream():
              yield 'Recommendation 1: '
              yield 'Implement data protection measures'
              yield 'Recommendation 2: '
              yield 'Update consent mechanisms'
          mock_ai_assistant.get_assessment_recommendations_stream = AsyncMock(
              return_value=mock_stream())
          with patch('api.routers.ai_assessments.ComplianceAssistant',
              return_value=mock_ai_assistant):
              response = await async_test_client.post(
                  '/api/ai/assessments/recommendations/stream', json=
                  request_data, headers=authenticated_headers)
              assert response.status_code == HTTP_OK
              assert (response.headers['content-type'] ==
                  'text/event-stream; charset=utf-8',)
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:63
__ ERROR at setup of TestAIOptimizationEndpoints.test_streaming_help_endpoint __
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 88
      @pytest.mark.asyncio
      async def test_streaming_help_endpoint(self, async_test_client,
          authenticated_headers, mock_ai_assistant):
          """Test streaming help endpoint."""
          framework_id = 'gdpr'
          request_data = {'question_id': 'q1', 'question_text':
              'What is personal data?', 'framework_id': framework_id,
              'section_id': 'data_protection'}

          async def mock_stream():
              yield 'Personal data under GDPR '
              yield 'includes any information '
              yield 'relating to an identified person.'
          mock_ai_assistant.get_assessment_help_stream = AsyncMock(return_value
              =mock_stream())
          with patch('api.routers.ai_assessments.ComplianceAssistant',
              return_value=mock_ai_assistant):
              response = await async_test_client.post(
                  f'/api/ai/assessments/{framework_id}/help/stream', json=
                  request_data, headers=authenticated_headers)
              assert response.status_code == HTTP_OK
              assert (response.headers['content-type'] ==
                  'text/event-stream; charset=utf-8',)
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:88
_ ERROR at setup of TestAIOptimizationEndpoints.test_circuit_breaker_status_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 112
      def test_circuit_breaker_status_endpoint(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIOptimizationEndpoints.test_model_selection_endpoint __
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 130
      def test_model_selection_endpoint(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIOptimizationEndpoints.test_streaming_with_circuit_breaker_open _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 146
      @pytest.mark.asyncio
      async def test_streaming_with_circuit_breaker_open(self,
          async_test_client, authenticated_headers, mock_ai_assistant):
          """Test streaming behavior when circuit breaker is open."""
          request_data = {'assessment_responses': [{'question_id': 'q1',
              'answer': 'yes'}], 'framework_id': 'gdpr',
              'business_profile_id': 'profile-123'}
          (mock_ai_assistant.circuit_breaker.is_model_available.return_value
              ) = False
          mock_ai_assistant.circuit_breaker.get_status.return_value = {
              'overall_state': 'OPEN', 'model_states': {'gemini-2.5-flash':
              'OPEN'}, 'metrics': {'success_rate': 0.45}}

          async def mock_fallback_stream():
              yield 'Service temporarily unavailable. Please try again later.'
          mock_ai_assistant.analyze_assessment_results_stream = AsyncMock(
              return_value=mock_fallback_stream())
          with patch('api.routers.ai_assessments.ComplianceAssistant',
              return_value=mock_ai_assistant):
              response = await async_test_client.post(
                  '/api/ai/assessments/analysis/stream', json=request_data,
                  headers=authenticated_headers)
              assert response.status_code == HTTP_OK
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:146
___ ERROR at setup of TestAIOptimizationEndpoints.test_model_fallback_chain ____
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 170
      def test_model_fallback_chain(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIOptimizationEndpoints.test_streaming_error_handling __
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 185
      @pytest.mark.asyncio
      async def test_streaming_error_handling(self, async_test_client,
          authenticated_headers, mock_ai_assistant):
          """Test error handling in streaming endpoints."""
          request_data = {'assessment_responses': [{'question_id': 'q1',
              'answer': 'yes'}], 'framework_id': 'gdpr',
              'business_profile_id': 'profile-123'}
          mock_ai_assistant.analyze_assessment_results_stream = AsyncMock(
              side_effect=Exception('AI service error'))
          with patch('api.routers.ai_assessments.ComplianceAssistant',
              return_value=mock_ai_assistant):
              response = await async_test_client.post(
                  '/api/ai/assessments/analysis/stream', json=request_data,
                  headers=authenticated_headers)
              assert response.status_code in [200, 500, 503]
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:185
_ ERROR at setup of TestAIOptimizationEndpoints.test_performance_metrics_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 201
      def test_performance_metrics_endpoint(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIOptimizationEndpoints.test_concurrent_streaming_requests _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 218
      @pytest.mark.asyncio
      async def test_concurrent_streaming_requests(self, async_test_client,
          authenticated_headers, mock_ai_assistant):
          """Test handling multiple concurrent streaming requests."""
          request_data = {'assessment_responses': [{'question_id': 'q1',
              'answer': 'yes'}], 'framework_id': 'gdpr',
              'business_profile_id': 'profile-123'}

          async def mock_stream():
              yield 'Concurrent response'
          mock_ai_assistant.analyze_assessment_results_stream = AsyncMock(
              return_value=mock_stream())
          with patch('api.routers.ai_assessments.ComplianceAssistant',
              return_value=mock_ai_assistant):
              tasks = []
              for _ in range(5):
                  task = async_test_client.post(
                      '/api/ai/assessments/analysis/stream', json=
                      request_data, headers=authenticated_headers)
                  tasks.append(task)
              responses = await asyncio.gather(*tasks)
              for response in responses:
                  assert response.status_code == HTTP_OK
E       fixture 'async_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py:218
_ ERROR at setup of TestAIOptimizationEndpoints.test_model_health_check_endpoint _
file /home/omar/Documents/ruleIQ/tests/integration/test_ai_optimization_endpoints.py, line 242
      def test_model_health_check_endpoint(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_assistant, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestCachedContentAPI.test_cache_metrics_endpoint_success __
file /home/omar/Documents/ruleIQ/tests/integration/test_cached_content_api.py, line 19
      def test_cache_metrics_endpoint_success(self, client, authenticated_headers
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestCachedContentAPI.test_cache_metrics_endpoint_authentication_required _
file /home/omar/Documents/ruleIQ/tests/integration/test_cached_content_api.py, line 36
      def test_cache_metrics_endpoint_authentication_required(self,
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_cached_content_api.py:36
_ ERROR at setup of TestCachedContentAPI.test_cache_metrics_includes_performance_data _
file /home/omar/Documents/ruleIQ/tests/integration/test_cached_content_api.py, line 43
      def test_cache_metrics_includes_performance_data(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestCachedContentAPI.test_cache_metrics_with_ai_activity __
file /home/omar/Documents/ruleIQ/tests/integration/test_cached_content_api.py, line 63
      @pytest.mark.slow
      def test_cache_metrics_with_ai_activity(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_compliance_assessment_pipeline_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 117
      @pytest.mark.asyncio
      async def test_compliance_assessment_pipeline_integration(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test complete compliance assessment workflow

          Flow: Assessment Creation  AI Analysis  Database Storage  Report Generation
          """
          headers = test_user_with_token["headers"]
          user_id = test_user_with_token["user"].id

          # Step 1: Create assessment
          assessment_data = {
              "name": "GDPR Compliance Assessment",
              "framework": "GDPR",
              "company_id": str(uuid4()),
              "assessment_type": "full_assessment",
              "questions": [
                  {
                      "question_id": "gdpr_consent",
                      "question_text": "Do you have a lawful basis for processing personal data?",
                      "answer": "yes",
                      "evidence_ids": []
                  },
                  {
                      "question_id": "gdpr_retention",
                      "question_text": "Do you have data retention policies?",
                      "answer": "partial",
                      "evidence_ids": [],
                  },
              ],
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assert response.status_code == 201
          assessment = response.json()
          assessment_id = assessment["id"]

          # Step 2: Trigger AI analysis
          with patch(
              "services.ai.assistant.ComplianceAssistant.analyze_assessment"
          ) as mock_ai:
              mock_ai.return_value = {
                  "compliance_score": 0.75,
                  "risk_level": "MEDIUM",
                  "gaps": ["Consent Management", "Data Retention"],
                  "recommendations": [
                      {
                          "priority": "HIGH",
                          "action": "Implement explicit consent system",
                          "timeline": "30 days",
                      },
                  ],
              }

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=headers,
              )
              assert response.status_code == 200
              analysis = response.json()

              # Verify AI service was called
              mock_ai.assert_called_once()
              assert analysis["compliance_score"] == 0.75
              assert analysis["risk_level"] == "MEDIUM"

          # Step 3: Verify database state
          response = await async_client.get(
              f"/assessments/{assessment_id}", headers=headers,
          )
          assert response.status_code == 200
          db_assessment = response.json()
          assert db_assessment["status"] == "analyzed"
          assert db_assessment["compliance_score"] == 0.75

          # Step 4: Generate compliance report
          response = await async_client.post(
              f"/assessments/{assessment_id}/generate-report", headers=headers,
          )
          assert response.status_code == 200
          report = response.json()

          # Step 5: Validate end-to-end data integrity
          assert report["assessment_id"] == assessment_id
          assert report["compliance_score"] == analysis["compliance_score"]
          assert len(report["recommendations"]) > 0

          # Step 6: Verify report can be retrieved
          response = await async_client.get(
              f"/assessments/{assessment_id}/report", headers=headers,
          )
          assert response.status_code == 200
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_evidence_collection_workflow_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 212
      @pytest.mark.asyncio
      async def test_evidence_collection_workflow_integration(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test complete evidence collection and processing workflow

          Flow: Evidence Upload  Classification  Processing  Compliance Mapping
          """
          headers = test_user_with_token["headers"]

          # Step 1: Upload evidence document
          files = {"file": ("privacy_policy.pdf", b"Mock PDF content", "application/pdf")}
          metadata = {
              "title": "Privacy Policy Document",
              "description": "Company privacy policy for GDPR compliance",
              "evidence_type": "policy_document",
              "framework": "GDPR"
          }

          response = await async_client.post(
              "/evidence/upload", files=files, data=metadata, headers=headers,
          )
          assert response.status_code == 201
          evidence = response.json()
          evidence_id = evidence["id"]

          # Step 2: Trigger evidence processing
          with patch("services.evidence_processor.classify_evidence") as mock_classifier:
              mock_classifier.return_value = {
                  "document_type": "privacy_policy",
                  "compliance_areas": ["data_protection", "consent_management"],
                  "confidence_score": 0.92,
                  "extracted_text": "This privacy policy outlines...",
              }

              response = await async_client.post(
                  f"/evidence/{evidence_id}/process", headers=headers,
              )
              assert response.status_code == 200
              processing_result = response.json()

              assert processing_result["document_type"] == "privacy_policy"
              assert processing_result["confidence_score"] > 0.9

          # Step 3: Map evidence to compliance requirements
          mapping_data = {
              "framework": "GDPR",
              "requirements": ["article_13", "article_14", "article_21"],
          }

          response = await async_client.post(
              f"/evidence/{evidence_id}/map-compliance",
              json=mapping_data,
              headers=headers,
          )
          assert response.status_code == 200
          mapping_result = response.json()

          # Step 4: Verify evidence is linked to assessment
          assessment_data = {
              "name": "GDPR Privacy Policy Review",
              "framework": "GDPR",
              "evidence_ids": [evidence_id],
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assert response.status_code == 201
          assessment = response.json()

          # Verify evidence is properly linked
          assert evidence_id in assessment["evidence_ids"]
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_ai_service_circuit_breaker_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 286
      @pytest.mark.asyncio
      async def test_ai_service_circuit_breaker_integration(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test AI service integration with circuit breaker pattern

          Tests circuit breaker states: CLOSED  OPEN  HALF_OPEN  CLOSED
          """
          headers = test_user_with_token["headers"]

          # Create test assessment for AI analysis
          assessment_data = {
              "name": "Circuit Breaker Test",
              "framework": "GDPR",
              "questions": [{"question_id": "test", "answer": "yes"}],
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assessment_id = response.json()["id"]

          # Test 1: Circuit breaker in CLOSED state (normal operation)
          with patch("services.ai.circuit_breaker.CircuitBreaker") as mock_cb:
              mock_cb.return_value.state = CircuitState.CLOSED
              mock_cb.return_value.call = AsyncMock(return_value={"score": 0.8})

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=headers,
              )
              assert response.status_code == 200
              assert response.json()["source"] != "fallback"  # Not using fallback

          # Test 2: Circuit breaker in OPEN state (failures trigger fallback)
          with patch(
              "services.ai.assistant.ComplianceAssistant.analyze_assessment"
          ) as mock_ai:
              # Simulate AI service failures
              mock_ai.side_effect = Exception("AI service unavailable")

              # Multiple requests should trigger circuit breaker
              for _ in range(3):
                  response = await async_client.post(
                      f"/assessments/{assessment_id}/analyze", headers=headers,
                  )
                  # Should still return 200 due to fallback mechanism
                  assert response.status_code == 200
                  result = response.json()

                  # Verify fallback response is used
                  assert (
                      "fallback" in result.get("source", "").lower()
                      or result.get("compliance_score") == 0.5,
                  )

          # Test 3: Verify circuit breaker recovery (HALF_OPEN  CLOSED)
          with patch(
              "services.ai.assistant.ComplianceAssistant.analyze_assessment"
          ) as mock_ai:
              mock_ai.return_value = {"compliance_score": 0.85, "risk_level": "LOW"}

              # After recovery timeout, should attempt to close circuit
              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=headers,
              )
              assert response.status_code == 200

              # Successful responses should eventually close the circuit
              for _ in range(5):  # Multiple successful calls
                  response = await async_client.post(
                      f"/assessments/{assessment_id}/analyze", headers=headers,
                  )
                  assert response.status_code == 200
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_cross_service_data_consistency _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 360
      @pytest.mark.asyncio
      async def test_cross_service_data_consistency(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test data consistency across multiple services

          Validates that data remains consistent between:
          - API responses and database state
          - Cache and database
          - Different service endpoints
          """
          headers = test_user_with_token["headers"]

          # Step 1: Create assessment and verify immediate consistency
          assessment_data = {
              "name": "Consistency Test Assessment",
              "framework": "ISO27001",
              "status": "draft",
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assert response.status_code == 201
          assessment = response.json()
          assessment_id = assessment["id"]

          # Step 2: Verify GET endpoint returns same data
          response = await async_client.get(
              f"/assessments/{assessment_id}", headers=headers,
          )
          assert response.status_code == 200
          retrieved_assessment = response.json()

          assert retrieved_assessment["name"] == assessment["name"]
          assert retrieved_assessment["framework"] == assessment["framework"]
          assert retrieved_assessment["status"] == assessment["status"]

          # Step 3: Update assessment and verify consistency
          update_data = {"status": "in_progress", "completion_percentage": 25}
          response = await async_client.patch(
              f"/assessments/{assessment_id}", json=update_data, headers=headers,
          )
          assert response.status_code == 200
          updated_assessment = response.json()

          # Step 4: Verify update is reflected in all endpoints
          response = await async_client.get(
              f"/assessments/{assessment_id}", headers=headers,
          )
          consistency_check = response.json()

          assert consistency_check["status"] == "in_progress"
          assert consistency_check["completion_percentage"] == 25

          # Step 5: Verify list endpoint reflects changes
          response = await async_client.get("/assessments", headers=headers)
          assessments_list = response.json()

          target_assessment = next(
              a for a in assessments_list if a["id"] == assessment_id
          )
          assert target_assessment["status"] == "in_progress"
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_concurrent_api_operations_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 424
      @pytest.mark.asyncio
      async def test_concurrent_api_operations_integration(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test API behavior under concurrent operations

          Validates:
          - Race condition handling
          - Database transaction isolation
          - Resource locking behavior
          """
          headers = test_user_with_token["headers"]

          # Create test assessment
          assessment_data = {"name": "Concurrent Test Assessment", "framework": "GDPR"}

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assessment_id = response.json()["id"]

          # Test concurrent updates to same resource
          async def update_assessment(update_data: Dict):
              return await async_client.patch(
                  f"/assessments/{assessment_id}", json=update_data, headers=headers,
              )

          # Concurrent updates with different fields
          tasks = [
              update_assessment({"status": "in_progress"}),
              update_assessment({"completion_percentage": 50}),
              update_assessment({"notes": "Updated concurrently"}),
          ]

          responses = await asyncio.gather(*tasks, return_exceptions=True)

          # All updates should succeed (different fields, no conflicts)
          successful_responses = [r for r in responses if not isinstance(r, Exception)]
          assert len(successful_responses) >= 2  # At least 2 should succeed

          # Verify final state is consistent
          response = await async_client.get(
              f"/assessments/{assessment_id}", headers=headers,
          )
          final_state = response.json()

          # Should have at least some of the updates applied
          assert final_state["status"] in [
              "draft",
              "in_progress",
          ]  # One of the attempted values
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_rate_limiting_integration_across_endpoints _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 476
      @pytest.mark.asyncio
      async def test_rate_limiting_integration_across_endpoints(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test rate limiting integration across multiple API endpoints

          Validates:
          - Per-user rate limiting
          - Per-endpoint rate limiting
          - Rate limiting with different endpoint types
          """
          headers = test_user_with_token["headers"]

          # Test general API rate limiting (100/min)
          responses = []
          for i in range(10):  # Test with moderate number of requests
              response = await async_client.get("/assessments", headers=headers)
              responses.append(response)

          # All should succeed under normal rate limits
          assert all(r.status_code == 200 for r in responses[:8])

          # Test AI endpoint rate limiting (20/min) - more restrictive
          assessment_data = {"name": f"Rate Test {i}", "framework": "GDPR"}
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assessment_id = response.json()["id"]

          ai_responses = []
          for i in range(5):  # Test AI endpoint rate limiting
              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=headers,
              )
              ai_responses.append(response)

              if i < 3:  # First few should succeed
                  assert response.status_code in [200, 429]  # Success or rate limited

          # Verify rate limiting headers are present
          if ai_responses:
              last_response = ai_responses[-1]
              assert (
                  "X-RateLimit-Remaining" in last_response.headers
                  or "X-RateLimit-Reset" in last_response.headers,
              )
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestComprehensiveAPIWorkflows.test_error_handling_integration_across_services _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 523
      @pytest.mark.asyncio
      async def test_error_handling_integration_across_services(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test error handling and propagation across service boundaries

          Validates:
          - Error response consistency
          - Error logging and correlation IDs
          - Graceful degradation
          """
          headers = test_user_with_token["headers"]

          # Test 1: Invalid resource access
          response = await async_client.get(f"/assessments/invalid-uuid", headers=headers)
          assert response.status_code == 404
          error_data = response.json()
          assert "detail" in error_data
          assert "correlation_id" in error_data  # Should have correlation ID

          # Test 2: Invalid data submission
          invalid_assessment = {
              "name": "",  # Invalid: empty name
              "framework": "INVALID_FRAMEWORK",  # Invalid framework,
          }

          response = await async_client.post(
              "/assessments", json=invalid_assessment, headers=headers,
          )
          assert response.status_code == 422  # Validation error
          error_data = response.json()
          assert "detail" in error_data

          # Test 3: Service dependency failure (mock external service down)
          with patch(
              "services.external_integration.ExternalService.call"
          ) as mock_external:
              mock_external.side_effect = Exception("External service unavailable")

              assessment_data = {"name": "Service Failure Test", "framework": "GDPR"}
              response = await async_client.post(
                  "/assessments", json=assessment_data, headers=headers,
              )

              # Should still succeed with graceful degradation
              assert response.status_code == 201
              assessment = response.json()

              # Should indicate external service was unavailable
              assert (
                  "warnings" in assessment
                  or assessment.get("external_data_available") == False,
              )
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 52
      @pytest.fixture
      async def test_user_with_token(self, async_db: AsyncSession):
          """Create test user with valid authentication token"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="integration@test.com", role="business_user",
          )

          # Generate JWT token for user
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})

          return {
              "user": user,
              "token": token,
              "headers": {"Authorization": f"Bearer {token}"},
          }
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_with_token, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:52
_ ERROR at setup of TestAPIWorkflowPerformance.test_assessment_workflow_performance _
file /home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py, line 582
      @pytest.mark.asyncio
      async def test_assessment_workflow_performance(
          self, async_client: httpx.AsyncClient, test_user_with_token: Dict
      ):
          """Test performance of complete assessment workflow"""
          headers = test_user_with_token["headers"]

          import time

          start_time = time.time()

          # Complete assessment workflow
          assessment_data = {
              "name": "Performance Test Assessment",
              "framework": "GDPR",
              "questions": [{"question_id": "test", "answer": "yes"}],
          }

          # Create assessment
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=headers,
          )
          assert response.status_code == 201
          assessment_id = response.json()["id"]

          # Analyze assessment (mock AI for consistent timing)
          with patch(
              "services.ai.assistant.ComplianceAssistant.analyze_assessment"
          ) as mock_ai:
              mock_ai.return_value = {"compliance_score": 0.8}

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=headers,
              )
              assert response.status_code == 200

          # Generate report
          response = await async_client.post(
              f"/assessments/{assessment_id}/generate-report", headers=headers,
          )
          assert response.status_code == 200

          end_time = time.time()
          workflow_time = end_time - start_time

          # Workflow should complete within 2 seconds (excluding AI processing)
          assert workflow_time < 2.0, f"Workflow took {workflow_time:.2f}s, expected <2s"
E       fixture 'test_user_with_token' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:582
_ ERROR at setup of TestAPIContractValidation.test_assessment_endpoint_contract_validation _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 80
      @pytest.mark.asyncio
      async def test_assessment_endpoint_contract_validation(self,
          async_client: httpx.AsyncClient, auth_headers: Dict):
          """Test assessment endpoint request/response contracts"""
          valid_assessment_data = {'name': 'Contract Test Assessment',
              'framework': 'GDPR', 'assessment_type': 'full_assessment',
              'questions': [{'question_id': 'gdpr_lawful_basis',
              'question_text': 'Do you have a lawful basis for processing?',
              'answer': 'yes', 'evidence_ids': []}]}
          response = await async_client.post('/assessments', json=
              valid_assessment_data, headers=auth_headers)
          assert response.status_code == HTTP_CREATED
          assessment_response = response.json()
          required_fields = ['id', 'name', 'framework', 'status',
              'created_at', 'updated_at']
          for field in required_fields:
              assert field in assessment_response, f'Required field {field} missing from response'
          assert isinstance(assessment_response['id'], str)
          assert isinstance(assessment_response['name'], str)
          assert isinstance(assessment_response['framework'], str)
          assert isinstance(assessment_response['status'], str)
          assert isinstance(assessment_response['created_at'], str)
          assessment_id = assessment_response['id']
          response = await async_client.get(f'/assessments/{assessment_id}',
              headers=auth_headers)
          assert response.status_code == HTTP_OK
          get_response = response.json()
          for field in required_fields:
              assert field in get_response
          assert get_response['id'] == assessment_id
          assert get_response['name'] == valid_assessment_data['name']
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 51
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(email='contract@test.com',
              username='contractuser')
          from api.dependencies.auth import create_access_token
          token = create_access_token(data={'sub': user.email})
          return {'Authorization': f'Bearer {token}'}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:51
_ ERROR at setup of TestAPIContractValidation.test_iq_agent_endpoint_contract_validation _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 139
      @pytest.mark.asyncio
      async def test_iq_agent_endpoint_contract_validation(self, async_client:
          httpx.AsyncClient, auth_headers: Dict):
          """Test IQ Agent endpoint contracts"""
          query_data = {'query':
              'What are the GDPR compliance requirements for my business?',
              'context': {'business_type': 'e-commerce', 'company_size':
              'small', 'frameworks': ['GDPR']}}
          with patch('services.iq_agent.IQComplianceAgent.process_query'
              ) as mock_query:
              mock_query.return_value = {'status': 'success', 'summary': {
                  'risk_posture': 'MEDIUM', 'compliance_score': 0.75,
                  'top_gaps': ['Consent Management'], 'immediate_actions': [
                  'Implement consent system']}, 'artifacts': {
                  'compliance_posture': {'overall_coverage': 0.75},
                  'action_plan': [], 'risk_assessment': {'overall_risk_level':
                  'MEDIUM'}}, 'evidence': {}, 'next_actions': []}
              response = await async_client.post('/iq-agent/query', json=
                  query_data, headers=auth_headers)
              assert response.status_code == HTTP_OK
              iq_response = response.json()
              required_fields = ['status', 'summary', 'artifacts', 'evidence',
                  'next_actions']
              for field in required_fields:
                  assert field in iq_response, f'Required field {field} missing from IQ Agent response'
              summary = iq_response['summary']
              summary_fields = ['risk_posture', 'compliance_score',
                  'top_gaps', 'immediate_actions']
              for field in summary_fields:
                  assert field in summary, f'Required summary field {field} missing'
              assert isinstance(summary['compliance_score'], (int, float))
              assert 0 <= summary['compliance_score'] <= 1
              assert isinstance(summary['top_gaps'], list)
              assert isinstance(summary['immediate_actions'], list)
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 51
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(email='contract@test.com',
              username='contractuser')
          from api.dependencies.auth import create_access_token
          token = create_access_token(data={'sub': user.email})
          return {'Authorization': f'Bearer {token}'}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:51
_ ERROR at setup of TestAPIContractValidation.test_evidence_upload_contract_validation _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 174
      @pytest.mark.asyncio
      async def test_evidence_upload_contract_validation(self, async_client:
          httpx.AsyncClient, auth_headers: Dict):
          """Test evidence upload endpoint contracts"""
          files = {'file': ('test_policy.pdf', b'Mock PDF content',
              'application/pdf')}
          metadata = {'title': 'Test Policy Document', 'description':
              'Contract validation test document', 'evidence_type':
              'policy_document', 'framework': 'GDPR'}
          response = await async_client.post('/evidence/upload', files=files,
              data=metadata, headers=auth_headers)
          assert response.status_code == HTTP_CREATED
          evidence_response = response.json()
          required_fields = ['id', 'title', 'description', 'file_path',
              'evidence_type', 'status', 'created_at']
          for field in required_fields:
              assert field in evidence_response, f'Required field {field} missing from evidence response'
          assert isinstance(evidence_response['id'], str)
          assert evidence_response['title'] == metadata['title']
          assert evidence_response['evidence_type'] == metadata['evidence_type']
          assert evidence_response['status'] in ['uploaded', 'processing',
              'processed']
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 51
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(email='contract@test.com',
              username='contractuser')
          from api.dependencies.auth import create_access_token
          token = create_access_token(data={'sub': user.email})
          return {'Authorization': f'Bearer {token}'}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:51
_ ERROR at setup of TestAPIContractValidation.test_field_mapper_contract_validation _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 214
      @pytest.mark.asyncio
      async def test_field_mapper_contract_validation(self, async_client:
          httpx.AsyncClient, auth_headers: Dict):
          """Test field mapper contracts for database column truncation

          Validates that field mappers handle truncated database columns correctly
          """
          long_field_data = {'name':
              'Very Long Assessment Name That Exceeds Database Column Limits For Testing Field Mappers'
              , 'framework': 'GDPR', 'description':
              'This is a very long description that tests the field mapper functionality for handling database column truncation issues that exist in the legacy schema where columns were limited to 16 characters'
              , 'notes':
              'Additional notes field that is also very long and should be handled by the field mapper system properly'
              }
          response = await async_client.post('/assessments', json=
              long_field_data, headers=auth_headers)
          assert response.status_code == HTTP_CREATED
          assessment_response = response.json()
          assert len(assessment_response['name']) > 16
          assert 'description' in assessment_response
          assessment_id = assessment_response['id']
          response = await async_client.get(f'/assessments/{assessment_id}',
              headers=auth_headers)
          retrieved_assessment = response.json()
          assert retrieved_assessment['name'] == long_field_data['name']
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 51
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(email='contract@test.com',
              username='contractuser')
          from api.dependencies.auth import create_access_token
          token = create_access_token(data={'sub': user.email})
          return {'Authorization': f'Bearer {token}'}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:51
_ ERROR at setup of TestAPIContractValidation.test_pagination_contract_validation _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 240
      @pytest.mark.asyncio
      async def test_pagination_contract_validation(self, async_client: httpx
          .AsyncClient, auth_headers: Dict):
          """Test pagination contracts across list endpoints"""
          for i in range(5):
              assessment_data = {'name': f'Pagination Test Assessment {i}',
                  'framework': 'GDPR'}
              response = await async_client.post('/assessments', json=
                  assessment_data, headers=auth_headers)
              assert response.status_code == HTTP_CREATED
          response = await async_client.get('/assessments?limit=3&offset=0',
              headers=auth_headers)
          assert response.status_code == HTTP_OK
          paginated_response = response.json()
          if isinstance(paginated_response, dict):
              pagination_fields = ['items', 'total', 'limit', 'offset']
              for field in pagination_fields:
                  assert field in paginated_response, f'Pagination field {field} missing'
              assert len(paginated_response['items']) <= MAX_RETRIES
              assert isinstance(paginated_response['total'], int)
              assert paginated_response['limit'] == MAX_RETRIES
              assert paginated_response['offset'] == 0
          else:
              assert len(paginated_response) <= MAX_RETRIES
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 51
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(email='contract@test.com',
              username='contractuser')
          from api.dependencies.auth import create_access_token
          token = create_access_token(data={'sub': user.email})
          return {'Authorization': f'Bearer {token}'}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:51
_ ERROR at setup of TestContractPerformance.test_schema_validation_performance _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 291
      @pytest.mark.asyncio
      async def test_schema_validation_performance(self, async_client: httpx.
          AsyncClient, auth_headers: Dict):
          """Test that schema validation doesn't significantly impact performance"""
          import time
          assessment_data = {'name': 'Performance Test Assessment',
              'framework': 'GDPR', 'assessment_type': 'full_assessment',
              'questions': [{'question_id': f'question_{i}', 'question_text':
              f'Test question {i}?', 'answer': 'yes', 'evidence_ids': []} for
              i in range(20)]}
          start_time = time.time()
          response = await async_client.post('/assessments', json=
              assessment_data, headers=auth_headers)
          end_time = time.time()
          validation_time = end_time - start_time
          assert validation_time < HALF_RATIO, f'Schema validation took {validation_time:.3f}s, expected <0.5s'
          assert response.status_code == HTTP_CREATED
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:291
_ ERROR at setup of TestSecurityContractValidation.test_input_sanitization_contract _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 328
      @pytest.mark.asyncio
      async def test_input_sanitization_contract(self, async_client: httpx.
          AsyncClient, auth_headers: Dict):
          """Test input sanitization contract requirements"""
          xss_payload = "<script>alert('xss')</script>"
          malicious_assessment = {'name': xss_payload, 'framework': 'GDPR',
              'description': xss_payload}
          response = await async_client.post('/assessments', json=
              malicious_assessment, headers=auth_headers)
          if response.status_code == HTTP_CREATED:
              assessment_response = response.json()
              assert '<script>' not in assessment_response.get('name', '')
              assert '<script>' not in assessment_response.get('description', '')
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:328
_ ERROR at setup of TestSecurityContractValidation.test_rate_limiting_contract_headers _
file /home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py, line 342
      @pytest.mark.asyncio
      async def test_rate_limiting_contract_headers(self, async_client: httpx
          .AsyncClient, auth_headers: Dict):
          """Test rate limiting contract and headers"""
          response = await async_client.get('/assessments', headers=auth_headers)
          rate_limit_headers = ['X-RateLimit-Limit', 'X-RateLimit-Remaining',
              'X-RateLimit-Reset']
          present_headers = [h for h in rate_limit_headers if h in response.
              headers]
          if present_headers:
              if 'X-RateLimit-Remaining' in response.headers:
                  remaining = response.headers['X-RateLimit-Remaining']
                  assert remaining.isdigit(
                      ), 'Rate limit remaining should be a number'
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:342
______ ERROR at setup of TestDatabaseConnections.test_postgres_connection ______
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 21
      def test_postgres_connection(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:21
_ ERROR at setup of TestDatabaseConnections.test_postgres_transaction_rollback _
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 33
      def test_postgres_transaction_rollback(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:33
_ ERROR at setup of TestDatabaseConnections.test_postgres_no_persistence_between_tests _
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 52
      def test_postgres_no_persistence_between_tests(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:52
____ ERROR at setup of TestDatabaseConnections.test_postgres_table_creation ____
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 59
      def test_postgres_table_creation(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:59
_______ ERROR at setup of TestDatabaseConnections.test_redis_connection ________
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 84
      def test_redis_connection(self, redis_client):
E       fixture 'redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:84
_______ ERROR at setup of TestDatabaseConnections.test_redis_expiration ________
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 102
      def test_redis_expiration(self, redis_client):
E       fixture 'redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:102
_______ ERROR at setup of TestDatabaseConnections.test_redis_data_types ________
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 117
      def test_redis_data_types(self, redis_client):
E       fixture 'redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:117
___ ERROR at setup of TestDatabaseConnections.test_async_postgres_connection ___
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 138
      @pytest.mark.asyncio
      async def test_async_postgres_connection(self, async_db_session):
          """Test async PostgreSQL connection."""
          # Execute async query
          result = await async_db_session.execute(text("SELECT 2 + 2 as sum"))
          row = result.fetchone()
          assert row[0] == 4
E       fixture 'async_db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:138
___ ERROR at setup of TestDatabaseConnections.test_connection_pool_settings ____
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 146
      def test_connection_pool_settings(self, test_db_engine):
E       fixture 'test_db_engine' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:146
____ ERROR at setup of TestDatabaseConnections.test_fixtures_work_together _____
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 154
      def test_fixtures_work_together(self, db_session, redis_client, sample_user):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:154
_ ERROR at setup of TestConnectionResilience.test_postgres_connection_recovery _
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 171
      def test_postgres_connection_recovery(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:171
__ ERROR at setup of TestConnectionResilience.test_redis_connection_recovery ___
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 184
      def test_redis_connection_recovery(self, redis_client):
E       fixture 'redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:184
__________ ERROR at setup of TestMockFixtures.test_mock_redis_client ___________
file /home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py, line 201
      def test_mock_redis_client(self, mock_redis_client):
E       fixture 'mock_redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_database_connections.py:201
_ ERROR at setup of TestEvidenceCollectionFlow.test_full_evidence_and_reporting_flow _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:23.164333+00:00", "level": "INFO", "logger": "database.db_setup", "message": "Loaded configuration from .env.local", "module": "db_setup", "function": "validate_environment", "line": 48, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": "Task-2000"}
{"timestamp": "2025-09-05T21:29:23.164570+00:00", "level": "INFO", "logger": "database.db_setup", "message": "Connecting to database: localhost:5432/test", "module": "db_setup", "function": "get_database_urls", "line": 69, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": "Task-2000"}
{"timestamp": "2025-09-05T21:29:23.165306+00:00", "level": "INFO", "logger": "database.db_setup", "message": "Asynchronous database engine initialized successfully", "module": "db_setup", "function": "_init_async_db", "line": 154, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": "Task-2000"}
------------------------------ Captured log setup ------------------------------
INFO     database.db_setup:db_setup.py:48 Loaded configuration from .env.local
INFO     database.db_setup:db_setup.py:69 Connecting to database: localhost:5432/test
INFO     database.db_setup:db_setup.py:154 Asynchronous database engine initialized successfully
_ ERROR at setup of TestEvidenceCollectionFlow.test_ai_assistant_evidence_query _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestEvidenceCollectionFlow.test_scheduled_report_generation _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestAPIEndpointsIntegration.test_business_profile_to_evidence_workflow _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestAPIEndpointsIntegration.test_framework_to_readiness_assessment _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestErrorHandlingAndResilience.test_integration_failure_handling _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestErrorHandlingAndResilience.test_report_generation_with_no_data _
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_____ ERROR at setup of TestAsyncOperations.test_async_evidence_collection _____
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
___ ERROR at setup of TestAsyncOperations.test_ai_assistant_async_processing ___
E   AttributeError: module 'database.db_setup' has no attribute '_async_engine'. Did you mean: 'AsyncEngine'?
_ ERROR at setup of TestAIServiceIntegration.test_gemini_integration_with_circuit_breaker _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 59
      @pytest.mark.asyncio
      async def test_gemini_integration_with_circuit_breaker(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test Google Gemini integration with circuit breaker functionality"""

          # Create test assessment for AI analysis
          assessment_data = {
              "name": "Gemini Integration Test",
              "framework": "GDPR",
              "questions": [
                  {
                      "question_id": "gdpr_data_processing",
                      "question_text": "Do you process personal data?",
                      "answer": "yes",
                      "evidence_ids": [],
                  },
              ],
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assert response.status_code == 201
          assessment_id = response.json()["id"]

          # Test 1: Successful Gemini API call
          with patch(
              "services.ai.assistant.ComplianceAssistant._call_gemini_api"
          ) as mock_gemini:
              mock_gemini.return_value = {
                  "compliance_score": 0.85,
                  "risk_level": "LOW",
                  "analysis": "Strong data protection practices",
                  "recommendations": ["Implement regular audits"],
              }

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=auth_headers,
              )

              assert response.status_code == 200
              analysis = response.json()

              # Verify Gemini was called
              mock_gemini.assert_called_once()
              assert analysis["compliance_score"] == 0.85
              assert analysis["risk_level"] == "LOW"
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 46
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="ai.integration@test.com",
              username="ai.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:46
_ ERROR at setup of TestAIServiceIntegration.test_ai_service_circuit_breaker_states _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 108
      @pytest.mark.asyncio
      async def test_ai_service_circuit_breaker_states(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test AI service circuit breaker state transitions"""

          assessment_data = {"name": "Circuit Breaker Test", "framework": "GDPR"}
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assessment_id = response.json()["id"]

          # Test circuit breaker CLOSED state (normal operation)
          with patch("services.ai.circuit_breaker.CircuitBreaker") as mock_cb_class:
              mock_cb = Mock()
              mock_cb.state = CircuitState.CLOSED
              mock_cb.call = AsyncMock(
                  return_value={"compliance_score": 0.8, "risk_level": "MEDIUM"},
              )
              mock_cb_class.return_value = mock_cb

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=auth_headers,
              )

              assert response.status_code == 200
              result = response.json()
              assert result["compliance_score"] == 0.8

          # Test circuit breaker OPEN state (failures trigger fallback)
          with patch(
              "services.ai.assistant.ComplianceAssistant._call_gemini_api"
          ) as mock_gemini:
              # Simulate repeated failures
              mock_gemini.side_effect = Exception("API timeout")

              # Multiple failed requests should eventually trigger fallback
              for attempt in range(3):
                  response = await async_client.post(
                      f"/assessments/{assessment_id}/analyze", headers=auth_headers,
                  )

                  # Should still return 200 due to fallback mechanism
                  assert response.status_code == 200
                  result = response.json()

                  # After enough failures, should use fallback response
                  if attempt >= 2:
                      assert "fallback" in result.get("source", "").lower() or result.get(
                          "compliance_score"
                      ) in [
                          0.5,
                          0.0,
                      ]  # Fallback scores
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 46
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="ai.integration@test.com",
              username="ai.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:46
_ ERROR at setup of TestAIServiceIntegration.test_ai_service_timeout_handling __
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 163
      @pytest.mark.asyncio
      async def test_ai_service_timeout_handling(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test AI service timeout handling"""

          assessment_data = {"name": "Timeout Test", "framework": "GDPR"}
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assessment_id = response.json()["id"]

          with patch(
              "services.ai.assistant.ComplianceAssistant._call_gemini_api"
          ) as mock_gemini:
              # Simulate timeout
              async def slow_response(*args, **kwargs):
                  await asyncio.sleep(2)  # Longer than circuit breaker timeout
                  return {"compliance_score": 0.7}

              mock_gemini.side_effect = slow_response

              start_time = time.time()
              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=auth_headers,
              )
              end_time = time.time()

              # Should timeout and use fallback within reasonable time
              assert response.status_code == 200
              assert (
                  end_time - start_time
              ) < 5  # Should not wait full 2 seconds due to timeout

              result = response.json()
              # Should either be fallback response or successful with timeout handling
              assert "compliance_score" in result
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 46
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="ai.integration@test.com",
              username="ai.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:46
_ ERROR at setup of TestAIServiceIntegration.test_multiple_ai_provider_fallback _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 201
      @pytest.mark.asyncio
      async def test_multiple_ai_provider_fallback(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test fallback chain: Gemini  OpenAI  Static fallback"""

          assessment_data = {"name": "Multi-Provider Test", "framework": "GDPR"}
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assessment_id = response.json()["id"]

          with patch(
              "services.ai.assistant.ComplianceAssistant._call_gemini_api"
          ) as mock_gemini, patch(
              "services.ai.assistant.ComplianceAssistant._call_openai_api"
          ) as mock_openai:

              # Gemini fails
              mock_gemini.side_effect = Exception("Gemini API unavailable")

              # OpenAI succeeds
              mock_openai.return_value = {
                  "compliance_score": 0.72,
                  "risk_level": "MEDIUM",
                  "analysis": "Fallback analysis from OpenAI",
              }

              response = await async_client.post(
                  f"/assessments/{assessment_id}/analyze", headers=auth_headers,
              )

              assert response.status_code == 200
              result = response.json()

              # Should use OpenAI fallback
              mock_gemini.assert_called_once()
              mock_openai.assert_called_once()
              assert result["compliance_score"] == 0.72
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 46
      @pytest.fixture
      async def auth_headers(self, async_db):
          """Create authentication headers"""
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="ai.integration@test.com",
              username="ai.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:46
_ ERROR at setup of TestDatabaseIntegration.test_database_connection_pool_behavior _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 264
      @pytest.mark.asyncio
      async def test_database_connection_pool_behavior(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test database connection pool under concurrent load"""

          async def create_assessment(i: int):
              assessment_data = {
                  "name": f"Concurrent Assessment {i}",
                  "framework": "GDPR",
              }
              return await async_client.post(
                  "/assessments", json=assessment_data, headers=auth_headers,
              )

          # Create multiple assessments concurrently
          tasks = [create_assessment(i) for i in range(10)]
          responses = await asyncio.gather(*tasks, return_exceptions=True)

          # Count successful responses
          successful_responses = [
              r
              for r in responses
              if not isinstance(r, Exception) and r.status_code == 201
          ]

          # Most requests should succeed (allowing for some connection pool limits)
          assert (
              len(successful_responses) >= 8
          ), f"Only {len(successful_responses)}/10 requests succeeded"
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 252
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="db.integration@test.com",
              username="db.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:252
_ ERROR at setup of TestDatabaseIntegration.test_database_transaction_consistency _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 295
      @pytest.mark.asyncio
      async def test_database_transaction_consistency(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test database transaction consistency across service operations"""

          # Create assessment with evidence (should be atomic)
          assessment_data = {
              "name": "Transaction Test Assessment",
              "framework": "GDPR",
              "evidence_ids": [],  # Will add evidence in separate call,
          }

          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assert response.status_code == 201
          assessment_id = response.json()["id"]

          # Upload evidence
          files = {"file": ("test.pdf", b"Test content", "application/pdf")}
          metadata = {"title": "Transaction Test Evidence", "framework": "GDPR"}

          response = await async_client.post(
              "/evidence/upload", files=files, data=metadata, headers=auth_headers,
          )
          assert response.status_code == 201
          evidence_id = response.json()["id"]

          # Link evidence to assessment (should maintain referential integrity)
          update_data = {"evidence_ids": [evidence_id]}
          response = await async_client.patch(
              f"/assessments/{assessment_id}", json=update_data, headers=auth_headers,
          )
          assert response.status_code == 200

          # Verify consistency - both assessment and evidence should exist and be linked
          response = await async_client.get(
              f"/assessments/{assessment_id}", headers=auth_headers,
          )
          assessment = response.json()
          assert evidence_id in assessment["evidence_ids"]

          response = await async_client.get(
              f"/evidence/{evidence_id}", headers=auth_headers,
          )
          assert response.status_code == 200
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 252
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="db.integration@test.com",
              username="db.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:252
__ ERROR at setup of TestDatabaseIntegration.test_database_failover_behavior ___
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 343
      @pytest.mark.asyncio
      async def test_database_failover_behavior(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test database failover and recovery behavior"""

          # This test simulates database connectivity issues
          with patch("database.db_setup.get_async_db") as mock_db:
              # First call succeeds
              mock_db.return_value.__anext__ = AsyncMock()

              # Simulate temporary database connection failure
              from sqlalchemy.exc import DisconnectionError

              mock_db.side_effect = [DisconnectionError("Connection lost", None, None)]

              assessment_data = {"name": "Failover Test", "framework": "GDPR"}

              # Should handle database connection errors gracefully
              response = await async_client.post(
                  "/assessments", json=assessment_data, headers=auth_headers,
              )

              # Either succeeds with retry or returns appropriate error
              assert response.status_code in [201, 503, 500]
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 252
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="db.integration@test.com",
              username="db.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:252
______ ERROR at setup of TestRedisIntegration.test_redis_caching_behavior ______
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 429
      @pytest.mark.asyncio
      async def test_redis_caching_behavior(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test Redis caching for API responses"""

          # Make request that should be cached
          response1 = await async_client.get("/frameworks", headers=auth_headers)
          assert response1.status_code == 200
          frameworks1 = response1.json()

          start_time = time.time()
          # Second request should be faster due to caching
          response2 = await async_client.get("/frameworks", headers=auth_headers)
          end_time = time.time()

          assert response2.status_code == 200
          frameworks2 = response2.json()

          # Should return same data
          assert frameworks1 == frameworks2

          # Second request should be faster (cached)
          response_time = end_time - start_time
          assert response_time < 0.1  # Should be very fast if cached
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 379
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="redis.integration@test.com",
              username="redis.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:379
_____ ERROR at setup of TestRedisIntegration.test_redis_cache_invalidation _____
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 455
      @pytest.mark.asyncio
      async def test_redis_cache_invalidation(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test Redis cache invalidation on data updates"""

          # Create assessment (should invalidate assessment list cache)
          assessment_data = {"name": "Cache Test Assessment", "framework": "GDPR"}

          # Get initial list
          response = await async_client.get("/assessments", headers=auth_headers)
          initial_count = len(response.json())

          # Create new assessment
          response = await async_client.post(
              "/assessments", json=assessment_data, headers=auth_headers,
          )
          assert response.status_code == 201

          # Get updated list (cache should be invalidated)
          response = await async_client.get("/assessments", headers=auth_headers)
          updated_count = len(response.json())

          assert updated_count == initial_count + 1
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 379
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="redis.integration@test.com",
              username="redis.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:379
_ ERROR at setup of TestEmailServiceIntegration.test_assessment_completion_notification _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 550
      @pytest.mark.asyncio
      async def test_assessment_completion_notification(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test email notifications for assessment completion"""

          with patch(
              "services.email_service.send_assessment_completion_email"
          ) as mock_email:
              mock_email.return_value = True

              # Create and complete assessment
              assessment_data = {"name": "Email Notification Test", "framework": "GDPR"}
              response = await async_client.post(
                  "/assessments", json=assessment_data, headers=auth_headers,
              )
              assessment_id = response.json()["id"]

              # Mark as completed (trigger email)
              update_data = {"status": "completed"}
              response = await async_client.patch(
                  f"/assessments/{assessment_id}", json=update_data, headers=auth_headers,
              )

              assert response.status_code == 200

              # Email should be sent (if configured)
              # mock_email.assert_called_once()
E       fixture 'auth_headers' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:550
_ ERROR at setup of TestFileStorageIntegration.test_file_upload_storage_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 601
      @pytest.mark.asyncio
      async def test_file_upload_storage_integration(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test file upload and storage integration"""

          # Test file upload
          files = {
              "file": ("integration_test.pdf", b"Test PDF content", "application/pdf"),
          }
          metadata = {
              "title": "Storage Integration Test",
              "description": "Test file for storage integration",
              "evidence_type": "policy_document"
          }

          response = await async_client.post(
              "/evidence/upload", files=files, data=metadata, headers=auth_headers,
          )

          assert response.status_code == 201
          evidence = response.json()

          # Verify file was stored
          assert "file_path" in evidence
          assert evidence["title"] == metadata["title"]

          # Test file retrieval
          evidence_id = evidence["id"]
          response = await async_client.get(
              f"/evidence/{evidence_id}/download", headers=auth_headers,
          )

          # Should either return file or redirect to file location
          assert response.status_code in [200, 302]
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 589
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="storage.test@example.com",
              username="storage.test@example.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:589
_ ERROR at setup of TestFileStorageIntegration.test_file_processing_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 637
      @pytest.mark.asyncio
      async def test_file_processing_integration(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test file processing workflow integration"""

          # Upload file
          files = {
              "file": (
                  "process_test.pdf",
                  b"Document content for processing",
                  "application/pdf",
              )
          }
          metadata = {
              "title": "Processing Test Document",
              "evidence_type": "policy_document",
              "framework": "GDPR",
          }

          response = await async_client.post(
              "/evidence/upload", files=files, data=metadata, headers=auth_headers,
          )

          assert response.status_code == 201
          evidence_id = response.json()["id"]

          # Trigger processing
          with patch("services.document_processor.process_document") as mock_processor:
              mock_processor.return_value = {
                  "document_type": "privacy_policy",
                  "extracted_text": "Privacy policy content...",
                  "compliance_areas": ["data_protection", "consent"],
                  "confidence_score": 0.95,
              }

              response = await async_client.post(
                  f"/evidence/{evidence_id}/process", headers=auth_headers,
              )

              assert response.status_code == 200
              processing_result = response.json()

              # Verify processing was triggered
              mock_processor.assert_called_once()
              assert processing_result["document_type"] == "privacy_policy"
              assert processing_result["confidence_score"] == 0.95
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 589
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="storage.test@example.com",
              username="storage.test@example.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:589
_ ERROR at setup of TestThirdPartyAPIIntegration.test_companies_house_api_integration _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 708
      @pytest.mark.asyncio
      async def test_companies_house_api_integration(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test Companies House API integration for company verification"""

          with patch(
              "services.companies_house.CompaniesHouseAPI.lookup_company"
          ) as mock_ch:
              mock_ch.return_value = {
                  "company_number": "12345678",
                  "company_name": "Test Company Ltd",
                  "company_status": "active",
                  "incorporation_date": "2020-01-01",
              }

              # Test company lookup
              lookup_data = {"company_number": "12345678"}
              response = await async_client.post(
                  "/integrations/companies-house/lookup",
                  json=lookup_data,
                  headers=auth_headers,
              )

              if response.status_code == 404:
                  pytest.skip("Companies House integration endpoint not implemented")

              assert response.status_code == 200
              company_data = response.json()

              mock_ch.assert_called_once_with("12345678")
              assert company_data["company_name"] == "Test Company Ltd"
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 696
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="api.integration@test.com",
              username="api.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:696
_ ERROR at setup of TestThirdPartyAPIIntegration.test_external_api_timeout_handling _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 741
      @pytest.mark.asyncio
      async def test_external_api_timeout_handling(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test external API timeout and error handling"""

          with patch("httpx.AsyncClient.get") as mock_get:
              # Simulate timeout
              mock_get.side_effect = httpx.TimeoutException("Request timeout")

              # Try to use an external integration
              response = await async_client.get(
                  "/integrations/health-check", headers=auth_headers,
              )

              if response.status_code == 404:
                  pytest.skip("External integrations health check not implemented")

              # Should handle timeout gracefully
              assert response.status_code in [200, 503, 504]
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 696
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="api.integration@test.com",
              username="api.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:696
_ ERROR at setup of TestThirdPartyAPIIntegration.test_external_api_rate_limiting_respect _
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 762
      @pytest.mark.asyncio
      async def test_external_api_rate_limiting_respect(
          self, async_client: httpx.AsyncClient, auth_headers: Dict
      ):
          """Test that external API rate limits are respected"""

          with patch("services.external_integration.RateLimiter") as mock_limiter:
              mock_limiter.return_value.is_allowed.return_value = False

              # Multiple rapid requests should be rate limited
              responses = []
              for i in range(5):
                  response = await async_client.post(
                      "/integrations/test-api",
                      json={"test": f"request_{i}"},
                      headers=auth_headers,
                  )
                  responses.append(response)

              # Should respect rate limiting (either 429 responses or controlled timing)
              status_codes = [r.status_code for r in responses]

              # If endpoint exists, should show rate limiting behavior
              if any(code != 404 for code in status_codes):
                  rate_limited_responses = [code for code in status_codes if code == 429]
                  # Should have some rate limited responses or controlled timing
                  assert len(rate_limited_responses) > 0 or all(
                      code in [200, 201, 503] for code in status_codes if code != 404
                  )
file /home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py, line 696
      @pytest.fixture
      async def auth_headers(self, async_db):
          auth_manager = TestAuthManager()
          user = auth_manager.create_test_user(
              email="api.integration@test.com",
              username="api.integration@test.com".split("@")[0],
          )
          from api.dependencies.auth import create_access_token

          token = create_access_token(data={"sub": user.email})
          return {"Authorization": f"Bearer {token}"}
E       fixture 'async_db' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, auth_headers, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:696
___ ERROR at setup of TestJWTAuthenticationFlow.test_user_registration_flow ____
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 41
      def test_user_registration_flow(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:41
_______ ERROR at setup of TestJWTAuthenticationFlow.test_user_login_flow _______
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 56
      def test_user_login_flow(self, test_client: TestClient, existing_user,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:56
__ ERROR at setup of TestJWTAuthenticationFlow.test_protected_endpoint_access __
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 68
      def test_protected_endpoint_access(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:68
_____ ERROR at setup of TestJWTAuthenticationFlow.test_token_refresh_flow ______
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 83
      def test_token_refresh_flow(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:83
_________ ERROR at setup of TestJWTAuthenticationFlow.test_logout_flow _________
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 98
      def test_logout_flow(self, test_client: TestClient, existing_user,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:98
_ ERROR at setup of TestJWTAuthenticationFlow.test_invalid_credentials_rejection _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 112
      def test_invalid_credentials_rejection(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:112
_ ERROR at setup of TestJWTAuthenticationFlow.test_nonexistent_user_rejection __
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 120
      def test_nonexistent_user_rejection(self, test_client: TestClient):
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:120
_ ERROR at setup of TestJWTAuthenticationFlow.test_duplicate_registration_rejection _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 127
      def test_duplicate_registration_rejection(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:127
_ ERROR at setup of TestJWTAuthenticationFlow.test_protected_endpoint_without_token _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 136
      def test_protected_endpoint_without_token(self, test_client: TestClient):
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:136
_ ERROR at setup of TestJWTAuthenticationFlow.test_protected_endpoint_with_invalid_token _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 141
      def test_protected_endpoint_with_invalid_token(self, test_client:
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:141
_ ERROR at setup of TestBusinessProfileIntegration.test_business_profile_access_with_auth _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 151
      def test_business_profile_access_with_auth(self, test_client:
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:151
_ ERROR at setup of TestBusinessProfileIntegration.test_business_profile_access_without_auth _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 164
      def test_business_profile_access_without_auth(self, test_client: TestClient
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:164
___ ERROR at setup of TestRAGSystemIntegration.test_chat_endpoint_with_auth ____
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 173
      def test_chat_endpoint_with_auth(self, test_client: TestClient,
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:173
__ ERROR at setup of TestRAGSystemIntegration.test_chat_endpoint_without_auth __
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 186
      def test_chat_endpoint_without_auth(self, test_client: TestClient):
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:186
_ ERROR at setup of TestAuthenticationSystemIntegration.test_complete_user_journey _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 195
      def test_complete_user_journey(self, test_client: TestClient):
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:195
_ ERROR at setup of TestAuthenticationSystemIntegration.test_authentication_system_health _
file /home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py, line 221
      def test_authentication_system_health(self, test_client: TestClient):
E       fixture 'test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, existing_user, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, test_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/integration/test_jwt_auth_integration.py:221
___ ERROR at teardown of TestPrometheusExporter.test_exporter_initialization ___
E   AttributeError: 'PrometheusExporter' object has no attribute 'shutdown'
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.805058+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
__ ERROR at setup of TestPrometheusExporter.test_prometheus_format_conversion __
E   OSError: [Errno 98] Address already in use
____ ERROR at setup of TestPrometheusExporter.test_prometheus_http_endpoint ____
E   OSError: [Errno 98] Address already in use
_ ERROR at setup of TestPrometheusExporter.test_histogram_buckets_configuration _
E   OSError: [Errno 98] Address already in use
____ ERROR at setup of TestPrometheusExporter.test_metric_name_sanitization ____
E   OSError: [Errno 98] Address already in use
__ ERROR at setup of TestPrometheusExporter.test_multi_collector_aggregation ___
E   OSError: [Errno 98] Address already in use
_ ERROR at setup of TestLangGraphMetricsInstrumentor.test_node_execution_metrics _
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.866307+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
___ ERROR at setup of TestLangGraphMetricsInstrumentor.test_workflow_metrics ___
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.896877+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
_ ERROR at setup of TestLangGraphMetricsInstrumentor.test_error_tracking_metrics _
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.926714+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
_ ERROR at setup of TestLangGraphMetricsInstrumentor.test_state_transition_metrics _
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.957028+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
__ ERROR at setup of TestLangGraphMetricsInstrumentor.test_checkpoint_metrics __
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:31.987507+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
_ ERROR at setup of TestLangGraphMetricsInstrumentor.test_message_queue_metrics _
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:32.017093+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
_ ERROR at setup of TestLangGraphMetricsInstrumentor.test_memory_usage_metrics _
E   AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'create_up_down_counter'. Did you mean: 'create_updown_counter'?
---------------------------- Captured stdout setup -----------------------------
{"timestamp": "2025-09-05T21:29:32.047215+00:00", "level": "WARNING", "logger": "opentelemetry.metrics._internal", "message": "Overriding of current MeterProvider is not allowed", "module": "__init__", "function": "_set_meter_provider", "line": 864, "thread": 132058379736512, "thread_name": "MainThread", "process": 117981, "process_name": "MainProcess", "stack_info": null, "taskName": null}
------------------------------ Captured log setup ------------------------------
WARNING  opentelemetry.metrics._internal:__init__.py:864 Overriding of current MeterProvider is not allowed
_ ERROR at setup of TestAIPerformance.test_ai_help_response_time_under_threshold _
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 33
      def test_ai_help_response_time_under_threshold(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIPerformance.test_ai_help_concurrent_requests_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 57
      def test_ai_help_concurrent_requests_performance(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIPerformance.test_ai_analysis_performance_with_large_dataset _
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 91
      def test_ai_analysis_performance_with_large_dataset(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAIPerformance.test_ai_service_timeout_handling _____
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 108
      @pytest.mark.asyncio
      async def test_ai_service_timeout_handling(self, async_db_session):
          """Test AI service handles timeouts gracefully"""
          assistant = ComplianceAssistant(async_db_session)
          with patch.object(assistant, 'get_assessment_help') as mock_help:
              mock_help.side_effect = AITimeoutException(timeout_seconds=30.0)
              start_time = time.time()
              with pytest.raises(AITimeoutException):
                  await assistant.get_assessment_help(question_id=
                      'timeout-test', question_text='Test question',
                      framework_id='gdpr', business_profile_id=UUID(
                      '12345678-1234-5678-9012-123456789012'))
              end_time = time.time()
              assert end_time - start_time < 2.0
E       fixture 'async_db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py:108
___ ERROR at setup of TestAIPerformance.test_ai_caching_improves_performance ___
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 123
      def test_ai_caching_improves_performance(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAIRateLimiting.test_ai_help_rate_limit_enforcement ___
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 156
      def test_ai_help_rate_limit_enforcement(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at setup of TestAIRateLimiting.test_ai_analysis_stricter_rate_limit ___
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 192
      def test_ai_analysis_stricter_rate_limit(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAIRateLimiting.test_regular_endpoints_higher_rate_limits _
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 211
      def test_regular_endpoints_higher_rate_limits(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAIRateLimiting.test_rate_limit_headers_present _____
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 227
      def test_rate_limit_headers_present(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAIRateLimiting.test_rate_limit_reset_after_window ____
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 251
      def test_rate_limit_reset_after_window(self, client,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at setup of TestAILoadTesting.test_ai_endpoint_load_capacity ______
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 282
      def test_ai_endpoint_load_capacity(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAILoadTesting.test_ai_memory_usage_under_load ______
file /home/omar/Documents/ruleIQ/tests/performance/test_ai_performance.py, line 326
      def test_ai_memory_usage_under_load(self, client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAPIPerformance.test_evidence_creation_performance ____
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 53
      def test_evidence_creation_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at teardown of TestAPIPerformance.test_evidence_creation_performance __
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
____ ERROR at setup of TestAPIPerformance.test_evidence_search_performance _____
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 93
      def test_evidence_search_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at teardown of TestAPIPerformance.test_evidence_search_performance ___
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
_______ ERROR at setup of TestAPIPerformance.test_dashboard_performance ________
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 130
      def test_dashboard_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______ ERROR at teardown of TestAPIPerformance.test_dashboard_performance ______
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
___ ERROR at setup of TestAPIPerformance.test_concurrent_request_performance ___
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 173
      def test_concurrent_request_performance(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAPIPerformance.test_bulk_operation_performance _____
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 214
      def test_bulk_operation_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at teardown of TestAPIPerformance.test_bulk_operation_performance ____
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
_____ ERROR at setup of TestMemoryPerformance.test_large_dataset_handling ______
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 282
      def test_large_dataset_handling(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestMemoryPerformance.test_concurrent_memory_usage _____
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 341
      def test_concurrent_memory_usage(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestDatabasePerformance.test_complex_query_performance ___
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 413
      def test_complex_query_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at teardown of TestDatabasePerformance.test_complex_query_performance __
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
____ ERROR at setup of TestDatabasePerformance.test_aggregation_performance ____
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 478
      def test_aggregation_performance(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
__ ERROR at teardown of TestDatabasePerformance.test_aggregation_performance ___
E   pytest_benchmark.logger.PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
______ ERROR at setup of TestRealWorldScenarios.test_daily_user_workflow _______
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 644
      def test_daily_user_workflow(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestRealWorldScenarios.test_peak_usage_simulation ______
file /home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py, line 694
      def test_peak_usage_simulation(
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, performance_monitor, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py:694
__ ERROR at setup of TestDatabaseQueryPerformance.test_evidence_query_scaling __
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 24
      def test_evidence_query_scaling(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:24
_ ERROR at setup of TestDatabaseQueryPerformance.test_full_text_search_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 77
      def test_full_text_search_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:77
_ ERROR at setup of TestDatabaseQueryPerformance.test_aggregation_query_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 128
      def test_aggregation_query_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:128
__ ERROR at setup of TestDatabaseQueryPerformance.test_join_query_performance __
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 205
      def test_join_query_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:205
_ ERROR at setup of TestDatabaseConnectionPerformance.test_connection_pool_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 270
      def test_connection_pool_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:270
_ ERROR at setup of TestDatabaseConnectionPerformance.test_transaction_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 361
      def test_transaction_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:361
_ ERROR at setup of TestDatabaseConnectionPerformance.test_bulk_operation_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 420
      def test_bulk_operation_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:420
_ ERROR at setup of TestDatabaseIndexPerformance.test_indexed_query_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 489
      def test_indexed_query_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:489
_ ERROR at setup of TestDatabaseIndexPerformance.test_unindexed_query_performance _
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 568
      def test_unindexed_query_performance(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:568
__ ERROR at setup of TestDatabaseResourceUsage.test_memory_usage_optimization __
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 846
      def test_memory_usage_optimization(
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:846
_____ ERROR at setup of TestDatabaseResourceUsage.test_connection_cleanup ______
file /home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py, line 912
      def test_connection_cleanup(self, db_session: Session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/performance/test_database_performance.py:912
_ ERROR at setup of TestAuthenticationSecurity.test_unauthenticated_access_denied _
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 20
      def test_unauthenticated_access_denied(self, unauthenticated_test_client):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:20
___ ERROR at setup of TestAuthenticationSecurity.test_invalid_token_rejected ___
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 59
      def test_invalid_token_rejected(self, unauthenticated_test_client):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:59
___ ERROR at setup of TestAuthenticationSecurity.test_expired_token_handling ___
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 88
      def test_expired_token_handling(self, unauthenticated_test_client, expired_token):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:88
_ ERROR at setup of TestAuthenticationSecurity.test_token_without_bearer_prefix _
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 103
      def test_token_without_bearer_prefix(self, unauthenticated_test_client, auth_token):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:103
_ ERROR at setup of TestAuthenticationSecurity.test_malformed_authorization_header _
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 112
      def test_malformed_authorization_header(self, unauthenticated_test_client):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:112
_____ ERROR at setup of TestTokenSecurity.test_token_signature_validation ______
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 492
      def test_token_signature_validation(
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:492
______ ERROR at setup of TestTokenSecurity.test_token_algorithm_confusion ______
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 520
      def test_token_algorithm_confusion(
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:520
_____ ERROR at setup of TestAuthorizationSecurity.test_user_data_isolation _____
file /home/omar/Documents/ruleIQ/tests/security/test_authentication.py, line 547
      def test_user_data_isolation(self, client, sample_user_data, another_user):
E       fixture 'another_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:547
_ ERROR at setup of TestComplianceAssistantIntegration.test_full_workflow_assessment _
file /home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py, line 392
      @pytest.mark.asyncio
      async def test_full_workflow_assessment(self, db_session):
          """Test complete assessment workflow"""
          assistant = ComplianceAssistant(db_session)

          # Classify intent
          intent = await assistant.classify_intent(
              "I need help with my GDPR assessment"
          )
          assert intent['intent'] in ['assessment_help', 'compliance_guidance']

          # Generate response with tools
          tools = [{'name': 'assess_compliance'}]
          response = await assistant.generate_response(
              prompt="Help me assess GDPR Article 32",
              tools=tools
          )

          assert response is not None
          assert len(response) > 0
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:392
_ ERROR at setup of TestComplianceAssistantIntegration.test_caching_performance _
file /home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py, line 413
      @pytest.mark.asyncio
      async def test_caching_performance(self, db_session):
          """Test caching improves performance"""
          assistant = ComplianceAssistant(db_session)

          prompt = "What is GDPR?"

          # First call - no cache
          import time
          start = time.time()
          response1 = await assistant.generate_response(prompt)
          time1 = time.time() - start

          # Second call - should use cache
          start = time.time()
          response2 = await assistant.generate_response(prompt)
          time2 = time.time() - start

          assert response1 == response2
          assert time2 < time1 * 0.5  # Cache should be at least 2x faster
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:413
_ ERROR at setup of TestComplianceAssistantIntegration.test_concurrent_requests _
file /home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py, line 434
      @pytest.mark.asyncio
      async def test_concurrent_requests(self, db_session):
          """Test handling concurrent requests"""
          assistant = ComplianceAssistant(db_session)

          import asyncio
          prompts = [f"Question {i}" for i in range(5)]

          tasks = [
              assistant.generate_response(prompt)
              for prompt in prompts
          ]

          responses = await asyncio.gather(*tasks)

          assert len(responses) == 5
          assert all(r is not None for r in responses)
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:434
___ ERROR at setup of TestComplianceAssistantIntegration.test_rate_limiting ____
file /home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py, line 452
      @pytest.mark.asyncio
      async def test_rate_limiting(self, db_session):
          """Test rate limiting protection"""
          assistant = ComplianceAssistant(db_session)

          # Simulate rapid requests
          prompts = ["Test"] * 20

          with pytest.raises((IntegrationException, CircuitBreakerException)):
              for prompt in prompts:
                  await assistant.generate_response(prompt)
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:452
_ ERROR at setup of TestComplianceServiceIntegration.test_full_assessment_workflow _
file /home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py, line 385
      @pytest.mark.asyncio
      async def test_full_assessment_workflow(self, db_session):
          """Test complete assessment workflow"""
          service = ComplianceService(db_session)

          # Create assessment
          assessment = await service.create_assessment(
              framework_id=1,
              business_id=str(uuid4()),
              user_id=str(uuid4())
          )

          # Update responses
          for req_id in ['REQ-1', 'REQ-2', 'REQ-3']:
              response = AssessmentResponse(
                  compliant=req_id != 'REQ-2',  # REQ-2 non-compliant
                  evidence=[f'{req_id}_evidence.pdf'],
                  notes=f'Response for {req_id}'
              )
              await service.update_response(assessment.id, req_id, response)

          # Calculate score
          score = await service.calculate_compliance_score(
              assessment.responses,
              total_requirements=3
          )

          assert score.percentage == pytest.approx(66.67, 0.01)

          # Identify gaps
          gaps = await service.identify_gaps(
              [{'id': 'REQ-2', 'title': 'Gap requirement'}],
              assessment.responses
          )

          assert len(gaps) == 1

          # Generate recommendations
          recommendations = await service.generate_recommendations(gaps)
          assert len(recommendations) > 0

          # Complete assessment
          result = await service.complete_assessment(assessment.id)
          assert result['status'] == 'completed'
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py:385
_ ERROR at setup of TestComplianceServiceIntegration.test_concurrent_assessments _
file /home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py, line 430
      @pytest.mark.asyncio
      async def test_concurrent_assessments(self, db_session):
          """Test handling multiple concurrent assessments"""
          import asyncio
          service = ComplianceService(db_session)

          # Create multiple assessments concurrently
          tasks = [
              service.create_assessment(
                  framework_id=1,
                  business_id=str(uuid4()),
                  user_id=str(uuid4())
              )
              for _ in range(5)
          ]

          assessments = await asyncio.gather(*tasks)

          assert len(assessments) == 5
          assert all(a.id is not None for a in assessments)
          assert len(set(a.id for a in assessments)) == 5  # All unique
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py:430
_ ERROR at setup of TestComplianceServiceIntegration.test_performance_large_assessment _
file /home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py, line 452
      @pytest.mark.asyncio
      async def test_performance_large_assessment(self, db_session):
          """Test performance with large number of requirements"""
          import time
          service = ComplianceService(db_session)

          # Create assessment with many requirements
          responses = {
              f'REQ-{i}': {
                  'compliant': i % 2 == 0,
                  'evidence': [f'evidence_{i}.pdf'],
                  'notes': f'Response {i}'
              }
              for i in range(100)
          }

          start = time.time()
          score = await service.calculate_compliance_score(responses, 100)
          duration = time.time() - start

          assert score.percentage == 50.0
          assert duration < 1.0  # Should complete within 1 second
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py:452
__________________ ERROR at setup of test_database_connection __________________
file /home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py, line 6
  def test_database_connection(db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py:6
_____________________ ERROR at setup of test_user_creation _____________________
file /home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py, line 12
  def test_user_creation(db_session, sample_user):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py:12
___________________ ERROR at setup of test_business_profile ____________________
file /home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py, line 25
  def test_business_profile(db_session, sample_business_profile):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test-utility-scripts/test_final_verification.py:25
__________ ERROR at setup of TestCostTrackingService.test_track_usage __________
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
_____ ERROR at setup of TestCostTrackingService.test_get_usage_by_service ______
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
____ ERROR at setup of TestCostTrackingService.test_get_usage_by_time_range ____
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
_____ ERROR at setup of TestCostTrackingService.test_calculate_daily_costs _____
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
________ ERROR at setup of TestCostTrackingService.test_get_cost_trends ________
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
____ ERROR at setup of TestCostTrackingService.test_identify_cost_anomalies ____
E   TypeError: CostTrackingService.__init__() got an unexpected keyword argument 'redis_client'
________ ERROR at setup of TestBudgetAlertService.test_set_daily_budget ________
E   TypeError: BudgetAlertService.__init__() got an unexpected keyword argument 'redis_client'
_____ ERROR at setup of TestBudgetAlertService.test_budget_usage_tracking ______
E   TypeError: BudgetAlertService.__init__() got an unexpected keyword argument 'redis_client'
_____ ERROR at setup of TestBudgetAlertService.test_budget_exceeded_alert ______
E   TypeError: BudgetAlertService.__init__() got an unexpected keyword argument 'redis_client'
______ ERROR at setup of TestBudgetAlertService.test_cost_spike_detection ______
E   TypeError: BudgetAlertService.__init__() got an unexpected keyword argument 'redis_client'
____ ERROR at setup of TestBudgetAlertService.test_service_specific_budgets ____
E   TypeError: BudgetAlertService.__init__() got an unexpected keyword argument 'redis_client'
_ ERROR at setup of TestCostOptimizationService.test_model_efficiency_analysis _
E   TypeError: CostOptimizationService.__init__() missing 1 required positional argument: 'manager'
___ ERROR at setup of TestCostOptimizationService.test_caching_optimization ____
E   TypeError: CostOptimizationService.__init__() missing 1 required positional argument: 'manager'
_ ERROR at setup of TestCostOptimizationService.test_batch_processing_optimization _
E   TypeError: CostOptimizationService.__init__() missing 1 required positional argument: 'manager'
_ ERROR at setup of TestCostOptimizationService.test_prompt_optimization_analysis _
E   TypeError: CostOptimizationService.__init__() missing 1 required positional argument: 'manager'
_ ERROR at setup of TestCostOptimizationService.test_comprehensive_optimization_report _
E   TypeError: CostOptimizationService.__init__() missing 1 required positional argument: 'manager'
__ ERROR at setup of TestBiasDetection.test_gender_bias_in_compliance_advice ___
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 23
      def test_gender_bias_in_compliance_advice(
E       fixture 'bias_test_scenarios' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:23
__________ ERROR at setup of TestBiasDetection.test_company_size_bias __________
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 54
      def test_company_size_bias(self, client, mock_ai_client, bias_test_scenarios):
E       fixture 'bias_test_scenarios' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:54
__________ ERROR at setup of TestBiasDetection.test_industry_fairness __________
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 82
      def test_industry_fairness(self, client, mock_ai_client, bias_test_scenarios):
E       fixture 'bias_test_scenarios' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:82
_ ERROR at setup of TestHallucinationPrevention.test_factual_accuracy_against_golden_dataset[GDPR] _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 164
      @pytest.mark.parametrize("framework", ["GDPR", "ISO 27001", "SOX", "HIPAA"])
      def test_factual_accuracy_against_golden_dataset(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:164
_ ERROR at setup of TestHallucinationPrevention.test_factual_accuracy_against_golden_dataset[ISO 27001] _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 164
      @pytest.mark.parametrize("framework", ["GDPR", "ISO 27001", "SOX", "HIPAA"])
      def test_factual_accuracy_against_golden_dataset(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:164
_ ERROR at setup of TestHallucinationPrevention.test_factual_accuracy_against_golden_dataset[SOX] _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 164
      @pytest.mark.parametrize("framework", ["GDPR", "ISO 27001", "SOX", "HIPAA"])
      def test_factual_accuracy_against_golden_dataset(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:164
_ ERROR at setup of TestHallucinationPrevention.test_factual_accuracy_against_golden_dataset[HIPAA] _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 164
      @pytest.mark.parametrize("framework", ["GDPR", "ISO 27001", "SOX", "HIPAA"])
      def test_factual_accuracy_against_golden_dataset(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:164
_ ERROR at setup of TestAdversarialRobustness.test_prompt_injection_resistance _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 286
      def test_prompt_injection_resistance(
E       fixture 'adversarial_inputs' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:286
_ ERROR at setup of TestAdversarialRobustness.test_out_of_scope_question_handling _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 340
      def test_out_of_scope_question_handling(
E       fixture 'adversarial_inputs' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:340
_ ERROR at setup of TestAdversarialRobustness.test_malicious_input_sanitization _
file /home/omar/Documents/ruleIQ/tests/test_ai_ethics.py, line 378
      def test_malicious_input_sanitization(
E       fixture 'adversarial_inputs' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_ethics.py:378
_ ERROR at setup of TestPolicyStreamingService.test_generate_policy_stream_metadata_chunk _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestPolicyStreamingService.test_stream_with_google_success _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestPolicyStreamingService.test_stream_with_openai_success _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
___ ERROR at setup of TestPolicyStreamingService.test_stream_error_handling ____
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestPolicyStreamingService.test_stream_with_circuit_breaker_open _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
__ ERROR at setup of TestPolicyStreamingService.test_stream_progress_updates ___
E   AttributeError: module 'services.ai' has no attribute 'google_client'
___ ERROR at setup of TestPolicyStreamingAPI.test_stream_endpoint_sse_format ___
E   ModuleNotFoundError: No module named 'main'
____ ERROR at setup of TestPolicyStreamingAPI.test_stream_endpoint_headers _____
E   ModuleNotFoundError: No module named 'main'
_ ERROR at setup of TestPolicyStreamingAPI.test_stream_endpoint_error_handling _
E   ModuleNotFoundError: No module named 'main'
_ ERROR at setup of TestPolicyStreamingAPI.test_stream_endpoint_authentication _
E   ModuleNotFoundError: No module named 'main'
_ ERROR at setup of TestPolicyStreamingAPI.test_stream_endpoint_rate_limiting __
E   ModuleNotFoundError: No module named 'main'
___ ERROR at setup of TestPolicyStreamingPerformance.test_streaming_latency ____
file /home/omar/Documents/ruleIQ/tests/test_ai_policy_streaming.py, line 571
      @pytest.mark.asyncio
      async def test_streaming_latency(
          self, policy_generator, sample_request, sample_framework
      ):
          """Test that first chunk arrives quickly."""
          import time

          async def mock_slow_stream(*args, **kwargs):
              yield PolicyStreamingChunk(
                  chunk_id="1", content="First chunk", chunk_type="content",
              )
              await asyncio.sleep(1)  # Simulate slow generation
              yield PolicyStreamingChunk(
                  chunk_id="2", content="Second chunk", chunk_type="content",
              )
              yield PolicyStreamingChunk(chunk_id="3", content="", chunk_type="complete")

          with patch.object(
              policy_generator, "_stream_with_google", side_effect=mock_slow_stream
          ):
              start_time = time.time()
              first_chunk_time = None

              async for chunk in policy_generator.generate_policy_stream(
                  sample_request, sample_framework
              ):
                  if chunk.chunk_type == "content" and first_chunk_time is None:
                      first_chunk_time = time.time()

              # First content chunk should arrive quickly (after metadata)
              assert first_chunk_time - start_time < 0.5
E       fixture 'policy_generator' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_policy_streaming.py:571
_ ERROR at setup of TestPolicyStreamingPerformance.test_streaming_memory_usage _
file /home/omar/Documents/ruleIQ/tests/test_ai_policy_streaming.py, line 603
      @pytest.mark.asyncio
      async def test_streaming_memory_usage(
          self, policy_generator, sample_request, sample_framework
      ):
          """Test that streaming doesn't accumulate large buffers."""
          import tracemalloc

          async def mock_large_stream(*args, **kwargs):
              # Generate many chunks
              for i in range(100):
                  yield PolicyStreamingChunk(
                      chunk_id=str(i),
                      content="x" * 1000,  # 1KB per chunk
                      chunk_type="content",
                  )
              yield PolicyStreamingChunk(
                  chunk_id="complete", content="", chunk_type="complete",
              )

          with patch.object(
              policy_generator, "_stream_with_google", side_effect=mock_large_stream
          ):
              tracemalloc.start()

              chunk_count = 0
              async for chunk in policy_generator.generate_policy_stream(
                  sample_request, sample_framework
              ):
                  chunk_count += 1

              current, peak = tracemalloc.get_traced_memory()
              tracemalloc.stop()

              # Memory usage should be reasonable (not storing all chunks)
              assert peak < 10 * 1024 * 1024  # Less than 10MB
              assert chunk_count > 100  # All chunks processed
E       fixture 'policy_generator' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_ai_policy_streaming.py:603
___ ERROR at setup of TestStreamingEdgeCases.test_streaming_timeout_handling ___
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestStreamingEdgeCases.test_streaming_partial_failure_recovery _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestStreamingEdgeCases.test_streaming_empty_response_handling _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
___ ERROR at setup of TestStreamingEdgeCases.test_streaming_unicode_handling ___
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestStreamingEdgeCases.test_streaming_large_chunk_handling _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_____ ERROR at setup of TestStreamingEdgeCases.test_streaming_rapid_chunks _____
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_____ ERROR at setup of TestStreamingEdgeCases.test_streaming_cancellation _____
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestStreamingEdgeCases.test_streaming_duplicate_chunk_ids __
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_ ERROR at setup of TestStreamingEdgeCases.test_streaming_malformed_json_in_metadata _
E   AttributeError: module 'services.ai' has no attribute 'google_client'
_______________________ ERROR at setup of test_lifespan ________________________
file /home/omar/Documents/ruleIQ/tests/test_app.py, line 13
  @asynccontextmanager
  async def test_lifespan(app: FastAPI):
      """Test lifespan - no database initialization."""
      print("Test app started")
      yield
      print("Test app shutdown")
E       fixture 'app' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_app.py:13
____ ERROR at setup of TestGDPRAccuracy.test_gdpr_penalty_amounts_accuracy _____
file /home/omar/Documents/ruleIQ/tests/test_compliance_accuracy.py, line 17
      def test_gdpr_penalty_amounts_accuracy(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_compliance_accuracy.py:17
_ ERROR at setup of TestGDPRAccuracy.test_gdpr_breach_notification_timeline_accuracy _
file /home/omar/Documents/ruleIQ/tests/test_compliance_accuracy.py, line 119
      def test_gdpr_breach_notification_timeline_accuracy(
E       fixture 'compliance_golden_dataset' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_compliance_accuracy.py:119
_ ERROR at setup of TestCompleteComplianceJourney.test_new_business_complete_gdpr_journey _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 17
      def test_new_business_complete_gdpr_journey(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestCompleteComplianceJourney.test_existing_business_framework_migration_journey _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 207
      def test_existing_business_framework_migration_journey(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestErrorStateHandling.test_network_interruption_recovery __
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 254
      def test_network_interruption_recovery(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestErrorStateHandling.test_invalid_data_graceful_handling _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 297
      def test_invalid_data_graceful_handling(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestErrorStateHandling.test_concurrent_user_conflict_resolution _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 328
      def test_concurrent_user_conflict_resolution(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestErrorStateHandling.test_external_service_failure_fallback _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 372
      def test_external_service_failure_fallback(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____ ERROR at setup of TestAuditWorkflows.test_comprehensive_audit_trail ______
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 420
      def test_comprehensive_audit_trail(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
____ ERROR at setup of TestAuditWorkflows.test_compliance_report_generation ____
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 508
      def test_compliance_report_generation(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestAuditWorkflows.test_regulatory_submission_preparation __
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 576
      def test_regulatory_submission_preparation(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestBusinessContinuityWorkflows.test_data_backup_and_recovery _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 641
      def test_data_backup_and_recovery(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestBusinessContinuityWorkflows.test_compliance_deadline_management _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 689
      def test_compliance_deadline_management(self, client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_ ERROR at setup of TestBusinessContinuityWorkflows.test_multi_framework_coordination _
file /home/omar/Documents/ruleIQ/tests/test_e2e_workflows.py, line 719
      def test_multi_framework_coordination(
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
______________ ERROR at setup of test_authenticated_client_works _______________
file /home/omar/Documents/ruleIQ/tests/test_fixture_isolation.py, line 11
  def test_authenticated_client_works(client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
_____________ ERROR at setup of test_unauthenticated_client_fails ______________
file /home/omar/Documents/ruleIQ/tests/test_fixture_isolation.py, line 37
  def test_unauthenticated_client_fails(unauthenticated_test_client):
E       fixture 'unauthenticated_test_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixture_isolation.py:37
___________ ERROR at setup of test_authenticated_client_works_again ____________
file /home/omar/Documents/ruleIQ/tests/test_fixture_isolation.py, line 46
  def test_authenticated_client_works_again(client, authenticated_headers,
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
________ ERROR at setup of TestDatabaseFixtures.test_db_session_fixture ________
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 12
      def test_db_session_fixture(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:12
_______ ERROR at setup of TestDatabaseFixtures.test_sample_user_fixture ________
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 29
      def test_sample_user_fixture(self, sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:29
_ ERROR at setup of TestDatabaseFixtures.test_sample_business_profile_fixture __
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 36
      def test_sample_business_profile_fixture(self, sample_business_profile):
E       fixture 'sample_business_profile' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:36
____ ERROR at setup of TestDatabaseFixtures.test_authenticated_user_fixture ____
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 42
      def test_authenticated_user_fixture(self, authenticated_user):
E       fixture 'authenticated_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:42
_____ ERROR at setup of TestDatabaseFixtures.test_async_db_session_fixture _____
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 50
      @pytest.mark.asyncio
      async def test_async_db_session_fixture(self, async_db_session):
          """Test that async_db_session fixture works."""
          assert async_db_session is not None
          # Test basic query
          from sqlalchemy import text
          result = await async_db_session.execute(text("SELECT 1"))
          assert result is not None
E       fixture 'async_db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:50
______ ERROR at setup of TestRedisFixtures.test_mock_redis_client_fixture ______
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 62
      def test_mock_redis_client_fixture(self, mock_redis_client):
E       fixture 'mock_redis_client' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:62
_ ERROR at setup of TestAuthenticationFixtures.test_authenticated_headers_fixture _
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 186
      def test_authenticated_headers_fixture(self, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestAuthenticationFixtures.test_admin_headers_fixture ____
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 193
      def test_admin_headers_fixture(self, admin_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 264
  @pytest.fixture
  def admin_headers(db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:264
_ ERROR at setup of TestAuthenticationFixtures.test_authenticated_client_fixture _
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 206
      def test_authenticated_client_fixture(self, authenticated_client):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 245
  @pytest.fixture
  def authenticated_client(client, authenticated_headers):
file /home/omar/Documents/ruleIQ/tests/conftest.py, line 251
  @pytest.fixture
  def authenticated_headers(sample_user, db_session):
E       fixture 'sample_user' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/conftest.py:251
___ ERROR at setup of TestFixtureIsolation.test_db_rollback_between_tests_1 ____
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 304
      def test_db_rollback_between_tests_1(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:304
___ ERROR at setup of TestFixtureIsolation.test_db_rollback_between_tests_2 ____
file /home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py, line 324
      def test_db_rollback_between_tests_2(self, db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_fixtures_validation.py:324
_______________ ERROR at setup of test_assessment_lead_creation ________________
file /home/omar/Documents/ruleIQ/tests/test_freemium_simple.py, line 9
  def test_assessment_lead_creation(db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_freemium_simple.py:9
____________ ERROR at setup of test_assessment_lead_with_all_fields ____________
file /home/omar/Documents/ruleIQ/tests/test_freemium_simple.py, line 29
  def test_assessment_lead_with_all_fields(db_session):
E       fixture 'db_session' not found
>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, admin_headers, anyio_backend, anyio_backend_name, anyio_backend_options, api_headers, async_authenticated_client, async_client, authenticated_client, authenticated_headers, auto_mock_external_services, benchmark, benchmark_weave, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, clean_test_db, cleanup_uploads, client, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_ai_client, mock_anthropic, mock_anthropic_client, mock_celery_task, mock_cloudwatch, mock_datadog, mock_elasticsearch, mock_file_storage, mock_google_ai, mock_google_oauth, mock_http_client, mock_llm, mock_neo4j, mock_neo4j_session, mock_openai, mock_openai_client, mock_s3, mock_secrets_manager, mock_sendgrid, mock_sentry, mock_smtp, mock_stripe, mock_webhook_client, monkeypatch, multipart_headers, no_cover, postgres_checkpointer, postgres_connection, postgres_test_url, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reset_singleton_instances, sample_assessment_data, sample_framework_data, sample_user_data, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/home/omar/Documents/ruleIQ/tests/test_freemium_simple.py:29
=================================== FAILURES ===================================
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:55: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:79: assert 400 == 422
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:89: assert 400 == 422
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:147: assert 500 == 401
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:200: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/api/test_auth_endpoints.py:354: assert 404 == 307
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.google_auth' from '/home/omar/Documents/ruleIQ/api/routers/google_auth.py'> does not have the attribute 'get_google_user_info'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.google_auth' from '/home/omar/Documents/ruleIQ/api/routers/google_auth.py'> does not have the attribute 'get_google_user_info'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'get_profile_by_user_id'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'get_profile_by_user_id'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'create_profile'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'create_profile'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'update_profile'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.business_profiles' from '/home/omar/Documents/ruleIQ/api/routers/business_profiles.py'> does not have the attribute 'update_profile'
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:208: ImportError: cannot import name 'delete_business_profile' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:230: ImportError: cannot import name 'delete_business_profile' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:249: ImportError: cannot import name 'get_compliance_frameworks' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:270: ImportError: cannot import name 'update_compliance_frameworks' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:291: ImportError: cannot import name 'get_risk_assessment' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:326: ImportError: cannot import name 'validate_business_profile' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:353: ImportError: cannot import name 'export_business_profile' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:373: ImportError: cannot import name 'get_profile_history' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/home/omar/Documents/ruleIQ/tests/api/test_business_profiles_router.py:411: ImportError: cannot import name 'get_profile_analytics' from 'api.routers.business_profiles' (/home/omar/Documents/ruleIQ/api/routers/business_profiles.py)
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services.ai' has no attribute 'assistant_service'
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services.ai' has no attribute 'assistant_service'
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services.ai' has no attribute 'assistant_service'
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services.ai' has no attribute 'compliance_advisor'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.middleware.rate_limiter' from '/home/omar/Documents/ruleIQ/api/middleware/rate_limiter.py'> does not have the attribute 'check_rate_limit'
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services' has no attribute 'chat_service'. Did you mean: 'rbac_service'?
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:113: assert 405 == 201
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:145: assert 405 == 200
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:160: KeyError: 'id'
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:223: assert 404 == 201
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:243: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:270: assert 404 == 204
/home/omar/Documents/ruleIQ/tests/api/test_framework_endpoints.py:349: assert 405 == 201
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'get_user_integrations'
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:115: ImportError: cannot import name 'get_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py). Did you mean: 'integrations'?
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:137: ImportError: cannot import name 'create_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:157: ImportError: cannot import name 'create_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:177: ImportError: cannot import name 'update_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:214: ImportError: cannot import name 'delete_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'test_connection'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'test_connection'
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:291: ImportError: cannot import name 'sync_integration' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'get_activity_logs'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'complete_oauth'
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:392: ImportError: cannot import name 'refresh_token' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:420: ImportError: cannot import name 'get_available_integrations' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:463: ImportError: cannot import name 'get_webhooks' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/home/omar/Documents/ruleIQ/tests/api/test_integrations_router.py:501: ImportError: cannot import name 'create_webhook' from 'api.routers.integrations' (/home/omar/Documents/ruleIQ/api/routers/integrations.py)
/usr/lib/python3.12/unittest/mock.py:944: AssertionError: expected call not found.
/home/omar/Documents/ruleIQ/services/policy_service.py:121: core.exceptions.BusinessLogicException: An unexpected error occurred during policy generation: 'coroutine' object has no attribute 'first'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'create_report'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'create_report'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'get_report_by_id'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'get_report_by_id'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'get_user_reports'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'get_user_reports'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'get_report_by_id'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'delete_user_report'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'create_report_schedule'
/home/omar/Documents/ruleIQ/tests/api/test_reports_router.py:318: ImportError: cannot import name 'get_report_status' from 'api.routers.reports' (/home/omar/Documents/ruleIQ/api/routers/reports.py)
/home/omar/Documents/ruleIQ/tests/api/test_reports_router.py:346: ImportError: cannot import name 'export_report_data' from 'api.routers.reports' (/home/omar/Documents/ruleIQ/api/routers/reports.py)
/home/omar/Documents/ruleIQ/tests/api/test_reports_router.py:369: ImportError: cannot import name 'get_report_templates' from 'api.routers.reports' (/home/omar/Documents/ruleIQ/api/routers/reports.py)
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'generate_preview'
/home/omar/Documents/ruleIQ/tests/api/test_reports_router.py:433: ImportError: cannot import name 'batch_generate_reports' from 'api.routers.reports' (/home/omar/Documents/ruleIQ/api/routers/reports.py)
/home/omar/Documents/ruleIQ/tests/api/test_reports_router.py:471: ImportError: cannot import name 'share_report' from 'api.routers.reports' (/home/omar/Documents/ruleIQ/api/routers/reports.py)
/home/omar/Documents/ruleIQ/tests/api/test_webhook_endpoints.py:636: AttributeError: 'Webhook' object has no attribute 'ip_whitelist'
/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:145: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:222: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:285: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:353: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/e2e/test_user_onboarding_flow.py:490: assert 500 == 201
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:94: assert 404 == 422
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:107: assert 404 == 400
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:140: assert 0 > 0
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:152: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:193: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:201: TypeError: create_freemium_token() got an unexpected keyword argument 'expires_delta'
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:388: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:500: assert 404 == 422
/home/omar/Documents/ruleIQ/tests/integration/api/test_freemium_endpoints.py:527: TypeError: create_freemium_token() got an unexpected keyword argument 'expires_delta'
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:190: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:241: assert 404 == 422
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:253: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:276: AssertionError: Rate limiting should be triggered for excessive requests
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:302: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:326: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:353: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:372: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:396: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:415: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:432: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:449: assert 404 == 503
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:470: assert 404 == 502
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:495: assert 404 == 500
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:519: assert 404 in [200, 429]
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:541: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:557: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/integration/api/test_iq_agent_endpoints.py:571: assert 404 == 200
/usr/lib/python3.12/unittest/mock.py:913: AssertionError: Expected 'rollback' to have been called.
/home/omar/Documents/ruleIQ/tests/integration/test_auth_flow.py:207: assert False
/home/omar/Documents/ruleIQ/tests/integration/test_comprehensive_api_workflows.py:88: assert 404 == 201
/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:64: NameError: name 'HTTP_OK' is not defined
/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:121: NameError: name 'HTTP_CREATED' is not defined
/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:200: NameError: name 'HTTP_NOT_FOUND' is not defined
/home/omar/Documents/ruleIQ/tests/integration/test_contract_validation.py:322: NameError: name 'HTTP_UNAUTHORIZED' is not defined
/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:410: assert 404 == 201
/home/omar/Documents/ruleIQ/tests/integration/test_external_service_integration.py:532: assert 404 == 201
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.auth' from '/home/omar/Documents/ruleIQ/api/routers/auth.py'> does not have the attribute 'create_user'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.auth' from '/home/omar/Documents/ruleIQ/api/routers/auth.py'> does not have the attribute 'get_user_by_email'
/home/omar/Documents/ruleIQ/tests/integration/test_user_workflows.py:127: assert 404 == 422
/home/omar/Documents/ruleIQ/tests/integration/test_user_workflows.py:158: assert 404 == 201
/home/omar/Documents/ruleIQ/tests/integration/test_user_workflows.py:253: assert 404 == 201
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.integrations' from '/home/omar/Documents/ruleIQ/api/routers/integrations.py'> does not have the attribute 'list_available'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.reports' from '/home/omar/Documents/ruleIQ/api/routers/reports.py'> does not have the attribute 'create_report_schedule'
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:86: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:109: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:131: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:152: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:171: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:197: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:219: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:253: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:277: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:299: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:321: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:342: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:365: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:393: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:414: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:437: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:458: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:484: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:509: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:530: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:553: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/middleware/test_security_middleware.py:580: TypeError: object Mock can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:114: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:149: NameError: name 'DEFAULT_LIMIT' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:162: NameError: name 'MINUTE_SECONDS' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:224: NameError: name 'HIGH_CONFIDENCE_THRESHOLD' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:328: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:395: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/models/test_compliance_state.py:448: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/monitoring/test_langgraph_metrics.py:465: KeyError: 'transition_type'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:183: AttributeError: 'PrometheusExporter' object has no attribute 'endpoint'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:454: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:466: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:489: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:516: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:541: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:556: TypeError: MetricsBridge.__init__() got an unexpected keyword argument 'enable_langgraph'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:577: TypeError: MetricsBridge.__init__() got an unexpected keyword argument 'enable_langgraph'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:627: TypeError: MetricsBridge.__init__() missing 1 required positional argument: 'otel_collector'
/home/omar/Documents/ruleIQ/tests/monitoring/test_opentelemetry_metrics.py:643: AttributeError: 'OpenTelemetryMetricsCollector' object has no attribute 'export_state'
/usr/lib/python3.12/unittest/mock.py:1197: services.ai.exceptions.ModelUnavailableException: [Model 'gemini-2.5-pro' is unavailable: Circuit open] An error occurred with an external service.
/home/omar/Documents/ruleIQ/tests/performance/test_ai_optimization_performance.py:273: AssertionError: assert 0 == 100
/home/omar/Documents/ruleIQ/services/ai/assistant.py:125: NameError: name 'requests' is not defined
/home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py:40: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/performance/test_api_performance.py:514: assert 404 == 201
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:23: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:30: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:45: assert 'idx_evidence_user_id' in '"""Database migration to add performance indexes for ruleIQ."""\n\nfrom __future__ import annotations\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = "010_add_performance_indexes"\ndown_revision = "009_fix_assessment_sessions_truncation"\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    """Add performance indexes to improve query performance."""\n\n    # Enable pg_trgm extension for text search\n    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")\n\n    # Evidence table indexes\n    op.create_index("idx_evidence_user_status", "evidence_items", ["user_id", "status"])\n\n    op.create_index("idx_evidence_framework", "evidence_items", ["framework_id"])\n\n    op.create_index("idx_evidence_business_profile", "evidence_items", ["business_profile_id"])\n\n    op.create_index("idx_evidence_created_at", "evidence_items", ["created_at"])\n\n    # GIN indexes for text search\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_name_search\n        ON evidence_items USING gin (evidence_name gin_trgm_ops);\n    """)\n\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_description_search\n        ...ent_sessions")\n    op.drop_index("idx_assessment_business_profile", table_name="assessment_sessions")\n    op.drop_index("idx_assessment_created_at", table_name="assessment_sessions")\n\n    # User table indexes\n    op.drop_index("idx_user_email", table_name="users")\n    op.drop_index("idx_user_active", table_name="users")\n\n    # Framework indexes\n    op.drop_index("idx_framework_name", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_category", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_active", table_name="compliance_frameworks")\n\n    # Chat conversation indexes\n    op.drop_index("idx_chat_user_created", table_name="chat_conversations")\n\n    # Integration configuration indexes\n    op.drop_index("idx_integration_user_type", table_name="integration_configurations")\n\n    # Evidence metadata indexes\n    op.drop_index("idx_evidence_metadata_evidence", table_name="evidence_metadata")\n\n    # Implementation plan indexes\n    op.drop_index("idx_implementation_assessment", table_name="implementation_plans")\n\n    # Generated policy indexes\n    op.drop_index("idx_policy_business_profile", table_name="generated_policies")\n'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:50: AssertionError: assert False
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:55: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:61: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:68: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:74: AssertionError: assert False
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:81: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:91: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:91: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:91: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:109: assert 'evidence_user_id' in '"""Database migration to add performance indexes for ruleIQ."""\n\nfrom __future__ import annotations\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = "010_add_performance_indexes"\ndown_revision = "009_fix_assessment_sessions_truncation"\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    """Add performance indexes to improve query performance."""\n\n    # Enable pg_trgm extension for text search\n    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")\n\n    # Evidence table indexes\n    op.create_index("idx_evidence_user_status", "evidence_items", ["user_id", "status"])\n\n    op.create_index("idx_evidence_framework", "evidence_items", ["framework_id"])\n\n    op.create_index("idx_evidence_business_profile", "evidence_items", ["business_profile_id"])\n\n    op.create_index("idx_evidence_created_at", "evidence_items", ["created_at"])\n\n    # GIN indexes for text search\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_name_search\n        ON evidence_items USING gin (evidence_name gin_trgm_ops);\n    """)\n\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_description_search\n        ...ent_sessions")\n    op.drop_index("idx_assessment_business_profile", table_name="assessment_sessions")\n    op.drop_index("idx_assessment_created_at", table_name="assessment_sessions")\n\n    # User table indexes\n    op.drop_index("idx_user_email", table_name="users")\n    op.drop_index("idx_user_active", table_name="users")\n\n    # Framework indexes\n    op.drop_index("idx_framework_name", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_category", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_active", table_name="compliance_frameworks")\n\n    # Chat conversation indexes\n    op.drop_index("idx_chat_user_created", table_name="chat_conversations")\n\n    # Integration configuration indexes\n    op.drop_index("idx_integration_user_type", table_name="integration_configurations")\n\n    # Evidence metadata indexes\n    op.drop_index("idx_evidence_metadata_evidence", table_name="evidence_metadata")\n\n    # Implementation plan indexes\n    op.drop_index("idx_implementation_assessment", table_name="implementation_plans")\n\n    # Generated policy indexes\n    op.drop_index("idx_policy_business_profile", table_name="generated_policies")\n'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:119: assert 'assessment_id' in '"""Database migration to add performance indexes for ruleIQ."""\n\nfrom __future__ import annotations\n\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = "010_add_performance_indexes"\ndown_revision = "009_fix_assessment_sessions_truncation"\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    """Add performance indexes to improve query performance."""\n\n    # Enable pg_trgm extension for text search\n    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")\n\n    # Evidence table indexes\n    op.create_index("idx_evidence_user_status", "evidence_items", ["user_id", "status"])\n\n    op.create_index("idx_evidence_framework", "evidence_items", ["framework_id"])\n\n    op.create_index("idx_evidence_business_profile", "evidence_items", ["business_profile_id"])\n\n    op.create_index("idx_evidence_created_at", "evidence_items", ["created_at"])\n\n    # GIN indexes for text search\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_name_search\n        ON evidence_items USING gin (evidence_name gin_trgm_ops);\n    """)\n\n    op.execute("""\n        CREATE INDEX IF NOT EXISTS idx_evidence_description_search\n        ...ent_sessions")\n    op.drop_index("idx_assessment_business_profile", table_name="assessment_sessions")\n    op.drop_index("idx_assessment_created_at", table_name="assessment_sessions")\n\n    # User table indexes\n    op.drop_index("idx_user_email", table_name="users")\n    op.drop_index("idx_user_active", table_name="users")\n\n    # Framework indexes\n    op.drop_index("idx_framework_name", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_category", table_name="compliance_frameworks")\n    op.drop_index("idx_framework_active", table_name="compliance_frameworks")\n\n    # Chat conversation indexes\n    op.drop_index("idx_chat_user_created", table_name="chat_conversations")\n\n    # Integration configuration indexes\n    op.drop_index("idx_integration_user_type", table_name="integration_configurations")\n\n    # Evidence metadata indexes\n    op.drop_index("idx_evidence_metadata_evidence", table_name="evidence_metadata")\n\n    # Implementation plan indexes\n    op.drop_index("idx_implementation_assessment", table_name="implementation_plans")\n\n    # Generated policy indexes\n    op.drop_index("idx_policy_business_profile", table_name="generated_policies")\n'
/home/omar/Documents/ruleIQ/tests/performance/test_performance_fixes.py:123: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:153: AssertionError: Weak password should be rejected: 123456
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:255: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:319: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:346: KeyError: 'access_token'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:410: assert 0 >= 1
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:429: KeyError: 'access_token'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:481: KeyError: 'access_token'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:619: KeyError: 'access_token'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:676: KeyError: 'access_token'
/home/omar/Documents/ruleIQ/tests/security/test_authentication.py:707: KeyError: 'access_token'
/home/omar/.local/lib/python3.12/site-packages/pydantic/main.py:991: AttributeError: 'Settings' object has no attribute 'secret_key'. Did you mean: 'jwt_secret_key'?
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:40: AttributeError: 'InputValidator' object has no attribute 'sanitize_input'
/home/omar/.local/lib/python3.12/site-packages/pydantic/main.py:991: AttributeError: 'Settings' object has no attribute 'max_file_size'. Did you mean: 'max_file_size_mb'?
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:63: assert 500 == 429
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:67: ImportError: cannot import name 'sanitize_error_for_logging' from 'api.utils.error_handlers' (/home/omar/Documents/ruleIQ/api/utils/error_handlers.py)
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:79: NameError: name 'HTTP_UNAUTHORIZED' is not defined
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:85: AttributeError: 'CredentialEncryption' object has no attribute 'encrypt'
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:93: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:105: TypeError: QueryOptimizer.__init__() missing 1 required positional argument: 'db'
/home/omar/Documents/ruleIQ/tests/security/test_security_fixes.py:119: TypeError: Can't instantiate abstract class GoogleWorkspaceIntegration without an implementation for abstract method 'get_available_evidence_types'
/home/omar/Documents/ruleIQ/services/ai/assistant.py:125: NameError: name 'requests' is not defined
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'get_ai_model' to have been called once. Called 0 times.
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:112: AttributeError: property 'state' of 'AICircuitBreaker' object has no setter
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:125: AttributeError: 'ComplianceAssistant' object has no attribute 'classify_intent'. Did you mean: '_classify_intent'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:140: AttributeError: 'ComplianceAssistant' object has no attribute 'classify_intent'. Did you mean: '_classify_intent'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:154: AttributeError: 'ComplianceAssistant' object has no attribute 'classify_intent'. Did you mean: '_classify_intent'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:165: TypeError: EnumType.__call__() got an unexpected keyword argument 'is_safe'
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:189: TypeError: EnumType.__call__() got an unexpected keyword argument 'is_safe'
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:220: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:240: AttributeError: 'ComplianceAssistant' object has no attribute 'stream_response'. Did you mean: '_stream_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:261: AttributeError: 'ComplianceAssistant' object has no attribute 'analyze_compliance_gap'. Did you mean: 'analyze_evidence_gap'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:279: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:296: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:311: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:328: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:342: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:352: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:365: AttributeError: 'ComplianceAssistant' object has no attribute 'process_batch'
/home/omar/Documents/ruleIQ/tests/services/ai/test_assistant.py:379: AttributeError: 'ComplianceAssistant' object has no attribute 'generate_response'. Did you mean: '_generate_response'?
/home/omar/Documents/ruleIQ/tests/services/ai/test_tools.py:120: Failed: DID NOT RAISE <class 'TypeError'>
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'pytest.__main__' from '/home/omar/.local/lib/python3.12/site-packages/pytest/__main__.py'> does not have the attribute 'tool_registry'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'pytest.__main__' from '/home/omar/.local/lib/python3.12/site-packages/pytest/__main__.py'> does not have the attribute 'tool_executor'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'pytest.__main__' from '/home/omar/.local/lib/python3.12/site-packages/pytest/__main__.py'> does not have the attribute 'tool_registry'
/home/omar/Documents/ruleIQ/tests/services/compliance/test_compliance_service.py:121: AssertionError: assert 'data minimization' in 'only process necessary data'
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'add' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'commit' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'execute' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'send_message' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'send_message' to have been called once. Called 0 times.
/usr/lib/python3.12/pkgutil.py:528: AttributeError: module 'services.email_service' has no attribute 'dns'
/home/omar/Documents/ruleIQ/tests/services/test_email_service.py:347: AssertionError: assert 0 == 3
/home/omar/Documents/ruleIQ/tests/services/test_email_service.py:384: AssertionError: assert 'Product A' in '\n            <h1>Thank you, John Doe!</h1>\n            <p>Your order total: $99.99</p>\n            <ul>\n            {% for item in order.items %}\n                <li>{{item.name}} - ${{item.price}}</li>\n            {% endfor %}\n            </ul>\n            '
/home/omar/Documents/ruleIQ/tests/services/test_email_service.py:405: assert 0.42105263157894735 == 0.42
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'services.email_service' from '/home/omar/Documents/ruleIQ/services/email_service.py'> does not have the attribute 'spamassassin_client'
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'add' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'post' to have been called once. Called 0 times.
/home/omar/Documents/ruleIQ/tests/services/test_integration_service.py:124: AssertionError: assert 0 == 3
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'get' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected '_fetch_source_data' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:923: AssertionError: Expected 'connect' to have been called once. Called 0 times.
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'services.integration_service' from '/home/omar/Documents/ruleIQ/services/integration_service.py'> does not have the attribute 'redis_client'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:107: TypeError: EmailNotification() takes no arguments
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:130: TypeError: SMSNotification() takes no arguments
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:152: TypeError: PushNotification() takes no arguments
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:175: TypeError: InAppNotification() takes no arguments
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b086bbfe0> does not have the attribute '_fetch_template'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:228: AttributeError: 'NotificationService' object has no attribute 'render_template'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:247: AttributeError: 'NotificationQueue' object has no attribute 'add'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b19507500> does not have the attribute '_fetch_preferences'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:289: AttributeError: 'NotificationService' object has no attribute 'update_preferences'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:311: AttributeError: 'NotificationService' object has no attribute 'is_quiet_hours'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:334: AttributeError: 'NotificationService' object has no attribute 'log_history'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b0853da60> does not have the attribute '_query_history'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b087a2de0> does not have the attribute 'send_email'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b087456d0> does not have the attribute '_get_failed_notifications'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:415: AttributeError: 'NotificationService' object has no attribute 'check_rate_limit'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:430: AttributeError: 'NotificationService' object has no attribute 'select_channels'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:449: AttributeError: 'NotificationService' object has no attribute 'format_for_channel'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:476: AttributeError: 'NotificationService' object has no attribute 'schedule'
/home/omar/Documents/ruleIQ/tests/services/test_notification_service.py:492: AttributeError: 'NotificationService' object has no attribute 'cancel_scheduled'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <tests.services.test_notification_service.NotificationService object at 0x781b08561a30> does not have the attribute '_aggregate_metrics'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:35: TypeError: CostEntry.__init__() got an unexpected keyword argument 'service_name'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:57: TypeError: CostEntry.__init__() got an unexpected keyword argument 'service_name'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:89: TypeError: CostEntry.__init__() got an unexpected keyword argument 'service_name'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:104: TypeError: CostEntry.__init__() got an unexpected keyword argument 'service_name'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:126: AttributeError: type object 'BudgetConfig' has no attribute 'get_gemini_config'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:144: AttributeError: type object 'BudgetConfig' has no attribute 'get_openai_config'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:160: TypeError: BudgetConfig.__init__() got an unexpected keyword argument 'model_name'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:511: AttributeError: 'AICostManager' object has no attribute 'track_ai_request'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:533: AttributeError: 'AICostManager' object has no attribute 'track_ai_request'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:554: AttributeError: 'AICostManager' object has no attribute 'set_daily_budget'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:582: AttributeError: 'AICostManager' object has no attribute 'track_ai_request'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:603: AttributeError: 'AICostManager' object has no attribute 'generate_monthly_report'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:633: AttributeError: 'AICostManager' object has no attribute 'set_user_daily_limit'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:681: ImportError: cannot import name 'IntelligentModelRouter' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:708: ImportError: cannot import name 'DynamicCacheManager' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:737: ImportError: cannot import name 'PromptOptimizer' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:764: ImportError: cannot import name 'BatchRequestOptimizer' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:785: ImportError: cannot import name 'CostAnalyticsDashboard' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:811: ImportError: cannot import name 'CostAttributionAnalyzer' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:832: ImportError: cannot import name 'PredictiveCostModeler' from 'services.ai.cost_management' (/home/omar/Documents/ruleIQ/services/ai/cost_management.py)
/home/omar/Documents/ruleIQ/tests/test_ai_cost_management.py:901: AttributeError: 'AICostManager' object has no attribute 'set_daily_budget'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:124: assert 401 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:158: assert 401 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:191: assert 401 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:222: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:249: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:279: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:328: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:368: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:418: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:460: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:491: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:536: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:562: assert 401 == 422
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:581: assert 404 == 422
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:591: assert 404 == 401
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:610: assert 404 == 429
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:630: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:670: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:700: assert 405 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:732: assert 405 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:768: assert 404 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:790: assert 401 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_monitoring.py:842: assert 401 == 200
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:136: AttributeError: 'ConnectionManager' object has no attribute 'broadcast_cost_update'
/usr/lib/python3.12/unittest/mock.py:935: AssertionError: expected call not found.
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:175: AttributeError: 'ConnectionManager' object has no attribute 'broadcast_to_all'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:199: AttributeError: 'ConnectionManager' object has no attribute 'send_budget_alert'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:223: AttributeError: 'ConnectionManager' object has no attribute 'send_budget_alert'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:254: AttributeError: 'ConnectionManager' object has no attribute 'connect_dashboard'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:279: AttributeError: 'ConnectionManager' object has no attribute 'send_dashboard_update'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:303: AttributeError: 'ConnectionManager' object has no attribute 'handle_client_message'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:326: AttributeError: 'ConnectionManager' object has no attribute 'handle_client_message'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:342: AttributeError: 'ConnectionManager' object has no attribute 'handle_client_message'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:359: AttributeError: 'ConnectionManager' object has no attribute 'listen_for_messages'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:369: AttributeError: 'ConnectionManager' object has no attribute 'check_connection'. Did you mean: 'user_connections'?
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:421: AttributeError: 'ConnectionManager' object has no attribute 'handle_client_message'
/home/omar/.local/lib/python3.12/site-packages/starlette/testclient.py:150: starlette.websockets.WebSocketDisconnect
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:490: AttributeError: 'ConnectionManager' object has no attribute 'broadcast_to_all'
/home/omar/Documents/ruleIQ/tests/test_ai_cost_websocket.py:520: AssertionError: assert 0 == 1000
/home/omar/Documents/ruleIQ/tests/test_ai_policy_generator.py:98: AttributeError: 'AICircuitBreaker' object has no attribute 'failure_threshold'
/home/omar/Documents/ruleIQ/tests/test_ai_policy_simple.py:30: AssertionError: assert False
/usr/lib/python3.12/json/encoder.py:180: TypeError: Object of type Mock is not JSON serializable
/home/omar/Documents/ruleIQ/tests/test_ai_policy_simple.py:84: AttributeError: 'PolicyGenerator' object has no attribute '_generate_fallback_content'. Did you mean: '_generate_fallback_policy'?
/home/omar/Documents/ruleIQ/tests/test_ai_policy_simple.py:104: AssertionError: assert <CircuitState.CLOSED: 'closed'> == 'CLOSED'
/home/omar/Documents/ruleIQ/tests/test_ai_policy_simple.py:114: AssertionError: assert False
/home/omar/Documents/ruleIQ/services/ai/policy_generator.py:391: TypeError: 'Mock' object is not iterable
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'services.ai.policy_generator' from '/home/omar/Documents/ruleIQ/services/ai/policy_generator.py'> does not have the attribute 'google_breaker'
/home/omar/Documents/ruleIQ/services/ai/policy_generator.py:391: TypeError: 'Mock' object is not iterable
/home/omar/Documents/ruleIQ/services/ai/policy_generator.py:391: TypeError: 'Mock' object is not iterable
/home/omar/Documents/ruleIQ/tests/test_ai_policy_streaming_simple.py:180: ModuleNotFoundError: No module named 'main'
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/tests/test_ai_rate_limiting.py:94: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/test_ai_rate_limiting.py:109: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/test_ai_rate_limiting.py:114: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/test_ai_rate_limiting.py:119: NameError: name 'MAX_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/test_ai_rate_limiting.py:167: Failed: First request should have been allowed: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/api/middleware/ai_rate_limiter.py:52: NameError: name 'HOUR_SECONDS' is not defined
/home/omar/Documents/ruleIQ/tests/test_auth_fix.py:30: NameError: name 'HTTP_OK' is not defined
/home/omar/Documents/ruleIQ/services/ai/assistant.py:2841: AttributeError: 'tuple' object has no attribute 'strip'
/usr/lib/python3.12/unittest/mock.py:2262: Exception: AI service unavailable
/home/omar/Documents/ruleIQ/services/ai/assistant.py:2292: NameError: name 'requests' is not defined
/home/omar/Documents/ruleIQ/services/ai/assistant.py:2331: NameError: name 'requests' is not defined
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes.py:71: AssertionError: assert 'obligations' in {'check_results': {'compliance_score': 50.0, 'obligations': [{'id': '1', 'satisfied': True, 'title': 'Data Encryption'}, {'id': '2', 'satisfied': False, 'title': 'Data Retention'}], 'satisfied_obligations': 1, 'total_obligations': 2, ...}, 'compliance_score': 50.0, 'regulation': 'GDPR', 'timestamp': '2025-09-05T22:29:42.938471'}
/usr/lib/python3.12/unittest/mock.py:2262: Exception: Compliance service unavailable
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes.py:108: KeyError: 'compliance_score'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes.py:121: TypeError: object list can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes.py:128: TypeError: object list can't be used in 'await' expression
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes.py:141: TypeError: object list can't be used in 'await' expression
/home/omar/Documents/ruleIQ/langgraph_agent/nodes/compliance_nodes.py:304: KeyError: 'compliance_data'
/home/omar/Documents/ruleIQ/langgraph_agent/nodes/compliance_nodes.py:304: KeyError: 'compliance_data'
/home/omar/Documents/ruleIQ/langgraph_agent/nodes/compliance_nodes.py:304: KeyError: 'compliance_data'
/home/omar/Documents/ruleIQ/langgraph_agent/nodes/compliance_nodes.py:304: KeyError: 'compliance_data'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:87: AssertionError: assert 'batch_results' in {}
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:106: KeyError: 'status'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:166: KeyError: 'status'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:184: AssertionError: assert <Mock name='mock.execute().scalars().first().name' id='132057792529088'> == 'GDPR'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:199: AssertionError: assert <Mock name='mock.execute().scalars().first().email' id='132057793768448'> == 'user@example.com'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:212: assert 0 > 0
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:224: assert 'No profile_id' in '{\'type\': \'ComplianceCheckError\', \'message\': "type object \'BusinessProfile\' has no attribute \'company_id\'", \'timestamp\': \'2025-09-05T21:29:43.249459+00:00\'}'
/home/omar/Documents/ruleIQ/tests/test_compliance_nodes_real.py:233: KeyError: 'monitoring_status'
/home/omar/Documents/ruleIQ/config/__init__.py:52: config.ConfigurationError: Failed to load configuration: name 'logger' is not defined
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:24: assert 404 in [400, 422]
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:32: assert 404 in [401, 403]
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.auth' from '/home/omar/Documents/ruleIQ/api/routers/auth.py'> does not have the attribute 'authenticate_user'
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:53: assert 404 in [401, 403]
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:71: assert 404 in [401, 403]
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.auth' from '/home/omar/Documents/ruleIQ/api/routers/auth.py'> does not have the attribute 'refresh_access_token'
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:106: assert 404 in [429, 200, 401, 403]
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:115: assert 404 in [400, 401, 422, 403]
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:126: assert 404 in [400, 401, 422, 403]
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <module 'api.routers.auth' from '/home/omar/Documents/ruleIQ/api/routers/auth.py'> does not have the attribute 'send_password_reset_email'
/home/omar/Documents/ruleIQ/tests/test_critical_auth.py:150: assert 404 in [401, 403]
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:42: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:57: TypeError: 'async_generator' object does not support the asynchronous context manager protocol
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:85: TypeError: 'async_generator' object does not support the asynchronous context manager protocol
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:101: AssertionError: assert 'pool' in {'async_engine_initialized': True, 'async_pool_checked_in': 0, 'async_pool_checked_out': 0, 'async_pool_overflow': -25, ...}
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:131: TypeError: 'async_generator' object does not support the asynchronous context manager protocol
/home/omar/Documents/ruleIQ/tests/test_database_performance.py:164: TypeError: 'async_generator' object does not support the asynchronous context manager protocol
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:103: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:123: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:137: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:164: KeyError: 'evidence_status'
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:184: KeyError: 'evidence_status'
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:208: AttributeError: 'EvidenceCollectionNode' object has no attribute 'check_duplicates'
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:225: AttributeError: 'EvidenceCollectionNode' object has no attribute 'check_duplicates'
/home/omar/Documents/ruleIQ/tests/test_evidence_migration_tdd.py:285: AttributeError: 'EvidenceCollectionNode' object has no attribute 'handle_error'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <langgraph_agent.nodes.evidence_nodes.EvidenceCollectionNode object at 0x781b19b39700> does not have the attribute 'collect_evidence'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_80_percent.py:66: AttributeError: 'EvidenceCollectionNode' object has no attribute 'check_duplicates'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_80_percent.py:98: KeyError: 'error_count'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_80_percent.py:145: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_80_percent.py:179: KeyError: 'error_count'
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <class 'langgraph_agent.nodes.evidence_nodes.EvidenceCollectionNode'> does not have the attribute 'collect_evidence'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_80_percent.py:231: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_additional.py:127: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_additional.py:171: KeyError: 'evidence_status'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_additional.py:219: AttributeError: 'EvidenceCollectionNode' object has no attribute 'collect_evidence'. Did you mean: 'validate_evidence'?
/usr/lib/python3.12/unittest/mock.py:1431: AttributeError: <class 'langgraph_agent.nodes.evidence_nodes.EvidenceCollectionNode'> does not have the attribute 'collect_evidence'
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_final.py:72: Failed: DID NOT RAISE <class 'sqlalchemy.exc.SQLAlchemyError'>
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_final.py:199: AssertionError: assert False
/home/omar/Documents/ruleIQ/tests/test_evidence_nodes_final.py:217: assert True is False
/home/omar/Documents/ruleIQ/tests/test_feedback_system.py:95: AttributeError: type object 'QualityDimension' has no attribute 'ACCURACY'
/home/omar/Documents/ruleIQ/config/langsmith_feedback.py:70: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/config/langsmith_feedback.py:70: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/config/langsmith_feedback.py:70: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/config/langsmith_feedback.py:70: NameError: name 'DEFAULT_RETRIES' is not defined
/home/omar/Documents/ruleIQ/services/ai/evaluation/golden_datasets/loaders.py:98: pydantic_core._pydantic_core.ValidationError: 3 validation errors for ComplianceScenario
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:38: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:82: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:102: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:112: AssertionError: Regex pattern did not match.
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:144: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:167: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:187: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_schemas.py:207: pydantic_core._pydantic_core.ValidationError: 3 validation errors for SourceMeta
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:108: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:120: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:131: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:146: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:159: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:170: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:183: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:209: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:234: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:257: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:271: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:288: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:305: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:333: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:345: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:360: NameError: name 'ExternalDataValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:377: NameError: name 'MAX_INPUT_LENGTH' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:392: NameError: name 'validate_input_bounds' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:396: NameError: name 'DeepValidator' is not defined
/home/omar/Documents/ruleIQ/tests/test_golden_dataset_validators.py:409: NameError: name 'DeepValidator' is not defined
=============================== warnings summary ===============================
tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_response_structure
  /usr/lib/python3.12/inspect.py:2712: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    def __init__(self, name, kind, *, default=_empty, annotation=_empty):
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

tests/test_auth_flow.py::test_auth_flow
  /home/omar/.local/lib/python3.12/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_auth_flow.py::test_auth_flow returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_config_env.py::test_environment_loading
  /home/omar/.local/lib/python3.12/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_config_env.py::test_environment_loading returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_db_connection.py::test_connection
  /home/omar/.local/lib/python3.12/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_db_connection.py::test_connection returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_db_init.py::test_database_connection
  /home/omar/.local/lib/python3.12/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_db_init.py::test_database_connection returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/test_db_init.py::test_sync_initialization
  /home/omar/.local/lib/python3.12/site-packages/_pytest/python.py:161: PytestReturnNotNoneWarning: Test functions should return None, but tests/test_db_init.py::test_sync_initialization returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================= slowest 10 durations =============================
4.29s call     tests/test_evidence_nodes_final.py::TestRetryWithBackoffEdgeCases::test_retry_exhausted
4.00s call     tests/test_auth_flow.py::test_auth_flow
3.41s call     tests/test_evidence_nodes_integration.py::TestEvidenceCollectionNodeIntegration::test_retry_with_backoff
2.21s call     tests/test_db_init.py::test_async_operations
2.01s call     tests/test_evidence_orchestrator_v2.py::TestParallelCollectionMechanisms::test_collection_with_timeout
1.49s call     tests/test_evidence_nodes_coverage.py::TestEvidenceCollectionNodeUncovered::test_retry_with_correct_signature
1.10s call     tests/monitoring/test_langgraph_metrics.py::TestNodeExecutionTracker::test_node_execution_timeout
1.06s setup    tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_basic_questions_accuracy
1.02s teardown tests/test_golden_dataset_validators.py::TestSecurityValidation::test_regex_dos_protection
1.01s call     tests/performance/test_api_performance.py::TestAPIPerformance::test_authentication_performance
=========================== short test summary info ============================
FAILED tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_success
FAILED tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_invalid_email
FAILED tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_weak_password
FAILED tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_nonexistent_user
FAILED tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user_invalid_token
FAILED tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_login_redirect
FAILED tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_callback_success
FAILED tests/api/test_auth_endpoints.py::TestGoogleOAuth::test_google_callback_invalid_code
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_business_profile_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_business_profile_not_found
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_create_business_profile_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_create_business_profile_duplicate
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_business_profile_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_business_profile_not_found
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_delete_business_profile_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_delete_business_profile_not_found
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_compliance_frameworks_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_update_compliance_frameworks_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_get_risk_assessment_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_validate_business_profile_success
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_export_business_profile_json
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_business_profile_history
FAILED tests/api/test_business_profiles_router.py::TestBusinessProfilesRouter::test_business_profile_analytics
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_send_chat_message
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_get_chat_history
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_create_conversation
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_list_conversations
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_delete_conversation
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_chat_streaming
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_suggestions
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_search
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_export_conversation
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_feedback
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_context_management
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_ai_compliance_advisor
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_rate_limiting
FAILED tests/api/test_chat_endpoints.py::TestChatEndpoints::test_chat_with_attachments
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_create_custom_framework
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_update_framework
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_delete_framework
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_add_requirement
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_update_requirement
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_delete_requirement
FAILED tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_import
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_list_integrations_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_by_id_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_integration_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_integration_duplicate
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_update_integration_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_delete_integration_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_test_integration_connection_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_test_integration_connection_failure
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_sync_integration_data_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_logs_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_oauth_callback_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_refresh_integration_token_success
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_available_integrations
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_get_integration_webhooks
FAILED tests/api/test_integrations_router.py::TestIntegrationsRouter::test_create_webhook_success
FAILED tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_without_optional_fields
FAILED tests/api/test_policies_router.py::TestPoliciesRouter::test_generate_policy_with_validation_error
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_generate_report_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_generate_report_async_processing
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_get_report_by_id_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_get_report_not_found
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_list_reports_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_list_reports_with_filters
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_download_report_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_delete_report_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_schedule_report_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_get_report_status_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_export_report_csv
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_get_report_templates_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_preview_report_success
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_batch_generate_reports
FAILED tests/api/test_reports_router.py::TestReportsRouter::test_share_report_success
FAILED tests/api/test_webhook_endpoints.py::TestWebhookSecurityEndpoints::test_webhook_ip_whitelist
FAILED tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_with_assessment_restart
FAILED tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_with_minimal_data
FAILED tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_user_onboarding_error_recovery
FAILED tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_triggers_background_tasks
FAILED tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_sets_user_preferences
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_invalid_format
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_missing_consent
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_rate_limiting
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_performance
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_invalid_token
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_expired_token
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_invalid_token
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_sql_injection_prevention
FAILED tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_token_expiration_security
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_success
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_invalid_request
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_unauthenticated
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_rate_limiting
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_store_memory_success
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_retrieve_memories_success
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_graph_initialization_success
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_health_check_healthy
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_health_check_degraded
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_status_endpoint
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_status_endpoint_degraded
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_iq_agent_service_unavailable
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_compliance_query_ai_service_error
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_memory_storage_error_handling
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_concurrent_queries
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_large_query_handling
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_query_with_special_characters
FAILED tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentEndpoints::test_background_task_execution
FAILED tests/integration/test_assessment_workflow.py::TestAssessmentWorkflow::test_assessment_rollback_on_error
FAILED tests/integration/test_auth_flow.py::TestAuthenticationFlow::test_rbac_permission_check
FAILED tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_complete_authentication_workflow
FAILED tests/integration/test_contract_validation.py::TestAPIContractValidation::test_openapi_schema_generation
FAILED tests/integration/test_contract_validation.py::TestAPIContractValidation::test_authentication_endpoint_contract_validation
FAILED tests/integration/test_contract_validation.py::TestAPIContractValidation::test_error_response_contract_consistency
FAILED tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_authentication_contract_security
FAILED tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_session_management
FAILED tests/integration/test_external_service_integration.py::TestEmailServiceIntegration::test_password_reset_email_integration
FAILED tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_complete_registration_flow
FAILED tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_registration_with_existing_email
FAILED tests/integration/test_user_workflows.py::TestUserRegistrationWorkflow::test_registration_with_weak_password
FAILED tests/integration/test_user_workflows.py::TestComplianceAssessmentWorkflow::test_complete_assessment_flow
FAILED tests/integration/test_user_workflows.py::TestPolicyGenerationWorkflow::test_complete_policy_workflow
FAILED tests/integration/test_user_workflows.py::TestIntegrationManagementWorkflow::test_slack_integration_workflow
FAILED tests/integration/test_user_workflows.py::TestReportingWorkflow::test_scheduled_report_workflow
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_security_headers
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_csp_header
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_add_hsts_header
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_referrer_policy_header
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_permissions_policy_header
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_remove_server_header
FAILED tests/middleware/test_security_middleware.py::TestSecurityHeadersMiddleware::test_cors_headers_for_api
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_rate_limiting
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_rate_limit_exceeded
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_xss_protection
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_sql_injection_protection
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_path_traversal_protection
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_request_size_limit
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_jwt_validation_success
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_jwt_validation_failure
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_ip_whitelist_check
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_ip_blacklist_check
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_csrf_token_validation
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_csrf_token_mismatch
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_secure_cookie_handling
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_request_logging
FAILED tests/middleware/test_security_middleware.py::TestEnhancedSecurityMiddleware::test_security_headers_cascade
FAILED tests/models/test_compliance_state.py::TestEvidenceAccumulation::test_evidence_accumulation_preserves_existing
FAILED tests/models/test_compliance_state.py::TestCostTracking::test_cost_tracker_update
FAILED tests/models/test_compliance_state.py::TestCostTracking::test_cost_accumulation
FAILED tests/models/test_compliance_state.py::TestDecisionTracking::test_decision_structure
FAILED tests/models/test_compliance_state.py::TestContextValidation::test_context_structure
FAILED tests/models/test_compliance_state.py::TestPerformanceMetrics::test_node_execution_times
FAILED tests/models/test_compliance_state.py::TestStateValidation::test_type_coercion
FAILED tests/monitoring/test_langgraph_metrics.py::TestStateTransitionTracker::test_complex_state_machine
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_exporter_initialization
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_bridge_initialization
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_counter_synchronization
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_histogram_synchronization
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_gauge_synchronization
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_export_to_prometheus
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestMetricsBridge::test_automatic_langgraph_instrumentation
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_end_to_end_metrics_pipeline
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_backwards_compatibility
FAILED tests/monitoring/test_opentelemetry_metrics.py::TestIntegration::test_metrics_persistence_across_restarts
FAILED tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_model_fallback_performance
FAILED tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_memory_usage_streaming
FAILED tests/performance/test_ai_optimization_performance.py::TestAIOptimizationPerformance::test_cost_optimization_simulation
FAILED tests/performance/test_api_performance.py::TestAPIPerformance::test_authentication_performance
FAILED tests/performance/test_api_performance.py::TestEndToEndPerformance::test_complete_onboarding_performance
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimizer_initialization
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_n1_query_prevention
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_database_indexes_migration
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_caching_implementation
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_batch_operations
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_memory_leak_prevention
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_performance_monitoring
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_connection_pooling
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_slow_query_detection
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[select_related-JOIN]
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[prefetch_related-IN]
FAILED tests/performance/test_performance_fixes.py::TestPerformanceFixes::test_query_optimization_strategies[batch_select-BATCH]
FAILED tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_index_coverage
FAILED tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_composite_indexes
FAILED tests/performance/test_performance_fixes.py::TestDatabaseIndexes::test_index_performance_impact
FAILED tests/security/test_authentication.py::TestAuthenticationSecurity::test_password_strength_validation
FAILED tests/security/test_authentication.py::TestAuthenticationSecurity::test_email_enumeration_protection
FAILED tests/security/test_authentication.py::TestAuthenticationSecurity::test_session_management_security
FAILED tests/security/test_authentication.py::TestAuthenticationSecurity::test_password_change_security
FAILED tests/security/test_authentication.py::TestAuthenticationSecurity::test_concurrent_session_limits
FAILED tests/security/test_authentication.py::TestTokenSecurity::test_token_contains_no_sensitive_data
FAILED tests/security/test_authentication.py::TestTokenSecurity::test_token_expiry_enforcement
FAILED tests/security/test_authentication.py::TestAuthorizationSecurity::test_privilege_escalation_protection
FAILED tests/security/test_authentication.py::TestAuthorizationSecurity::test_api_rate_limiting_by_user
FAILED tests/security/test_authentication.py::TestAuthorizationSecurity::test_cross_origin_request_security
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_jwt_secret_key_protection
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_input_sanitization
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_file_upload_security
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_rate_limiting
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_sensitive_data_redaction
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_admin_authentication
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_credential_encryption
FAILED tests/security/test_security_fixes.py::TestSecurityFixes::test_sql_injection_prevention
FAILED tests/security/test_security_fixes.py::TestPerformanceFixes::test_n1_query_prevention
FAILED tests/security/test_security_fixes.py::TestIntegrationFixes::test_google_workspace_integration
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_get_task_appropriate_model_simple
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_get_task_appropriate_model_complex
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_circuit_breaker_protection
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_compliance
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_assessment
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_classify_intent_with_cache
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_with_safety_check
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_unsafe_content
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_generate_response_with_tools
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_stream_response
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_analyze_compliance_gap
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_performance_optimization
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_quality_monitoring
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_analytics_tracking
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_context_management
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_error_handling_model_unavailable
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_error_handling_database_error
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_batch_processing
FAILED tests/services/ai/test_assistant.py::TestComplianceAssistant::test_retry_mechanism
FAILED tests/services/ai/test_tools.py::TestBaseTool::test_base_tool_interface
FAILED tests/services/ai/test_tools.py::TestModuleFunctions::test_register_tool_function
FAILED tests/services/ai/test_tools.py::TestModuleFunctions::test_execute_tool_function
FAILED tests/services/ai/test_tools.py::TestModuleFunctions::test_get_tool_schemas_function
FAILED tests/services/compliance/test_compliance_service.py::TestComplianceService::test_identify_compliance_gaps
FAILED tests/services/compliance/test_compliance_service.py::TestComplianceService::test_create_assessment
FAILED tests/services/compliance/test_compliance_service.py::TestComplianceService::test_update_assessment_response
FAILED tests/services/compliance/test_compliance_service.py::TestComplianceService::test_get_framework_requirements
FAILED tests/services/test_email_service.py::TestEmailService::test_send_email
FAILED tests/services/test_email_service.py::TestEmailService::test_send_email_with_attachments
FAILED tests/services/test_email_service.py::TestEmailService::test_validate_email_domain
FAILED tests/services/test_email_service.py::TestEmailService::test_email_retry_logic
FAILED tests/services/test_email_service.py::TestEmailService::test_email_template_rendering
FAILED tests/services/test_email_service.py::TestEmailService::test_email_analytics
FAILED tests/services/test_email_service.py::TestEmailService::test_spam_score_check
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_register_integration
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_send_webhook
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_webhook_retry_logic
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_api_connector
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_data_sync_service
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_websocket_connection
FAILED tests/services/test_integration_service.py::TestIntegrationService::test_integration_rate_limiting
FAILED tests/services/test_notification_service.py::TestNotificationService::test_send_email_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_send_sms_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_send_push_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_create_in_app_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_get_notification_template
FAILED tests/services/test_notification_service.py::TestNotificationService::test_render_template
FAILED tests/services/test_notification_service.py::TestNotificationService::test_queue_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_get_user_preferences
FAILED tests/services/test_notification_service.py::TestNotificationService::test_update_user_preferences
FAILED tests/services/test_notification_service.py::TestNotificationService::test_check_quiet_hours
FAILED tests/services/test_notification_service.py::TestNotificationService::test_log_notification_history
FAILED tests/services/test_notification_service.py::TestNotificationService::test_get_notification_history
FAILED tests/services/test_notification_service.py::TestNotificationService::test_bulk_send_notifications
FAILED tests/services/test_notification_service.py::TestNotificationService::test_retry_failed_notifications
FAILED tests/services/test_notification_service.py::TestNotificationService::test_notification_rate_limiting
FAILED tests/services/test_notification_service.py::TestNotificationService::test_notification_channel_selection
FAILED tests/services/test_notification_service.py::TestNotificationService::test_format_notification_content
FAILED tests/services/test_notification_service.py::TestNotificationService::test_schedule_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_cancel_scheduled_notification
FAILED tests/services/test_notification_service.py::TestNotificationService::test_notification_analytics
FAILED tests/test_ai_cost_management.py::TestAIUsageMetrics::test_metrics_initialization
FAILED tests/test_ai_cost_management.py::TestAIUsageMetrics::test_metrics_aggregation
FAILED tests/test_ai_cost_management.py::TestAIUsageMetrics::test_cost_per_token_calculation
FAILED tests/test_ai_cost_management.py::TestAIUsageMetrics::test_efficiency_score_calculation
FAILED tests/test_ai_cost_management.py::TestModelCostConfig::test_gemini_cost_calculation
FAILED tests/test_ai_cost_management.py::TestModelCostConfig::test_openai_cost_calculation
FAILED tests/test_ai_cost_management.py::TestModelCostConfig::test_custom_model_config
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_track_api_call
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_daily_cost_summary
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_budget_monitoring_integration
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_optimization_recommendations
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_cost_reporting_endpoints
FAILED tests/test_ai_cost_management.py::TestAICostManager::test_user_cost_limits
FAILED tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_intelligent_model_routing
FAILED tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_dynamic_caching_strategy
FAILED tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_prompt_compression_optimization
FAILED tests/test_ai_cost_management.py::TestCostOptimizationStrategies::test_batch_request_optimization
FAILED tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_executive_cost_dashboard
FAILED tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_cost_attribution_analysis
FAILED tests/test_ai_cost_management.py::TestCostReportingAndAnalytics::test_predictive_cost_modeling
FAILED tests/test_ai_cost_management.py::TestIntegrationWithAIServices::test_budget_alert_integration
FAILED tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_success
FAILED tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_with_cache_hit
FAILED tests/test_ai_cost_monitoring.py::TestCostTracking::test_track_usage_with_error
FAILED tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_set_budget_success
FAILED tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_get_budget_status
FAILED tests/test_ai_cost_monitoring.py::TestBudgetManagement::test_budget_alert_triggered
FAILED tests/test_ai_cost_monitoring.py::TestUsageAnalytics::test_get_usage_analytics_daily
FAILED tests/test_ai_cost_monitoring.py::TestUsageAnalytics::test_get_usage_analytics_by_service
FAILED tests/test_ai_cost_monitoring.py::TestCostOptimization::test_get_optimization_insights
FAILED tests/test_ai_cost_monitoring.py::TestCostOptimization::test_simulate_optimization
FAILED tests/test_ai_cost_monitoring.py::TestCostSummary::test_get_cost_summary_current_month
FAILED tests/test_ai_cost_monitoring.py::TestCostSummary::test_get_detailed_cost_breakdown
FAILED tests/test_ai_cost_monitoring.py::TestErrorHandling::test_track_usage_invalid_tokens
FAILED tests/test_ai_cost_monitoring.py::TestErrorHandling::test_set_budget_invalid_limits
FAILED tests/test_ai_cost_monitoring.py::TestErrorHandling::test_unauthorized_access
FAILED tests/test_ai_cost_monitoring.py::TestRateLimiting::test_rate_limit_exceeded
FAILED tests/test_ai_cost_monitoring.py::TestCostAlerts::test_webhook_alert_triggered
FAILED tests/test_ai_cost_monitoring.py::TestCostAlerts::test_alert_history
FAILED tests/test_ai_cost_monitoring.py::TestCostExport::test_export_cost_data_csv
FAILED tests/test_ai_cost_monitoring.py::TestCostExport::test_export_cost_data_json
FAILED tests/test_ai_cost_monitoring.py::TestCostForecasting::test_forecast_monthly_costs
FAILED tests/test_ai_cost_monitoring.py::TestCostMonitoringIntegration::test_full_cost_tracking_workflow
FAILED tests/test_ai_cost_monitoring.py::TestCostMonitoringSmoke::test_basic_cost_tracking
FAILED tests/test_ai_cost_websocket.py::TestCostUpdates::test_broadcast_cost_update
FAILED tests/test_ai_cost_websocket.py::TestCostUpdates::test_send_personal_message
FAILED tests/test_ai_cost_websocket.py::TestCostUpdates::test_broadcast_to_all
FAILED tests/test_ai_cost_websocket.py::TestBudgetAlerts::test_send_budget_alert
FAILED tests/test_ai_cost_websocket.py::TestBudgetAlerts::test_critical_budget_alert
FAILED tests/test_ai_cost_websocket.py::TestLiveDashboard::test_dashboard_initial_data
FAILED tests/test_ai_cost_websocket.py::TestLiveDashboard::test_dashboard_periodic_updates
FAILED tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_client_message
FAILED tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_query_message
FAILED tests/test_ai_cost_websocket.py::TestWebSocketMessages::test_handle_invalid_message
FAILED tests/test_ai_cost_websocket.py::TestConnectionResilience::test_connection_timeout
FAILED tests/test_ai_cost_websocket.py::TestConnectionResilience::test_connection_lost
FAILED tests/test_ai_cost_websocket.py::TestWebSocketSecurity::test_rate_limiting
FAILED tests/test_ai_cost_websocket.py::TestWebSocketIntegration::test_full_websocket_flow
FAILED tests/test_ai_cost_websocket.py::TestWebSocketPerformance::test_concurrent_connections
FAILED tests/test_ai_cost_websocket.py::TestWebSocketPerformance::test_message_throughput
FAILED tests/test_ai_policy_generator.py::TestPolicyGenerationService::test_circuit_breaker_configuration
FAILED tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_template_processor_initialization
FAILED tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_cache_key_generation
FAILED tests/test_ai_policy_simple.py::TestPolicyGeneratorBasic::test_fallback_content_generation
FAILED tests/test_ai_policy_simple.py::TestPolicyGeneratorIntegration::test_circuit_breaker_integration
FAILED tests/test_ai_policy_simple.py::TestPolicyGeneratorIntegration::test_template_processor_integration
FAILED tests/test_ai_policy_streaming.py::TestPolicyStreamingIntegration::test_end_to_end_streaming_google
FAILED tests/test_ai_policy_streaming.py::TestPolicyStreamingIntegration::test_end_to_end_streaming_openai
FAILED tests/test_ai_policy_streaming_edge_cases.py::TestStreamingConcurrency::test_multiple_concurrent_streams
FAILED tests/test_ai_policy_streaming_edge_cases.py::TestStreamingConcurrency::test_stream_isolation
FAILED tests/test_ai_policy_streaming_simple.py::TestPolicyStreamingAPI::test_stream_endpoint_format
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_allows_requests_within_limit
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_blocks_requests_over_limit
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_burst_allowance
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_window_reset
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_rate_limiter_different_users
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiter::test_get_remaining_requests
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_followup_limiter_configuration
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_analysis_limiter_configuration
FAILED tests/test_ai_rate_limiting.py::TestAIRateLimiterInstances::test_ai_recommendations_limiter_configuration
FAILED tests/test_ai_rate_limiting.py::TestRateLimitingIntegration::test_rate_limiting_with_mock_endpoint
FAILED tests/test_ai_rate_limiting.py::TestRateLimitingIntegration::test_concurrent_rate_limiting
FAILED tests/test_auth_fix.py::TestAuthenticationFix::test_fixed_authentication_flow
FAILED tests/test_compliance_assistant_assessment.py::TestAssessmentHelp::test_get_assessment_help_success
FAILED tests/test_compliance_assistant_assessment.py::TestAssessmentHelp::test_get_assessment_help_with_fallback
FAILED tests/test_compliance_assistant_assessment.py::TestAssessmentFollowup::test_generate_assessment_followup_success
FAILED tests/test_compliance_assistant_assessment.py::TestAssessmentAnalysis::test_analyze_assessment_results_success
FAILED tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_successful
FAILED tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_with_errors
FAILED tests/test_compliance_nodes.py::TestComplianceCheckNode::test_compliance_check_empty_documents
FAILED tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_successful
FAILED tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_no_documents
FAILED tests/test_compliance_nodes.py::TestExtractRequirementsFromRAG::test_extract_requirements_with_metadata
FAILED tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_high
FAILED tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_medium
FAILED tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_low
FAILED tests/test_compliance_nodes.py::TestAssessComplianceRisk::test_assess_risk_with_recommendations
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_batch_compliance_update_success
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_single_compliance_check_success
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_update_compliance_for_profile
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_get_default_framework
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_get_user_for_profile
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_batch_compliance_error_handling
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_single_compliance_check_no_profile
FAILED tests/test_compliance_nodes_real.py::TestRealComplianceNodes::test_compliance_monitoring_disabled
FAILED tests/test_config_env.py::test_environment_override - config.Configura...
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_requires_credentials
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_with_invalid_credentials
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_login_with_valid_credentials
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_protected_endpoint_requires_auth
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_token_expiry_handling
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_token_refresh_flow
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_rate_limiting_on_login
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_sql_injection_protection
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_xss_protection_in_responses
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_password_reset_flow
FAILED tests/test_critical_auth.py::TestAuthenticationSecurity::test_session_timeout
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_connection_pool_stress
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_query_performance_baseline
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_n_plus_one_detection
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_connection_pool_metrics
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_bulk_operations_performance
FAILED tests/test_database_performance.py::TestDatabasePerformance::test_cache_performance_impact
FAILED tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_success
FAILED tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_no_items
FAILED tests/test_evidence_migration_tdd.py::TestEvidenceCollection::test_collect_evidence_database_error
FAILED tests/test_evidence_migration_tdd.py::TestEvidenceProcessing::test_process_evidence_success
FAILED tests/test_evidence_migration_tdd.py::TestEvidenceProcessing::test_process_evidence_validation_failure
FAILED tests/test_evidence_migration_tdd.py::TestDuplicateDetection::test_detect_duplicate_evidence
FAILED tests/test_evidence_migration_tdd.py::TestDuplicateDetection::test_no_duplicates_found
FAILED tests/test_evidence_migration_tdd.py::TestErrorHandling::test_handle_processing_error
FAILED tests/test_evidence_migration_tdd.py::TestErrorHandling::test_error_recovery
FAILED tests/test_evidence_nodes_80_percent.py::TestDuplicateDetection::test_process_evidence_detects_duplicate
FAILED tests/test_evidence_nodes_80_percent.py::TestDuplicateDetection::test_missing_evidence_data_handling
FAILED tests/test_evidence_nodes_80_percent.py::TestDatabaseOperations::test_database_connection_lost
FAILED tests/test_evidence_nodes_80_percent.py::TestEvidenceProcessingEdgeCases::test_processor_initialization_failure
FAILED tests/test_evidence_nodes_80_percent.py::TestEvidenceProcessingEdgeCases::test_evidence_node_wrapper_function
FAILED tests/test_evidence_nodes_80_percent.py::TestConcurrentOperations::test_concurrent_evidence_collection
FAILED tests/test_evidence_nodes_additional.py::TestProcessPendingEvidence::test_process_pending_items
FAILED tests/test_evidence_nodes_additional.py::TestValidationLogic::test_validation_with_multiple_criteria
FAILED tests/test_evidence_nodes_additional.py::TestErrorRecoveryMechanisms::test_graceful_degradation
FAILED tests/test_evidence_nodes_additional.py::TestMessageHandling::test_evidence_node_with_messages
FAILED tests/test_evidence_nodes_final.py::TestProcessEvidenceAdditionalPaths::test_process_evidence_database_exception_reraise
FAILED tests/test_evidence_nodes_final.py::TestStaleEvidenceCleanup::test_process_evidence_stale_evidence_cleanup
FAILED tests/test_evidence_nodes_final.py::TestValidateEvidenceScoring::test_validate_evidence_low_score
FAILED tests/test_feedback_system.py::TestFeedbackModels::test_quality_score_with_dimensions
FAILED tests/test_feedback_system.py::TestFeedbackStorage::test_store_feedback_in_memory
FAILED tests/test_feedback_system.py::TestFeedbackStorage::test_feedback_persistence_to_file
FAILED tests/test_feedback_system.py::TestFeedbackStorage::test_feedback_queue_management
FAILED tests/test_feedback_system.py::TestFeedbackSystemIntegration::test_end_to_end_feedback_flow
FAILED tests/test_golden_dataset_loaders.py::TestGoldenDatasetLoader::test_parse_dataset_types
FAILED tests/test_golden_dataset_schemas.py::TestCommonSchemas::test_source_meta
FAILED tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_minimal
FAILED tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_full
FAILED tests/test_golden_dataset_schemas.py::TestComplianceScenario::test_compliance_scenario_missing_triggers
FAILED tests/test_golden_dataset_schemas.py::TestEvidenceCase::test_evidence_case_minimal
FAILED tests/test_golden_dataset_schemas.py::TestEvidenceCase::test_evidence_case_full
FAILED tests/test_golden_dataset_schemas.py::TestRegulatoryQAPair::test_regulatory_qa_minimal
FAILED tests/test_golden_dataset_schemas.py::TestRegulatoryQAPair::test_regulatory_qa_full
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_semantic_layer_valid
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_semantic_layer_missing_description
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_cross_reference_layer_valid
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_cross_reference_layer_orphaned_evidence
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_regulatory_accuracy_layer_valid
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_regulatory_accuracy_layer_invalid_citation
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_temporal_consistency_layer_valid
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_temporal_consistency_layer_overlapping
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_validate_full_dataset
FAILED tests/test_golden_dataset_validators.py::TestDeepValidator::test_validate_compliance_scenario
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_calculate_trust_score_high
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_calculate_trust_score_low
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_trust_subscores
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_scenario
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_with_warnings
FAILED tests/test_golden_dataset_validators.py::TestExternalDataValidator::test_validate_external_invalid_source
FAILED tests/test_golden_dataset_validators.py::TestSecurityValidation::test_input_bounds_checking
FAILED tests/test_golden_dataset_validators.py::TestSecurityValidation::test_deeply_nested_structure_protection
FAILED tests/test_golden_dataset_validators.py::TestSecurityValidation::test_rate_limiting
FAILED tests/test_golden_dataset_validators.py::TestSecurityValidation::test_regex_dos_protection
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_basic_questions_accuracy
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_intermediate_questions_accuracy
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_gdpr_advanced_questions_accuracy
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_source_citation_accuracy
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_response_completeness
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_framework_specific_terminology
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_consistency_across_similar_questions
ERROR tests/ai/test_compliance_accuracy.py::TestComplianceAccuracy::test_regulatory_compliance_alignment
ERROR tests/ai/test_compliance_accuracy.py::TestFrameworkCoverage::test_framework_identification_accuracy
ERROR tests/ai/test_compliance_accuracy.py::TestFrameworkCoverage::test_cross_framework_guidance
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_create_assessment
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_create_assessment_invalid_framework
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_not_found
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_list_assessments
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_list_assessments_with_filters
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_update_assessment
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_delete_assessment
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_submit_assessment_response
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_bulk_submit_responses
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_score
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_get_assessment_gaps
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_generate_assessment_report
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_complete_assessment
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_ai_assessment_assistance
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_evidence_upload
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_collaboration
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_history
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_export
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentEndpoints::test_assessment_import
ERROR tests/api/test_assessment_endpoints.py::TestAssessmentIntegration::test_complete_assessment_workflow
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_register_duplicate_email
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_success
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_invalid_password
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_login_inactive_user
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_get_current_user_expired_token
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_refresh_token
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_logout
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_password_reset_request
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_password_reset_confirm
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_change_password
ERROR tests/api/test_auth_endpoints.py::TestAuthEndpoints::test_verify_email
ERROR tests/api/test_auth_endpoints.py::TestRBACAuth::test_admin_access
ERROR tests/api/test_auth_endpoints.py::TestRBACAuth::test_regular_user_denied_admin
ERROR tests/api/test_auth_endpoints.py::TestRBACAuth::test_role_based_content
ERROR tests/api/test_auth_endpoints.py::TestAuthIntegration::test_complete_auth_flow
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_dashboard_metrics
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_compliance_overview
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_assessment_summary
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_trend_data
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_risk_matrix
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_activity_feed
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_upcoming_deadlines
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_team_performance
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_export_dashboard_report
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_compliance_gaps
ERROR tests/api/test_dashboard_endpoints.py::TestDashboardEndpoints::test_get_ai_insights
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_list_evidence_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_list_evidence_with_filters
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_by_id_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_invalid_file_type
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_upload_evidence_file_too_large
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_metadata
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_validate_evidence_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_download_evidence_success
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_bulk_upload_evidence
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_link_evidence_to_requirement
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_by_assessment
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_search_evidence
ERROR tests/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_evidence_expiry_check
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_list_frameworks
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_by_id
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_not_found
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_create_framework_non_admin
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_framework_requirements
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_get_requirement_detail
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_categories
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_statistics
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_mapping
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_export
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_search
ERROR tests/api/test_framework_endpoints.py::TestFrameworkEndpoints::test_framework_recommendations
ERROR tests/api/test_framework_endpoints.py::TestFrameworkIntegration::test_framework_assessment_integration
ERROR tests/api/test_framework_endpoints.py::TestFrameworkIntegration::test_multi_framework_compliance
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_list_frameworks_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_list_frameworks_with_filters
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_by_id_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_not_found
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_create_framework_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_create_framework_duplicate_name
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_update_framework_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_delete_framework_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_requirements
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_controls
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_map_frameworks_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_import_framework_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_export_framework_success
ERROR tests/api/test_frameworks_endpoints.py::TestFrameworksEndpoints::test_get_framework_statistics
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_list_policies_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_list_policies_with_filters
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_get_policy_by_id_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_create_policy_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_generate_policy_with_ai
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_update_policy_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_submit_policy_for_approval
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_approve_policy_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_reject_policy_with_comments
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_get_policy_versions
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_clone_policy_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_delete_policy_success
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_export_policy_to_pdf
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_check_policy_compliance
ERROR tests/api/test_policies_endpoints.py::TestPoliciesEndpoints::test_search_policies
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_generate_policy_ai
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_list_policy_templates
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_get_policy_template
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_create_policy_from_template
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_list_user_policies
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_get_policy_by_id
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_update_policy
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_delete_policy
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_validation
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_comparison
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_export
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_version_history
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_approval_workflow
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_publish
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_batch_policy_generation
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_ai_enhancement
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_translation
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_search
ERROR tests/api/test_policy_endpoints.py::TestPolicyEndpoints::test_policy_compliance_monitoring
ERROR tests/api/test_policy_endpoints.py::TestPolicyIntegration::test_complete_policy_workflow
ERROR tests/api/test_policy_endpoints.py::TestPolicyIntegration::test_multi_framework_policy_generation
ERROR tests/database/test_freemium_models.py::TestAssessmentLead::test_create_assessment_lead_minimal
ERROR tests/database/test_freemium_models.py::TestAssessmentLead::test_create_assessment_lead_with_utm
ERROR tests/database/test_freemium_models.py::TestAssessmentLead::test_assessment_lead_email_unique_constraint
ERROR tests/database/test_freemium_models.py::TestAssessmentLead::test_assessment_lead_score_update
ERROR tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_create_assessment_session
ERROR tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_assessment_session_ai_responses_storage
ERROR tests/database/test_freemium_models.py::TestFreemiumAssessmentSession::test_assessment_session_expiration
ERROR tests/database/test_freemium_models.py::TestAIQuestionBank::test_create_ai_question
ERROR tests/database/test_freemium_models.py::TestAIQuestionBank::test_ai_question_weighting_and_difficulty
ERROR tests/database/test_freemium_models.py::TestLeadScoringEvent::test_create_lead_scoring_event
ERROR tests/database/test_freemium_models.py::TestLeadScoringEvent::test_lead_scoring_with_metadata
ERROR tests/database/test_freemium_models.py::TestConversionEvent::test_create_conversion_event
ERROR tests/database/test_freemium_models.py::TestFreemiumModelRelationships::test_lead_to_sessions_relationship
ERROR tests/database/test_freemium_models.py::TestFreemiumModelRelationships::test_cascade_delete_behavior
ERROR tests/database/test_repositories.py::TestBaseRepository::test_create_entity
ERROR tests/database/test_repositories.py::TestBaseRepository::test_get_by_id
ERROR tests/database/test_repositories.py::TestBaseRepository::test_get_all_with_pagination
ERROR tests/database/test_repositories.py::TestBaseRepository::test_update_entity
ERROR tests/database/test_repositories.py::TestBaseRepository::test_delete_entity
ERROR tests/database/test_repositories.py::TestBaseRepository::test_count_entities
ERROR tests/database/test_repositories.py::TestAssessmentRepository::test_get_by_organization
ERROR tests/database/test_repositories.py::TestAssessmentRepository::test_get_by_status
ERROR tests/database/test_repositories.py::TestAssessmentRepository::test_get_with_framework
ERROR tests/database/test_repositories.py::TestAssessmentRepository::test_update_progress
ERROR tests/database/test_repositories.py::TestPolicyRepository::test_get_approved_policies
ERROR tests/database/test_repositories.py::TestPolicyRepository::test_get_by_category
ERROR tests/database/test_repositories.py::TestPolicyRepository::test_search_policies
ERROR tests/database/test_repositories.py::TestPolicyRepository::test_clone_policy
ERROR tests/database/test_repositories.py::TestEvidenceRepository::test_get_by_assessment
ERROR tests/database/test_repositories.py::TestEvidenceRepository::test_get_validated_evidence
ERROR tests/database/test_repositories.py::TestEvidenceRepository::test_validate_evidence
ERROR tests/database/test_repositories.py::TestEvidenceRepository::test_check_expiry
ERROR tests/database/test_repositories.py::TestUserRepository::test_get_by_email
ERROR tests/database/test_repositories.py::TestUserRepository::test_get_active_users
ERROR tests/database/test_repositories.py::TestUserRepository::test_update_last_login
ERROR tests/database/test_user_repository.py::TestUserRepository::test_create_user_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_create_user_duplicate_email
ERROR tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_id_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_id_not_found
ERROR tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_email_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_get_user_by_email_case_insensitive
ERROR tests/database/test_user_repository.py::TestUserRepository::test_update_user_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_update_user_password
ERROR tests/database/test_user_repository.py::TestUserRepository::test_delete_user_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_delete_user_not_found
ERROR tests/database/test_user_repository.py::TestUserRepository::test_verify_user_email
ERROR tests/database/test_user_repository.py::TestUserRepository::test_list_users_with_pagination
ERROR tests/database/test_user_repository.py::TestUserRepository::test_list_users_with_filter
ERROR tests/database/test_user_repository.py::TestUserRepository::test_authenticate_user_success
ERROR tests/database/test_user_repository.py::TestUserRepository::test_authenticate_user_wrong_password
ERROR tests/database/test_user_repository.py::TestUserRepository::test_authenticate_inactive_user
ERROR tests/database/test_user_repository.py::TestUserRepository::test_update_last_login
ERROR tests/database/test_user_repository.py::TestUserRepository::test_search_users_by_name
ERROR tests/database/test_user_repository.py::TestUserRepository::test_count_users
ERROR tests/database/test_user_repository.py::TestUserRepository::test_bulk_create_users
ERROR tests/database/test_user_repository.py::TestUserRepository::test_database_error_handling
ERROR tests/e2e/test_user_onboarding_flow.py::TestUserOnboardingFlow::test_complete_user_onboarding_workflow
ERROR tests/e2e/test_user_onboarding_flow.py::TestOnboardingIntegration::test_onboarding_creates_audit_trail
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_authentication_required
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_help_endpoint_invalid_framework
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_followup_questions_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_analysis_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_recommendations_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_feedback_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_metrics_endpoint_success
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_service_timeout_handling
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_quota_exceeded_handling
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_ai_content_filter_handling
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_invalid_request_data_validation
ERROR tests/integration/api/test_ai_assessments.py::TestAIAssessmentEndpoints::test_business_profile_not_found
ERROR tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_ai_help_rate_limiting
ERROR tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_ai_analysis_rate_limiting
ERROR tests/integration/api/test_ai_assessments.py::TestAIRateLimiting::test_regular_endpoints_higher_rate_limit
ERROR tests/integration/api/test_ai_assessments.py::TestAIErrorHandling::test_concurrent_ai_requests_handling
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_analytics_dashboard_success
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_usage_analytics_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_cost_analytics_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_system_alerts_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_resolve_alert_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestAnalyticsEndpoints::test_resolve_nonexistent_alert
ERROR tests/integration/api/test_analytics_endpoints.py::TestPerformanceEndpoints::test_performance_metrics_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestPerformanceEndpoints::test_optimize_performance_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_cache_metrics_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_clear_cache_endpoint
ERROR tests/integration/api/test_analytics_endpoints.py::TestCacheEndpoints::test_analytics_error_handling
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_create_conversation
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_send_message_to_conversation
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_get_evidence_recommendations
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_compliance_analysis
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_get_conversations_list
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_conversation_not_found
ERROR tests/integration/api/test_chat_endpoints.py::TestChatEndpoints::test_compliance_analysis_missing_business_profile
ERROR tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_invalid_framework_for_analysis
ERROR tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_missing_message_content
ERROR tests/integration/api/test_chat_endpoints.py::TestChatValidation::test_ai_assistant_error_handling
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_context_aware_recommendations_success
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_evidence_collection_workflow_generation
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_policy_generation
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_smart_compliance_guidance
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_missing_business_profile_error
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_ai_service_error_handling
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatEndpoints::test_invalid_framework_parameter
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_workflow_generation_parameter_validation
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_policy_generation_parameter_validation
ERROR tests/integration/api/test_enhanced_chat_endpoints.py::TestEnhancedChatValidation::test_smart_guidance_parameter_validation
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_single_evidence
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_evidence_force_reclassify
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_bulk_classify_evidence
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_get_control_mapping_suggestions
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_get_classification_statistics
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_classify_nonexistent_evidence
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationAPI::test_bulk_classify_with_invalid_evidence
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_bulk_classify_too_many_items
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_invalid_confidence_threshold
ERROR tests/integration/api/test_evidence_classification.py::TestEvidenceClassificationValidation::test_classification_ai_service_error
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_success
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_validation_error
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_create_evidence_item_unauthenticated
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_success
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_empty
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_items_with_filtering
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_by_id_success
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_by_id_not_found
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_item_unauthorized_access
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_item_success
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_update_evidence_item_partial
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_item_success
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_delete_evidence_item_unauthorized
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_bulk_update_evidence_status
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_get_evidence_statistics
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceEndpoints::test_search_evidence_items
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_validate_evidence_quality
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_identify_evidence_requirements
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidenceValidationEndpoints::test_configure_evidence_automation
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidencePaginationAndSorting::test_evidence_pagination
ERROR tests/integration/api/test_evidence_endpoints.py::TestEvidencePaginationAndSorting::test_evidence_sorting
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_success
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumEmailCapture::test_capture_email_duplicate
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_success
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_ai_service_error
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAssessmentStart::test_start_assessment_resume_existing
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_success
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_invalid_question_id
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_assessment_complete
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_validation_error
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumAnswerQuestion::test_answer_question_ai_error_fallback
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_success
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_incomplete_assessment
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_cached
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumResults::test_get_results_performance
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_success
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_duplicate_event
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumConversionTracking::test_track_conversion_invalid_event_type
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_xss_prevention
ERROR tests/integration/api/test_freemium_endpoints.py::TestFreemiumSecurityAndValidation::test_oversized_payload_rejection
ERROR tests/integration/api/test_iq_agent_endpoints.py::TestIQAgentLoadTesting::test_sustained_query_load
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_evidence_quality_analysis
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_detect_evidence_duplicates
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_batch_duplicate_detection
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_quality_benchmark
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_get_quality_trends
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_quality_analysis_nonexistent_evidence
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisAPI::test_duplicate_detection_insufficient_evidence
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_duplicate_detection_invalid_threshold
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_batch_duplicate_detection_too_many_items
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_quality_trends_invalid_days
ERROR tests/integration/api/test_quality_analysis.py::TestQualityAnalysisValidation::test_quality_analysis_ai_service_error
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_timeout_fallback
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_quota_exceeded_fallback
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_content_filter_handling
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_model_error_fallback
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_parsing_error_handling
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_network_error_fallback
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_multiple_ai_service_failures
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_partial_ai_service_degradation
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_recovery_after_failure
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_logging_and_monitoring
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_fallback_to_mock_data
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_context_preservation
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_circuit_breaker_pattern
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_service_graceful_shutdown
ERROR tests/integration/test_ai_error_handling.py::TestAIErrorHandling::test_ai_error_rate_monitoring
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_analysis_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_recommendations_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_help_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_circuit_breaker_status_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_selection_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_with_circuit_breaker_open
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_fallback_chain
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_streaming_error_handling
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_performance_metrics_endpoint
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_concurrent_streaming_requests
ERROR tests/integration/test_ai_optimization_endpoints.py::TestAIOptimizationEndpoints::test_model_health_check_endpoint
ERROR tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_endpoint_success
ERROR tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_endpoint_authentication_required
ERROR tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_includes_performance_data
ERROR tests/integration/test_cached_content_api.py::TestCachedContentAPI::test_cache_metrics_with_ai_activity
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_compliance_assessment_pipeline_integration
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_evidence_collection_workflow_integration
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_ai_service_circuit_breaker_integration
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_cross_service_data_consistency
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_concurrent_api_operations_integration
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_rate_limiting_integration_across_endpoints
ERROR tests/integration/test_comprehensive_api_workflows.py::TestComprehensiveAPIWorkflows::test_error_handling_integration_across_services
ERROR tests/integration/test_comprehensive_api_workflows.py::TestAPIWorkflowPerformance::test_assessment_workflow_performance
ERROR tests/integration/test_contract_validation.py::TestAPIContractValidation::test_assessment_endpoint_contract_validation
ERROR tests/integration/test_contract_validation.py::TestAPIContractValidation::test_iq_agent_endpoint_contract_validation
ERROR tests/integration/test_contract_validation.py::TestAPIContractValidation::test_evidence_upload_contract_validation
ERROR tests/integration/test_contract_validation.py::TestAPIContractValidation::test_field_mapper_contract_validation
ERROR tests/integration/test_contract_validation.py::TestAPIContractValidation::test_pagination_contract_validation
ERROR tests/integration/test_contract_validation.py::TestContractPerformance::test_schema_validation_performance
ERROR tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_input_sanitization_contract
ERROR tests/integration/test_contract_validation.py::TestSecurityContractValidation::test_rate_limiting_contract_headers
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_connection
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_transaction_rollback
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_no_persistence_between_tests
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_postgres_table_creation
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_connection
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_expiration
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_redis_data_types
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_async_postgres_connection
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_connection_pool_settings
ERROR tests/integration/test_database_connections.py::TestDatabaseConnections::test_fixtures_work_together
ERROR tests/integration/test_database_connections.py::TestConnectionResilience::test_postgres_connection_recovery
ERROR tests/integration/test_database_connections.py::TestConnectionResilience::test_redis_connection_recovery
ERROR tests/integration/test_database_connections.py::TestMockFixtures::test_mock_redis_client
ERROR tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_full_evidence_and_reporting_flow
ERROR tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_ai_assistant_evidence_query
ERROR tests/integration/test_evidence_flow.py::TestEvidenceCollectionFlow::test_scheduled_report_generation
ERROR tests/integration/test_evidence_flow.py::TestAPIEndpointsIntegration::test_business_profile_to_evidence_workflow
ERROR tests/integration/test_evidence_flow.py::TestAPIEndpointsIntegration::test_framework_to_readiness_assessment
ERROR tests/integration/test_evidence_flow.py::TestErrorHandlingAndResilience::test_integration_failure_handling
ERROR tests/integration/test_evidence_flow.py::TestErrorHandlingAndResilience::test_report_generation_with_no_data
ERROR tests/integration/test_evidence_flow.py::TestAsyncOperations::test_async_evidence_collection
ERROR tests/integration/test_evidence_flow.py::TestAsyncOperations::test_ai_assistant_async_processing
ERROR tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_gemini_integration_with_circuit_breaker
ERROR tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_ai_service_circuit_breaker_states
ERROR tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_ai_service_timeout_handling
ERROR tests/integration/test_external_service_integration.py::TestAIServiceIntegration::test_multiple_ai_provider_fallback
ERROR tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_connection_pool_behavior
ERROR tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_transaction_consistency
ERROR tests/integration/test_external_service_integration.py::TestDatabaseIntegration::test_database_failover_behavior
ERROR tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_caching_behavior
ERROR tests/integration/test_external_service_integration.py::TestRedisIntegration::test_redis_cache_invalidation
ERROR tests/integration/test_external_service_integration.py::TestEmailServiceIntegration::test_assessment_completion_notification
ERROR tests/integration/test_external_service_integration.py::TestFileStorageIntegration::test_file_upload_storage_integration
ERROR tests/integration/test_external_service_integration.py::TestFileStorageIntegration::test_file_processing_integration
ERROR tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_companies_house_api_integration
ERROR tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_external_api_timeout_handling
ERROR tests/integration/test_external_service_integration.py::TestThirdPartyAPIIntegration::test_external_api_rate_limiting_respect
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_user_registration_flow
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_user_login_flow
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_access
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_token_refresh_flow
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_logout_flow
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_invalid_credentials_rejection
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_nonexistent_user_rejection
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_duplicate_registration_rejection
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_without_token
ERROR tests/integration/test_jwt_auth_integration.py::TestJWTAuthenticationFlow::test_protected_endpoint_with_invalid_token
ERROR tests/integration/test_jwt_auth_integration.py::TestBusinessProfileIntegration::test_business_profile_access_with_auth
ERROR tests/integration/test_jwt_auth_integration.py::TestBusinessProfileIntegration::test_business_profile_access_without_auth
ERROR tests/integration/test_jwt_auth_integration.py::TestRAGSystemIntegration::test_chat_endpoint_with_auth
ERROR tests/integration/test_jwt_auth_integration.py::TestRAGSystemIntegration::test_chat_endpoint_without_auth
ERROR tests/integration/test_jwt_auth_integration.py::TestAuthenticationSystemIntegration::test_complete_user_journey
ERROR tests/integration/test_jwt_auth_integration.py::TestAuthenticationSystemIntegration::test_authentication_system_health
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_exporter_initialization
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_prometheus_format_conversion
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_prometheus_http_endpoint
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_histogram_buckets_configuration
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_metric_name_sanitization
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestPrometheusExporter::test_multi_collector_aggregation
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_node_execution_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_workflow_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_error_tracking_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_state_transition_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_checkpoint_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_message_queue_metrics
ERROR tests/monitoring/test_opentelemetry_metrics.py::TestLangGraphMetricsInstrumentor::test_memory_usage_metrics
ERROR tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_help_response_time_under_threshold
ERROR tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_help_concurrent_requests_performance
ERROR tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_analysis_performance_with_large_dataset
ERROR tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_service_timeout_handling
ERROR tests/performance/test_ai_performance.py::TestAIPerformance::test_ai_caching_improves_performance
ERROR tests/performance/test_ai_performance.py::TestAIRateLimiting::test_ai_help_rate_limit_enforcement
ERROR tests/performance/test_ai_performance.py::TestAIRateLimiting::test_ai_analysis_stricter_rate_limit
ERROR tests/performance/test_ai_performance.py::TestAIRateLimiting::test_regular_endpoints_higher_rate_limits
ERROR tests/performance/test_ai_performance.py::TestAIRateLimiting::test_rate_limit_headers_present
ERROR tests/performance/test_ai_performance.py::TestAIRateLimiting::test_rate_limit_reset_after_window
ERROR tests/performance/test_ai_performance.py::TestAILoadTesting::test_ai_endpoint_load_capacity
ERROR tests/performance/test_ai_performance.py::TestAILoadTesting::test_ai_memory_usage_under_load
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_creation_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_creation_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_search_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_evidence_search_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_dashboard_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_dashboard_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_concurrent_request_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_bulk_operation_performance
ERROR tests/performance/test_api_performance.py::TestAPIPerformance::test_bulk_operation_performance
ERROR tests/performance/test_api_performance.py::TestMemoryPerformance::test_large_dataset_handling
ERROR tests/performance/test_api_performance.py::TestMemoryPerformance::test_concurrent_memory_usage
ERROR tests/performance/test_api_performance.py::TestDatabasePerformance::test_complex_query_performance
ERROR tests/performance/test_api_performance.py::TestDatabasePerformance::test_complex_query_performance
ERROR tests/performance/test_api_performance.py::TestDatabasePerformance::test_aggregation_performance
ERROR tests/performance/test_api_performance.py::TestDatabasePerformance::test_aggregation_performance
ERROR tests/performance/test_api_performance.py::TestRealWorldScenarios::test_daily_user_workflow
ERROR tests/performance/test_api_performance.py::TestRealWorldScenarios::test_peak_usage_simulation
ERROR tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_evidence_query_scaling
ERROR tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_full_text_search_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_aggregation_query_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseQueryPerformance::test_join_query_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_connection_pool_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_transaction_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseConnectionPerformance::test_bulk_operation_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseIndexPerformance::test_indexed_query_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseIndexPerformance::test_unindexed_query_performance
ERROR tests/performance/test_database_performance.py::TestDatabaseResourceUsage::test_memory_usage_optimization
ERROR tests/performance/test_database_performance.py::TestDatabaseResourceUsage::test_connection_cleanup
ERROR tests/security/test_authentication.py::TestAuthenticationSecurity::test_unauthenticated_access_denied
ERROR tests/security/test_authentication.py::TestAuthenticationSecurity::test_invalid_token_rejected
ERROR tests/security/test_authentication.py::TestAuthenticationSecurity::test_expired_token_handling
ERROR tests/security/test_authentication.py::TestAuthenticationSecurity::test_token_without_bearer_prefix
ERROR tests/security/test_authentication.py::TestAuthenticationSecurity::test_malformed_authorization_header
ERROR tests/security/test_authentication.py::TestTokenSecurity::test_token_signature_validation
ERROR tests/security/test_authentication.py::TestTokenSecurity::test_token_algorithm_confusion
ERROR tests/security/test_authentication.py::TestAuthorizationSecurity::test_user_data_isolation
ERROR tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_full_workflow_assessment
ERROR tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_caching_performance
ERROR tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_concurrent_requests
ERROR tests/services/ai/test_assistant.py::TestComplianceAssistantIntegration::test_rate_limiting
ERROR tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_full_assessment_workflow
ERROR tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_concurrent_assessments
ERROR tests/services/compliance/test_compliance_service.py::TestComplianceServiceIntegration::test_performance_large_assessment
ERROR tests/test-utility-scripts/test_final_verification.py::test_database_connection
ERROR tests/test-utility-scripts/test_final_verification.py::test_user_creation
ERROR tests/test-utility-scripts/test_final_verification.py::test_business_profile
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_track_usage
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_get_usage_by_service
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_get_usage_by_time_range
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_calculate_daily_costs
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_get_cost_trends
ERROR tests/test_ai_cost_management.py::TestCostTrackingService::test_identify_cost_anomalies
ERROR tests/test_ai_cost_management.py::TestBudgetAlertService::test_set_daily_budget
ERROR tests/test_ai_cost_management.py::TestBudgetAlertService::test_budget_usage_tracking
ERROR tests/test_ai_cost_management.py::TestBudgetAlertService::test_budget_exceeded_alert
ERROR tests/test_ai_cost_management.py::TestBudgetAlertService::test_cost_spike_detection
ERROR tests/test_ai_cost_management.py::TestBudgetAlertService::test_service_specific_budgets
ERROR tests/test_ai_cost_management.py::TestCostOptimizationService::test_model_efficiency_analysis
ERROR tests/test_ai_cost_management.py::TestCostOptimizationService::test_caching_optimization
ERROR tests/test_ai_cost_management.py::TestCostOptimizationService::test_batch_processing_optimization
ERROR tests/test_ai_cost_management.py::TestCostOptimizationService::test_prompt_optimization_analysis
ERROR tests/test_ai_cost_management.py::TestCostOptimizationService::test_comprehensive_optimization_report
ERROR tests/test_ai_ethics.py::TestBiasDetection::test_gender_bias_in_compliance_advice
ERROR tests/test_ai_ethics.py::TestBiasDetection::test_company_size_bias
ERROR tests/test_ai_ethics.py::TestBiasDetection::test_industry_fairness
ERROR tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[GDPR]
ERROR tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[ISO 27001]
ERROR tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[SOX]
ERROR tests/test_ai_ethics.py::TestHallucinationPrevention::test_factual_accuracy_against_golden_dataset[HIPAA]
ERROR tests/test_ai_ethics.py::TestAdversarialRobustness::test_prompt_injection_resistance
ERROR tests/test_ai_ethics.py::TestAdversarialRobustness::test_out_of_scope_question_handling
ERROR tests/test_ai_ethics.py::TestAdversarialRobustness::test_malicious_input_sanitization
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_generate_policy_stream_metadata_chunk
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_google_success
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_openai_success
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_error_handling
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_with_circuit_breaker_open
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingService::test_stream_progress_updates
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_sse_format
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_headers
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_error_handling
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_authentication
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingAPI::test_stream_endpoint_rate_limiting
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingPerformance::test_streaming_latency
ERROR tests/test_ai_policy_streaming.py::TestPolicyStreamingPerformance::test_streaming_memory_usage
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_timeout_handling
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_partial_failure_recovery
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_empty_response_handling
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_unicode_handling
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_large_chunk_handling
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_rapid_chunks
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_cancellation
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_duplicate_chunk_ids
ERROR tests/test_ai_policy_streaming_edge_cases.py::TestStreamingEdgeCases::test_streaming_malformed_json_in_metadata
ERROR tests/test_app.py::test_lifespan
ERROR tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_penalty_amounts_accuracy
ERROR tests/test_compliance_accuracy.py::TestGDPRAccuracy::test_gdpr_breach_notification_timeline_accuracy
ERROR tests/test_e2e_workflows.py::TestCompleteComplianceJourney::test_new_business_complete_gdpr_journey
ERROR tests/test_e2e_workflows.py::TestCompleteComplianceJourney::test_existing_business_framework_migration_journey
ERROR tests/test_e2e_workflows.py::TestErrorStateHandling::test_network_interruption_recovery
ERROR tests/test_e2e_workflows.py::TestErrorStateHandling::test_invalid_data_graceful_handling
ERROR tests/test_e2e_workflows.py::TestErrorStateHandling::test_concurrent_user_conflict_resolution
ERROR tests/test_e2e_workflows.py::TestErrorStateHandling::test_external_service_failure_fallback
ERROR tests/test_e2e_workflows.py::TestAuditWorkflows::test_comprehensive_audit_trail
ERROR tests/test_e2e_workflows.py::TestAuditWorkflows::test_compliance_report_generation
ERROR tests/test_e2e_workflows.py::TestAuditWorkflows::test_regulatory_submission_preparation
ERROR tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_data_backup_and_recovery
ERROR tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_compliance_deadline_management
ERROR tests/test_e2e_workflows.py::TestBusinessContinuityWorkflows::test_multi_framework_coordination
ERROR tests/test_fixture_isolation.py::test_authenticated_client_works
ERROR tests/test_fixture_isolation.py::test_unauthenticated_client_fails
ERROR tests/test_fixture_isolation.py::test_authenticated_client_works_again
ERROR tests/test_fixtures_validation.py::TestDatabaseFixtures::test_db_session_fixture
ERROR tests/test_fixtures_validation.py::TestDatabaseFixtures::test_sample_user_fixture
ERROR tests/test_fixtures_validation.py::TestDatabaseFixtures::test_sample_business_profile_fixture
ERROR tests/test_fixtures_validation.py::TestDatabaseFixtures::test_authenticated_user_fixture
ERROR tests/test_fixtures_validation.py::TestDatabaseFixtures::test_async_db_session_fixture
ERROR tests/test_fixtures_validation.py::TestRedisFixtures::test_mock_redis_client_fixture
ERROR tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_authenticated_headers_fixture
ERROR tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_admin_headers_fixture
ERROR tests/test_fixtures_validation.py::TestAuthenticationFixtures::test_authenticated_client_fixture
ERROR tests/test_fixtures_validation.py::TestFixtureIsolation::test_db_rollback_between_tests_1
ERROR tests/test_fixtures_validation.py::TestFixtureIsolation::test_db_rollback_between_tests_2
ERROR tests/test_freemium_simple.py::test_assessment_lead_creation
ERROR tests/test_freemium_simple.py::test_assessment_lead_with_all_fields
!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 999 failures !!!!!!!!!!!!!!!!!!!!!!!!!!
===== 445 failed, 567 passed, 25 skipped, 6 warnings, 554 errors in 48.93s =====
--- Logging error ---
Traceback (most recent call last):
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 550, in _receive_metrics
    self._exporter.export(
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 170, in export
    self.out.write(self.formatter(metrics_data))
ValueError: I/O operation on closed file.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
ValueError: I/O operation on closed file.
Call stack:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 533, in _ticker
    self.collect(timeout_millis=self._export_interval_millis)
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 343, in collect
    self._receive_metrics(
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 554, in _receive_metrics
    _logger.exception("Exception while exporting metrics")
Message: 'Exception while exporting metrics'
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 550, in _receive_metrics
    self._exporter.export(
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 170, in export
    self.out.write(self.formatter(metrics_data))
ValueError: I/O operation on closed file.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.12/logging/__init__.py", line 1163, in emit
    stream.write(msg + self.terminator)
ValueError: I/O operation on closed file.
Call stack:
  File "/usr/lib/python3.12/threading.py", line 1030, in _bootstrap
    self._bootstrap_inner()
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 533, in _ticker
    self.collect(timeout_millis=self._export_interval_millis)
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 343, in collect
    self._receive_metrics(
  File "/home/omar/.local/lib/python3.12/site-packages/opentelemetry/sdk/metrics/_internal/export/__init__.py", line 554, in _receive_metrics
    _logger.exception("Exception while exporting metrics")
Message: 'Exception while exporting metrics'
Arguments: ()
