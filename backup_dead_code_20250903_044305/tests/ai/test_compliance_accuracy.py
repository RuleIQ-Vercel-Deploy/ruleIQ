"""
from __future__ import annotations

AI Tests for Compliance Accuracy

Tests the AI assistant's accuracy on compliance questions using golden datasets
and validates response quality, factual correctness, and regulatory alignment.
"""

import json
from pathlib import Path
from typing import Any, Dict, List
from unittest.mock import patch
from uuid import uuid4

import pytest

from services.ai.assistant import ComplianceAssistant


@pytest.mark.ai
@pytest.mark.compliance
@pytest.mark.golden
class TestComplianceAccuracy:
    """Test AI compliance accuracy using golden datasets"""

    @pytest.fixture(scope="class")
    def gdpr_golden_dataset(self):
        """Load GDPR golden dataset"""
        dataset_path = Path(__file__).parent / "golden_datasets" / "gdpr_questions.json"
        with open(dataset_path) as f:
            return json.load(f)

    @pytest.mark.asyncio
    async def test_gdpr_basic_questions_accuracy(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test AI accuracy on basic GDPR questions"""
        assistant = ComplianceAssistant(db_session)

        basic_questions = [q for q in gdpr_golden_dataset if q["difficulty"] == "basic"]
        correct_answers = 0
        total_questions = len(basic_questions)

        for question_data in basic_questions:
            # Mock AI response with realistic compliance content
            mock_response = self._generate_mock_response(question_data)
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                # Check if response contains expected keywords
                if self._validate_response_accuracy(response, question_data):
                    correct_answers += 1

        # Require 85% accuracy on basic questions
        accuracy = correct_answers / total_questions
        assert (
            accuracy >= 0.85
        ), f"Basic GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"

    @pytest.mark.asyncio
    async def test_gdpr_intermediate_questions_accuracy(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test AI accuracy on intermediate GDPR questions"""
        assistant = ComplianceAssistant(db_session)

        intermediate_questions = [
            q for q in gdpr_golden_dataset if q["difficulty"] == "intermediate"
        ]
        correct_answers = 0
        total_questions = len(intermediate_questions)

        for question_data in intermediate_questions:
            mock_response = self._generate_mock_response(question_data)
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.85,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                if self._validate_response_accuracy(response, question_data):
                    correct_answers += 1

        # Require 75% accuracy on intermediate questions
        accuracy = correct_answers / total_questions
        assert (
            accuracy >= 0.75
        ), f"Intermediate GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"

    @pytest.mark.asyncio
    async def test_gdpr_advanced_questions_accuracy(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test AI accuracy on advanced GDPR questions"""
        assistant = ComplianceAssistant(db_session)

        advanced_questions = [
            q for q in gdpr_golden_dataset if q["difficulty"] == "advanced"
        ]
        correct_answers = 0
        total_questions = len(advanced_questions)

        for question_data in advanced_questions:
            mock_response = self._generate_mock_response(question_data)
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.8,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                if self._validate_response_accuracy(response, question_data):
                    correct_answers += 1

        # Require 65% accuracy on advanced questions
        accuracy = correct_answers / total_questions
        assert (
            accuracy >= 0.65
        ), f"Advanced GDPR accuracy too low: {accuracy:.2%} ({correct_answers}/{total_questions})"

    @pytest.mark.asyncio
    async def test_source_citation_accuracy(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test that AI responses include accurate source citations"""
        assistant = ComplianceAssistant(db_session)

        questions_with_sources = [q for q in gdpr_golden_dataset if "source" in q]
        correct_citations = 0

        for question_data in questions_with_sources:
            # Generate response that includes source citation
            mock_response = f"""
            {self._generate_mock_response(question_data)}

            **Source:** {question_data["source"]}
            """
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "sources": [question_data["source"]],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                # Check if response includes correct source citation
                expected_source = question_data["source"]
                if expected_source.lower() in response.lower():
                    correct_citations += 1

        # Require 80% accurate source citations
        citation_accuracy = correct_citations / len(questions_with_sources)
        assert (
            citation_accuracy >= 0.8
        ), f"Source citation accuracy too low: {citation_accuracy:.2%}"

    @pytest.mark.asyncio
    async def test_response_completeness(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test that AI responses are comprehensive and complete"""
        assistant = ComplianceAssistant(db_session)

        for question_data in gdpr_golden_dataset[:5]:  # Test subset for performance
            mock_response = self._generate_comprehensive_response(question_data)
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                # Check response completeness criteria
                assert len(response) >= 100, "Response too short to be comprehensive"
                assert self._contains_key_information(
                    response, question_data
                ), "Response missing key information"
                assert self._has_practical_guidance(
                    response
                ), "Response lacks practical guidance"

    @pytest.mark.asyncio
    async def test_framework_specific_terminology(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test that AI uses correct framework-specific terminology"""
        assistant = ComplianceAssistant(db_session)

        gdpr_terminology = [
            "data subject",
            "controller",
            "processor",
            "supervisory authority",
            "lawful basis",
            "legitimate interests",
            "data protection officer",
        ]

        for question_data in gdpr_golden_dataset[:3]:
            mock_response = self._generate_mock_response(
                question_data, include_terminology=True,
            )
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                # Check that response uses appropriate GDPR terminology
                response_lower = response.lower()
                relevant_terms = [
                    term
                    for term in gdpr_terminology
                    if any(keyword in term for keyword in question_data["keywords"])
                ]

                if relevant_terms:
                    found_terms = [
                        term for term in relevant_terms if term in response_lower
                    ]
                    assert (
                        len(found_terms) > 0
                    ), f"Response should use GDPR terminology for question about {question_data['category']}"

    @pytest.mark.asyncio
    async def test_consistency_across_similar_questions(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test that AI provides consistent answers to similar questions"""
        assistant = ComplianceAssistant(db_session)

        # Group questions by category
        categories = {}
        for question_data in gdpr_golden_dataset:
            category = question_data["category"]
            if category not in categories:
                categories[category] = []
            categories[category].append(question_data)

        # Test consistency within categories that have multiple questions
        for category, questions in categories.items():
            if len(questions) < 2:
                continue

            responses = []
            for question_data in questions:
                mock_response = self._generate_mock_response(question_data)
                mock_ai_client.generate_content.return_value.text = mock_response

                with patch.object(assistant, "process_message") as mock_process:
                    mock_process.return_value = (
                        mock_response,
                        {
                            "intent": "compliance_guidance",
                            "framework": question_data["framework"],
                            "confidence": 0.9,
                        },
                    )

                    response, metadata = await assistant.process_message(
                        conversation_id=uuid4(),
                        user=None,
                        message=question_data["question"],
                        business_profile_id=uuid4(),
                    )
                    responses.append(response)

            # Check for consistency in key concepts across responses
            assert self._check_conceptual_consistency(
                responses, category
            ), f"Inconsistent responses in category: {category}"

    @pytest.mark.asyncio
    async def test_regulatory_compliance_alignment(
        self,
        db_session,
        mock_ai_client,
        gdpr_golden_dataset,
    ):
        """Test that AI responses align with current regulatory requirements"""
        assistant = ComplianceAssistant(db_session)

        # Test that responses don't contradict regulatory requirements
        regulatory_violations = [
            "you don't need to comply",
            "gdpr doesn't apply",
            "ignore the regulation",
            "bypass the requirement",
            "this law is optional",
        ]

        for question_data in gdpr_golden_dataset:
            mock_response = self._generate_mock_response(question_data)
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "compliance_guidance",
                        "framework": question_data["framework"],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=question_data["question"],
                    business_profile_id=uuid4(),
                )

                # Check that response doesn't contain regulatory violations
                response_lower = response.lower()
                violations_found = [
                    v for v in regulatory_violations if v in response_lower
                ]
                assert (
                    len(violations_found) == 0
                ), f"Response contains regulatory violations: {violations_found}"

                # Check that response encourages compliance
                compliance_indicators = [
                    "should comply",
                    "required",
                    "mandatory",
                    "must implement",
                ]
                has_compliance_guidance = any(
                    indicator in response_lower for indicator in compliance_indicators
                )
                assert has_compliance_guidance, "Response should encourage compliance"

    def _generate_mock_response(
        self, question_data: Dict[str, Any], include_terminology: bool = False
    ) -> str:
        """Generate realistic mock AI response based on question data"""
        expected_answer = question_data["expected_answer"]
        keywords = question_data["keywords"]
        category = question_data["category"]

        # Create comprehensive response that includes expected information
        response_parts = [
            f"Based on {question_data['framework']} requirements:",
            "",
            expected_answer,
            "",
            f"This relates to {category.replace('_', ' ')} under {question_data['framework']}.",
            "",
            "You must implement these measures to ensure compliance with regulatory requirements.",
        ]

        # Add some keywords naturally
        if len(keywords) > 2:
            response_parts.extend(
                ["", f"Key considerations include: {', '.join(keywords[:3])}."],
            )

        if include_terminology and question_data["framework"] == "GDPR":
            response_parts.extend(
                [
                    "",
                    "As a data controller, you must ensure compliance with supervisory authority requirements.",
                ],
            )

        return "\n".join(response_parts)

    def _generate_comprehensive_response(self, question_data: Dict[str, Any]) -> str:
        """Generate comprehensive mock response with practical guidance"""
        base_response = self._generate_mock_response(question_data)

        practical_guidance = [
            "",
            "**Practical Implementation Steps:**",
            "1. Review your current practices",
            "2. Implement necessary controls",
            "3. Document your compliance measures",
            "4. Train your team on requirements",
            "",
            "**Next Steps:**",
            f"Consider conducting a {question_data['framework']} gap analysis to identify areas for improvement.",
        ]

        return base_response + "\n".join(practical_guidance)

    def _validate_response_accuracy(
        self, response: str, question_data: Dict[str, Any]
    ) -> bool:
        """Validate if response contains accurate information"""
        response_lower = response.lower()
        keywords = [kw.lower() for kw in question_data["keywords"]]

        # Check if response contains majority of expected keywords
        found_keywords = [kw for kw in keywords if kw in response_lower]
        keyword_accuracy = len(found_keywords) / len(keywords)

        # Check if response mentions the correct framework
        framework_mentioned = question_data["framework"].lower() in response_lower

        # Consider response accurate if it has good keyword coverage and mentions framework
        return keyword_accuracy >= 0.6 and framework_mentioned

    def _contains_key_information(
        self, response: str, question_data: Dict[str, Any]
    ) -> bool:
        """Check if response contains key information from expected answer"""
        expected_words = question_data["expected_answer"].lower().split()
        response_words = response.lower().split()

        # Check overlap between expected answer and response
        common_words = set(expected_words) & set(response_words)
        overlap_ratio = len(common_words) / len(expected_words)

        return overlap_ratio >= 0.4  # At least 40% word overlap

    def _has_practical_guidance(self, response: str) -> bool:
        """Check if response includes practical implementation guidance"""
        practical_indicators = [
            "steps",
            "implement",
            "consider",
            "should",
            "must",
            "recommendation",
            "best practice",
            "ensure",
            "review",
        ]

        response_lower = response.lower()
        guidance_count = sum(
            1 for indicator in practical_indicators if indicator in response_lower
        )

        return guidance_count >= 2  # At least 2 practical guidance indicators

    def _check_conceptual_consistency(
        self, responses: List[str], category: str
    ) -> bool:
        """Check conceptual consistency across responses in same category"""
        # Extract key concepts from all responses
        all_words = []
        for response in responses:
            words = response.lower().split()
            all_words.extend(words)

        # Check that responses share common terminology
        from collections import Counter

        word_counts = Counter(all_words)

        # Look for category-specific terms that appear in multiple responses
        category_terms = {
            "penalties": ["fine", "penalty", "sanctions"],
            "breach_notification": ["breach", "notification", "report", "hours"],
            "data_subject_rights": ["rights", "subject", "request", "access"],
            "consent": ["consent", "freely", "specific", "informed"],
        }

        if category in category_terms:
            expected_terms = category_terms[category]
            shared_terms = sum(1 for term in expected_terms if word_counts[term] >= 2)
            return shared_terms >= 1  # At least one shared term across responses

        return True  # Default to consistent for unknown categories


@pytest.mark.ai
@pytest.mark.compliance
class TestFrameworkCoverage:
    """Test AI coverage across different compliance frameworks"""

    @pytest.mark.asyncio
    async def test_framework_identification_accuracy(self, db_session, mock_ai_client):
        """Test AI accurately identifies relevant compliance frameworks"""
        assistant = ComplianceAssistant(db_session)

        framework_test_cases = [
            {
                "question": "We process customer personal data in the EU",
                "expected_frameworks": ["GDPR"],
                "context": {"location": "EU", "data_type": "personal"},
            },
            {
                "question": "Our healthcare app needs security compliance",
                "expected_frameworks": ["HIPAA", "ISO 27001"],
                "context": {"industry": "healthcare", "focus": "security"},
            },
            {
                "question": "SOC 2 audit requirements for our SaaS platform",
                "expected_frameworks": ["SOC 2"],
                "context": {"industry": "saas", "focus": "audit"},
            },
        ]

        for test_case in framework_test_cases:
            # Mock response that identifies correct frameworks
            frameworks_text = ", ".join(test_case["expected_frameworks"])
            mock_response = f"Based on your requirements, the relevant frameworks are: {frameworks_text}."
            mock_ai_client.generate_content.return_value.text = mock_response

            with patch.object(assistant, "process_message") as mock_process:
                mock_process.return_value = (
                    mock_response,
                    {
                        "intent": "framework_identification",
                        "identified_frameworks": test_case["expected_frameworks"],
                        "confidence": 0.9,
                    },
                )

                response, metadata = await assistant.process_message(
                    conversation_id=uuid4(),
                    user=None,
                    message=test_case["question"],
                    business_profile_id=uuid4(),
                )

                # Verify correct frameworks are identified
                for framework in test_case["expected_frameworks"]:
                    assert (
                        framework.lower() in response.lower()
                    ), f"Response should identify {framework} framework"

    @pytest.mark.asyncio
    async def test_cross_framework_guidance(self, db_session, mock_ai_client):
        """Test AI provides guidance when multiple frameworks apply"""
        assistant = ComplianceAssistant(db_session)

        multi_framework_question = "We're a fintech company processing EU customer data. What compliance frameworks apply?"

        mock_response = """
        For a fintech company processing EU customer data, multiple compliance frameworks apply:

        1. **GDPR** - Required for processing EU personal data
        2. **PCI DSS** - Required for payment card data processing
        3. **ISO 27001** - Recommended for information security management
        4. **SOC 2** - May be required by business customers

        These frameworks complement each other and share common security requirements.
        """

        mock_ai_client.generate_content.return_value.text = mock_response

        with patch.object(assistant, "process_message") as mock_process:
            mock_process.return_value = (
                mock_response,
                {
                    "intent": "multi_framework_guidance",
                    "identified_frameworks": ["GDPR", "PCI DSS", "ISO 27001", "SOC 2"],
                    "confidence": 0.9,
                },
            )

            response, metadata = await assistant.process_message(
                conversation_id=uuid4(),
                user=None,
                message=multi_framework_question,
                business_profile_id=uuid4(),
            )

            # Check that multiple frameworks are mentioned
            frameworks = ["GDPR", "PCI DSS", "ISO 27001", "SOC 2"]
            mentioned_frameworks = [fw for fw in frameworks if fw in response]

            assert (
                len(mentioned_frameworks) >= 3
            ), f"Should mention multiple frameworks, found: {mentioned_frameworks}"

            # Check that response explains relationships between frameworks
            relationship_indicators = ["complement", "overlap", "common", "together"]
            has_relationship_guidance = any(
                indicator in response.lower() for indicator in relationship_indicators
            )
            assert has_relationship_guidance, "Should explain framework relationships"
