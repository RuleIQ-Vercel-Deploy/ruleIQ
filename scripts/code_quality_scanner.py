#!/usr/bin/env python3
"""
Code Quality Scanner and Fixer for RuleIQ
Identifies and fixes common code quality issues to achieve SonarCloud Grade A/B
"""

import ast
import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Set, Tuple

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class CodeQualityScanner:
    """Scans Python files for code quality issues"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.issues = {
            "cognitive_complexity": [],
            "long_methods": [],
            "code_duplication": [],
            "todo_comments": [],
            "missing_docstrings": [],
            "unused_imports": [],
            "magic_numbers": [],
            "deep_nesting": [],
        }
        self.stats = {
            "files_scanned": 0,
            "total_lines": 0,
            "functions_analyzed": 0,
            "classes_analyzed": 0,
        }

    def scan_project(self) -> Dict: python_files = self._find_python_files()
        
        for file_path in python_files:
            self._scan_file(file_path)
            
        return {
            "issues": self.issues,
            "stats": self.stats,
            "summary": self._generate_summary(),
        }

    def _find_python_files(self) -> List[Path]: exclude_dirs = {
            "__pycache__", ".venv", "venv", "env", 
            "migrations", "alembic", "historical", 
            "backups", ".git", "htmlcov", "coverage"
        }
        
        python_files = []
        for root, dirs, files in os.walk(self.project_root):
            # Remove excluded directories from search
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file.endswith(".py"):
                    python_files.append(Path(root) / file)
                    
        return python_files

    def _scan_file(self, file_path: Path) -> None: try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                
            self.stats["files_scanned"] += 1
            self.stats["total_lines"] += len(content.splitlines())
            
            # Parse AST
            tree = ast.parse(content, filename=str(file_path))
            
            # Run various checks
            self._check_cognitive_complexity(tree, file_path)
            self._check_long_methods(tree, file_path)
            self._check_todo_comments(content, file_path)
            self._check_missing_docstrings(tree, file_path)
            self._check_deep_nesting(tree, file_path)
            
        except (SyntaxError, UnicodeDecodeError) as e:
            print(f"Error scanning {file_path}: {e}")

    def _check_cognitive_complexity(self, tree: ast.AST, file_path: Path) -> None: for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                complexity = self._calculate_cognitive_complexity(node)
                if complexity > 15:  # SonarCloud threshold
                    self.issues["cognitive_complexity"].append({
                        "file": str(file_path),
                        "function": node.name,
                        "line": node.lineno,
                        "complexity": complexity,
                    })
                self.stats["functions_analyzed"] += 1

    def _calculate_cognitive_complexity(self, node: ast.AST) -> int: complexity = 0
        nesting_level = 0
        
        class ComplexityVisitor(ast.NodeVisitor):
            def __init__(self): self.complexity = 0
                self.nesting_level = 0
                
            def visit_If(self, node):
                """Visit If"""
                self.complexity += 1 + self.nesting_level
                self.nesting_level += 1
                self.generic_visit(node)
                self.nesting_level -= 1
                
            def visit_While(self, node):
                """Visit While"""
                self.complexity += 1 + self.nesting_level
                self.nesting_level += 1
                self.generic_visit(node)
                self.nesting_level -= 1
                
            def visit_For(self, node):
                """Visit For"""
                self.complexity += 1 + self.nesting_level
                self.nesting_level += 1
                self.generic_visit(node)
                self.nesting_level -= 1
                
            def visit_ExceptHandler(self, node):
                """Visit Excepthandler"""
                self.complexity += 1 + self.nesting_level
                self.nesting_level += 1
                self.generic_visit(node)
                self.nesting_level -= 1
                
            def visit_BoolOp(self, node):
                """Visit Boolop"""
                # Each boolean operator adds complexity
                self.complexity += len(node.values) - 1
                self.generic_visit(node)
                
        visitor = ComplexityVisitor()
        visitor.visit(node)
        return visitor.complexity

    def _check_long_methods(self, tree: ast.AST, file_path: Path) -> None: for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if hasattr(node, "end_lineno") and hasattr(node, "lineno"):
                    length = node.end_lineno - node.lineno
                    if length > 50:
                        self.issues["long_methods"].append({
                            "file": str(file_path),
                            "function": node.name,
                            "line": node.lineno,
                            "length": length,
                        })

    def _check_todo_comments(self, content: str, file_path: Path) -> None: pattern = re.compile(r"#\s*(TODO|FIXME|XXX|HACK|NOTE|BUG)[\s:]+(.*)", re.IGNORECASE)
        
        for i, line in enumerate(content.splitlines(), 1):
            match = pattern.search(line)
            if match:
                self.issues["todo_comments"].append({
                    "file": str(file_path),
                    "line": i,
                    "type": match.group(1).upper(),
                    "message": match.group(2).strip(),
                })

    def _check_missing_docstrings(self, tree: ast.AST, file_path: Path) -> None: for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):
                docstring = ast.get_docstring(node)
                if not docstring and not node.name.startswith("_"):
                    self.issues["missing_docstrings"].append({
                        "file": str(file_path),
                        "name": node.name,
                        "line": node.lineno,
                        "type": "function" if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) else "class",
                    })
                if isinstance(node, ast.ClassDef):
                    self.stats["classes_analyzed"] += 1

    def _check_deep_nesting(self, tree: ast.AST, file_path: Path) -> None: class NestingVisitor(ast.NodeVisitor):
            def __init__(self): self.max_depth = 0
                self.current_depth = 0
                self.deep_locations = []
                
            def _enter_block(self, node):
                self.current_depth += 1
                if self.current_depth > 4:
                    self.deep_locations.append({
                        "line": node.lineno,
                        "depth": self.current_depth,
                    })
                self.max_depth = max(self.max_depth, self.current_depth)
                
            def _exit_block(self):
                self.current_depth -= 1
                
            def visit_FunctionDef(self, node):
                """Visit Functiondef"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
            def visit_AsyncFunctionDef(self, node):
                """Visit Asyncfunctiondef"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
            def visit_If(self, node):
                """Visit If"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
            def visit_While(self, node):
                """Visit While"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
            def visit_For(self, node):
                """Visit For"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
            def visit_With(self, node):
                """Visit With"""
                self._enter_block(node)
                self.generic_visit(node)
                self._exit_block()
                
        visitor = NestingVisitor()
        visitor.visit(tree)
        
        if visitor.deep_locations:
            self.issues["deep_nesting"].append({
                "file": str(file_path),
                "locations": visitor.deep_locations,
                "max_depth": visitor.max_depth,
            })

    def _generate_summary(self) -> Dict: total_issues = sum(len(issues) for issues in self.issues.values())
        
        return {
            "total_issues": total_issues,
            "critical_issues": len(self.issues["cognitive_complexity"]),
            "major_issues": len(self.issues["long_methods"]) + len(self.issues["deep_nesting"]),
            "minor_issues": len(self.issues["todo_comments"]) + len(self.issues["missing_docstrings"]),
            "files_with_issues": len(set(
                issue["file"] 
                for issue_list in self.issues.values() 
                for issue in issue_list
            )),
        }


class CodeQualityFixer:
    """Automatically fixes common code quality issues"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.fixes_applied = {
            "docstrings_added": 0,
            "todos_converted": 0,
            "imports_removed": 0,
            "functions_refactored": 0,
        }

    def fix_missing_docstrings(self, issues: List[Dict]) -> None: files_to_fix = {}
        
        for issue in issues:
            file_path = issue["file"]
            if file_path not in files_to_fix:
                files_to_fix[file_path] = []
            files_to_fix[file_path].append(issue)
            
        for file_path, file_issues in files_to_fix.items():
            self._add_docstrings_to_file(file_path, file_issues)

    def _add_docstrings_to_file(self, file_path: str, issues: List[Dict]) -> None: try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()
                
            # Sort issues by line number in reverse order
            issues.sort(key=lambda x: x["line"], reverse=True)
            
            for issue in issues:
                line_num = issue["line"] - 1  # Convert to 0-based index
                name = issue["name"]
                item_type = issue["type"]
                
                # Find the correct indentation
                indent = len(lines[line_num]) - len(lines[line_num].lstrip())
                indent_str = " " * (indent + 4)
                
                # Create appropriate docstring
                if item_type == "function":
                    docstring = f'{indent_str}"""{name.replace("_", " ").title()}"""\n'
                else:  # class
                    docstring = f'{indent_str}"""Class for {name}"""\n'
                    
                # Insert docstring after the function/class definition
                insert_line = line_num + 1
                while insert_line < len(lines) and lines[insert_line].strip().startswith("@"):
                    insert_line += 1
                    
                if insert_line < len(lines) and not lines[insert_line].strip().startswith('"""'):
                    lines.insert(insert_line + 1, docstring)
                    self.fixes_applied["docstrings_added"] += 1
                    
            # Write back the file
            with open(file_path, "w", encoding="utf-8") as f:
                f.writelines(lines)
                
        except Exception as e:
            print(f"Error fixing docstrings in {file_path}: {e}")

    def convert_todos_to_issues(self, todos: List[Dict]) -> None: issues_file = self.project_root / "CODE_QUALITY_ISSUES.md"
        
        with open(issues_file, "w", encoding="utf-8") as f:
            f.write("# Code Quality Issues\n\n")
            f.write("## TODO Comments to Address\n\n")
            
            # Group by type
            by_type = {}
            for todo in todos:
                todo_type = todo["type"]
                if todo_type not in by_type:
                    by_type[todo_type] = []
                by_type[todo_type].append(todo)
                
            for todo_type, items in by_type.items():
                f.write(f"### {todo_type} ({len(items)} items)\n\n")
                for item in items:
                    f.write(f"- [ ] **{item['file']}:{item['line']}** - {item['message']}\n")
                f.write("\n")
                
        self.fixes_applied["todos_converted"] = len(todos)
        print(f"Created {issues_file} with {len(todos)} TODO items")


def generate_sonarcloud_report(scanner_results: Dict) -> None: report_file = Path("sonarcloud_quality_report.json")
    
    import json
    
    sonar_issues = []
    
    # Convert issues to SonarCloud format
    for issue in scanner_results["issues"]["cognitive_complexity"]:
        sonar_issues.append({
            "engineId": "custom-python",
            "ruleId": "python:S3776",
            "severity": "CRITICAL",
            "type": "CODE_SMELL",
            "primaryLocation": {
                "message": f"Refactor this function to reduce its Cognitive Complexity from {issue['complexity']} to the 15 allowed.",
                "filePath": issue["file"],
                "textRange": {
                    "startLine": issue["line"],
                }
            }
        })
        
    for issue in scanner_results["issues"]["long_methods"]:
        sonar_issues.append({
            "engineId": "custom-python",
            "ruleId": "python:S138",
            "severity": "MAJOR",
            "type": "CODE_SMELL",
            "primaryLocation": {
                "message": f"This function has {issue['length']} lines, which is greater than the 50 authorized.",
                "filePath": issue["file"],
                "textRange": {
                    "startLine": issue["line"],
                }
            }
        })
        
    report = {
        "issues": sonar_issues,
        "summary": scanner_results["summary"],
        "stats": scanner_results["stats"],
    }
    
    with open(report_file, "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2)
        
    print(f"Generated SonarCloud report: {report_file}")


def main(): project_root = Path(__file__).parent.parent
    
    print("ðŸ” Scanning RuleIQ codebase for quality issues...")
    scanner = CodeQualityScanner(project_root)
    results = scanner.scan_project()
    
    # Print summary
    print("\nðŸ“Š Code Quality Summary:")
    print(f"  Files scanned: {results['stats']['files_scanned']}")
    print(f"  Total lines: {results['stats']['total_lines']}")
    print(f"  Functions analyzed: {results['stats']['functions_analyzed']}")
    print(f"  Classes analyzed: {results['stats']['classes_analyzed']}")
    
    print("\nâš ï¸  Issues Found:")
    print(f"  Critical (Cognitive Complexity > 15): {len(results['issues']['cognitive_complexity'])}")
    print(f"  Major (Long Methods > 50 lines): {len(results['issues']['long_methods'])}")
    print(f"  Major (Deep Nesting > 4 levels): {len(results['issues']['deep_nesting'])}")
    print(f"  Minor (TODO Comments): {len(results['issues']['todo_comments'])}")
    print(f"  Minor (Missing Docstrings): {len(results['issues']['missing_docstrings'])}")
    
    # Apply fixes
    print("\nðŸ”§ Applying automatic fixes...")
    fixer = CodeQualityFixer(project_root)
    
    if results["issues"]["missing_docstrings"]:
        fixer.fix_missing_docstrings(results["issues"]["missing_docstrings"])
        print(f"  Added {fixer.fixes_applied['docstrings_added']} docstrings")
        
    if results["issues"]["todo_comments"]:
        fixer.convert_todos_to_issues(results["issues"]["todo_comments"])
        
    # Generate SonarCloud report
    generate_sonarcloud_report(results)
    
    # Calculate estimated quality rating
    total_issues = results["summary"]["total_issues"]
    critical = results["summary"]["critical_issues"]
    
    if critical == 0 and total_issues < 50:
        rating = "A"
    elif critical < 5 and total_issues < 100:
        rating = "B"
    elif critical < 10 and total_issues < 200:
        rating = "C"
    elif critical < 20 and total_issues < 500:
        rating = "D"
    else:
        rating = "E"
        
    print(f"\nðŸŽ¯ Estimated Quality Rating: {rating}")
    
    if rating in ["A", "B"]:
        print("âœ… Code quality meets target (Grade A/B)")
    else:
        print(f"âŒ Code quality needs improvement to reach Grade A/B (currently {rating})")
        print("\nRecommended actions:")
        print("1. Refactor functions with high cognitive complexity")
        print("2. Break down long methods into smaller, focused functions")
        print("3. Reduce nesting levels by using early returns")
        print("4. Address all TODO comments")
        print("5. Add comprehensive docstrings")


if __name__ == "__main__":
    main()