"""
from __future__ import annotations

Phase 4: Unified Celery Migration Graph
Central orchestrator for all migrated Celery tasks with scheduling support
"""
import logging
from typing import Any, Dict, Optional
from datetime import datetime, timezone
from uuid import uuid4
import json
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langchain_core.messages import HumanMessage, AIMessage
from ..nodes.celery_migration_nodes import TaskMigrationState, ComplianceTaskNode, EvidenceTaskNode, NotificationTaskNode, ReportingTaskNode, MonitoringTaskNode
from ..graph.error_handler import ErrorHandlerNode
from ..scheduler.task_scheduler import TaskScheduler
logger = logging.getLogger(__name__)

class CeleryMigrationGraph:
    """
    Unified orchestrator for all migrated Celery tasks.
    Replaces Celery with LangGraph for 100% task coverage.
    """

    def __init__(self, checkpointer: Optional[AsyncPostgresSaver]=None) -> None:
        self.checkpointer = checkpointer
        self.graph = None
        self.compiled_graph = None
        self.compliance_node = ComplianceTaskNode()
        self.evidence_node = EvidenceTaskNode()
        self.notification_node = NotificationTaskNode()
        self.reporting_node = ReportingTaskNode()
        self.monitoring_node = MonitoringTaskNode()
        self.error_handler = ErrorHandlerNode()
        self.scheduler = TaskScheduler()
        self.task_routes = {'update_compliance_scores': 'compliance', 'check_compliance_alerts': 'compliance', 'process_evidence': 'evidence', 'sync_evidence': 'evidence', 'compliance_alert': 'notification', 'weekly_summary': 'notification', 'broadcast': 'notification', 'generate_report': 'reporting', 'on_demand_report': 'reporting', 'cleanup_reports': 'reporting', 'report_notifications': 'reporting', 'database_metrics': 'monitoring', 'health_check': 'monitoring', 'cleanup_monitoring': 'monitoring', 'system_metrics': 'monitoring', 'register_tasks': 'monitoring'}
        self._setup_graph()
        self._register_scheduled_tasks()

    def _setup_graph(self):
        """Build the unified task processing graph"""
        workflow = StateGraph(TaskMigrationState)
        workflow.add_node('router', self._router_node)
        workflow.add_node('compliance', self.compliance_node.process)
        workflow.add_node('evidence', self.evidence_node.process)
        workflow.add_node('notification', self.notification_node.process)
        workflow.add_node('reporting', self.reporting_node.process)
        workflow.add_node('monitoring', self.monitoring_node.process)
        workflow.add_node('error_handler', self.error_handler.process)
        workflow.add_node('result_processor', self._result_processor_node)
        workflow.set_entry_point('router')
        workflow.add_conditional_edges('router', self._route_task, {'compliance': 'compliance', 'evidence': 'evidence', 'notification': 'notification', 'reporting': 'reporting', 'monitoring': 'monitoring', 'error': 'error_handler', 'end': END})
        for node in ['compliance', 'evidence', 'notification', 'reporting', 'monitoring']:
            workflow.add_edge(node, 'result_processor')
        workflow.add_conditional_edges('result_processor', self._check_retry, {'retry': 'router', 'end': END})
        workflow.add_conditional_edges('error_handler', self._check_error_retry, {'retry': 'router', 'end': END})
        self.graph = workflow
        self.compiled_graph = workflow.compile(checkpointer=self.checkpointer)

    async def _router_node(self, state: TaskMigrationState) -> TaskMigrationState:
        """Initial routing node for incoming tasks"""
        logger.info(f"Routing task: {state.get('task_type')}")
        state['migration_timestamp'] = datetime.now(timezone.utc).isoformat()
        state['task_status'] = 'running'
        state['execution_metrics'] = {'start_time': datetime.now(timezone.utc).isoformat(), 'route_taken': self.task_routes.get(state.get('task_type'), 'unknown')}
        return state

    def _route_task(self, state: TaskMigrationState) -> str:
        """Determine which node should handle the task"""
        task_type = state.get('task_type', '')
        if task_type in self.task_routes:
            return self.task_routes[task_type]
        else:
            logger.error(f'Unknown task type: {task_type}')
            return 'error'

    async def _result_processor_node(self, state: TaskMigrationState) -> TaskMigrationState:
        """Process task results and handle post-execution logic"""
        logger.info(f"Processing results for task: {state.get('task_type')}")
        if 'execution_metrics' in state:
            state['execution_metrics']['end_time'] = datetime.now(timezone.utc).isoformat()
            start = datetime.fromisoformat(state['execution_metrics']['start_time'])
            end = datetime.now(timezone.utc)
            state['execution_metrics']['execution_time_ms'] = (end - start).total_seconds() * 1000
        if state.get('task_status') == 'completed':
            logger.info(f"Task {state.get('task_type')} completed successfully")
            result_summary = json.dumps(state.get('task_result', {}), indent=2)
            state['messages'].append(AIMessage(content=f'Task completed: {result_summary[:500]}'))
        return state

    def _check_retry(self, state: TaskMigrationState) -> str:
        """Determine if task should be retried"""
        if state.get('task_status') == 'failed':
            retry_count = state.get('retry_count', 0)
            max_retries = state.get('max_retries', 3)
            if retry_count < max_retries:
                logger.info(f"Retrying task {state.get('task_type')} (attempt {retry_count + 1})")
                state['retry_count'] = retry_count + 1
                return 'retry'
        return 'end'

    def _check_error_retry(self, state: TaskMigrationState) -> str:
        """Check if error handler suggests retry"""
        if state.get('should_retry', False):
            retry_count = state.get('retry_count', 0)
            max_retries = state.get('max_retries', 3)
            if retry_count < max_retries:
                state['retry_count'] = retry_count + 1
                state['should_retry'] = False
                return 'retry'
        return 'end'

    def _register_scheduled_tasks(self):
        """Register periodic tasks with the scheduler"""
        self.scheduler.register_task('system_metrics_collection', self._create_task_executor('system_metrics'), interval_seconds=60)
        self.scheduler.register_task('database_metrics_collection', self._create_task_executor('database_metrics'), interval_seconds=300)
        self.scheduler.register_task('database_health_check', self._create_task_executor('health_check'), interval_seconds=900)
        self.scheduler.register_task('compliance_score_update', self._create_task_executor('update_compliance_scores'), interval_seconds=3600)
        self.scheduler.register_task('compliance_alert_check', self._create_task_executor('check_compliance_alerts'), interval_seconds=21600)
        self.scheduler.register_task('evidence_status_sync', self._create_task_executor('sync_evidence'), interval_seconds=7200)
        self.scheduler.register_task('monitoring_data_cleanup', self._create_task_executor('cleanup_monitoring'), interval_seconds=21600)
        self.scheduler.register_task('old_reports_cleanup', self._create_task_executor('cleanup_reports'), interval_seconds=86400)
        self.scheduler.register_task('weekly_summary_send', self._create_task_executor('weekly_summary'), interval_seconds=604800)
        logger.info('Registered 9 scheduled tasks with LangGraph scheduler')

    def _create_task_executor(self, task_type: str):
        """Create an executor function for a scheduled task"""

        async def executor() -> Dict[str, Any]:
            state = self._create_initial_state(task_type)
            result = await self.execute_task(state)
            return result
        return executor

    def _create_initial_state(self, task_type: str, params: Optional[Dict]=None) -> TaskMigrationState:
        """Create initial state for a task"""
        return {'messages': [HumanMessage(content=f'Execute {task_type} task')], 'session_id': str(uuid4()), 'company_id': str(uuid4()), 'task_type': task_type, 'task_params': params or {}, 'task_result': None, 'task_status': 'pending', 'original_task_name': task_type, 'migration_timestamp': datetime.now(timezone.utc).isoformat(), 'execution_metrics': {}, 'errors': [], 'retry_count': 0, 'max_retries': 3}

    async def execute_task(self, state: Optional[TaskMigrationState]=None, task_type: Optional[str]=None, params: Optional[Dict]=None) -> Dict[str, Any]:
        """
        Execute a migrated Celery task through LangGraph

        Args:
            state: Pre-built state (optional)
            task_type: Type of task to execute
            params: Task parameters

        Returns:
            Task execution result
        """
        if state is None:
            if task_type is None:
                raise ValueError('Either state or task_type must be provided')
            state = self._create_initial_state(task_type, params)
        try:
            config = {'configurable': {'thread_id': state['session_id']}}
            final_state = None
            async for event in self.compiled_graph.astream(state, config):
                final_state = event
            if final_state and isinstance(final_state, dict):
                for _key, value in final_state.items():
                    if isinstance(value, dict) and 'task_result' in value:
                        return value.get('task_result', {})
            return {'status': 'completed', 'message': 'Task executed successfully'}
        except Exception as e:
            logger.error(f'Error executing task {task_type}: {str(e)}')
            return {'status': 'failed', 'error': str(e), 'task_type': task_type}

    async def start_scheduler(self) -> None:
        """Start the task scheduler for periodic tasks"""
        await self.scheduler.start()
        logger.info('Celery migration scheduler started - all periodic tasks active')

    async def stop_scheduler(self) -> None:
        """Stop the task scheduler"""
        await self.scheduler.stop()
        logger.info('Celery migration scheduler stopped')

    def get_task_statistics(self) -> Dict[str, Any]:
        """Get statistics about migrated tasks"""
        return {'total_tasks_migrated': 16, 'task_categories': {'compliance': 2, 'evidence': 2, 'notification': 3, 'reporting': 4, 'monitoring': 5}, 'scheduled_tasks': 9, 'on_demand_tasks': 7, 'migration_complete': True, 'coverage_percentage': 100.0}

async def create_celery_migration_graph(checkpointer: Optional[AsyncPostgresSaver]=None) -> CeleryMigrationGraph:
    """
    Factory function to create and initialize the Celery migration graph

    Args:
        checkpointer: Optional PostgreSQL checkpointer for state persistence

    Returns:
        Initialized CeleryMigrationGraph instance
    """
    graph = CeleryMigrationGraph(checkpointer)
    await graph.start_scheduler()
    logger.info('Celery Migration Graph initialized - 100% task coverage achieved')
    return graph
